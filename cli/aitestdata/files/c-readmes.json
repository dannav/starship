[
  {
    "repo": "torvalds/linux",
    "content": "Linux kernel\n============\n\nThere are several guides for kernel developers and users. These guides can\nbe rendered in a number of formats, like HTML and PDF. Please read\nDocumentation/admin-guide/README.rst first.\n\nIn order to build the documentation, use ``make htmldocs`` or\n``make pdfdocs``.  The formatted documentation can also be read online at:\n\n    https://www.kernel.org/doc/html/latest/\n\nThere are various text files in the Documentation/ subdirectory,\nseveral of them using the Restructured Text markup notation.\n\nPlease read the Documentation/process/changes.rst file, as it contains the\nrequirements for building and running the kernel, and information about\nthe problems which may result by upgrading your kernel.\n"
  },
  {
    "repo": "netdata/netdata",
    "content": "# netdata [![Build Status](https://travis-ci.com/netdata/netdata.svg?branch=master)](https://travis-ci.com/netdata/netdata) [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/2231/badge)](https://bestpractices.coreinfrastructure.org/projects/2231) [![License: GPL v3+](https://img.shields.io/badge/License-GPL%20v3%2B-blue.svg)](https://www.gnu.org/licenses/gpl-3.0) [![analytics](https://www.google-analytics.com/collect?v=1\u0026aip=1\u0026t=pageview\u0026_s=1\u0026ds=github\u0026dr=https%3A%2F%2Fgithub.com%2Fnetdata%2Fnetdata\u0026dl=https%3A%2F%2Fmy-netdata.io%2Fgithub%2Freadme\u0026_u=MAC~\u0026cid=5792dfd7-8dc4-476b-af31-da2fdb9f93d2\u0026tid=UA-64295674-3)]()\n  \n[![Code Climate](https://codeclimate.com/github/netdata/netdata/badges/gpa.svg)](https://codeclimate.com/github/netdata/netdata) [![Codacy Badge](https://api.codacy.com/project/badge/Grade/a994873f30d045b9b4b83606c3eb3498)](https://www.codacy.com/app/netdata/netdata?utm_source=github.com\u0026amp;utm_medium=referral\u0026amp;utm_content=netdata/netdata\u0026amp;utm_campaign=Badge_Grade) [![LGTM C](https://img.shields.io/lgtm/grade/cpp/g/netdata/netdata.svg?logo=lgtm)](https://lgtm.com/projects/g/netdata/netdata/context:cpp) [![LGTM JS](https://img.shields.io/lgtm/grade/javascript/g/netdata/netdata.svg?logo=lgtm)](https://lgtm.com/projects/g/netdata/netdata/context:javascript) [![LGTM PYTHON](https://img.shields.io/lgtm/grade/python/g/netdata/netdata.svg?logo=lgtm)](https://lgtm.com/projects/g/netdata/netdata/context:python)\n\n---\n\n**Netdata** is **distributed, real-time, performance and health monitoring for systems and applications**. It is a highly optimized monitoring agent you install on all your systems and containers.\n\nNetdata provides **unparalleled insights**, **in real-time**, of everything happening on the systems it runs (including web servers, databases, applications), using **highly interactive web dashboards**.  It can run autonomously, without any third party components, or it can be integrated to existing monitoring tool chains (Prometheus, Graphite, OpenTSDB, Kafka, Grafana, etc).\n\n_Netdata is **fast** and **efficient**, designed to permanently run on all systems (**physical** \u0026 **virtual** servers, **containers**, **IoT** devices), without disrupting their core function._\n\nNetdata is **free, open-source software** and it currently runs on **Linux**, **FreeBSD**, and **MacOS**.  \n\n![cncf](https://www.cncf.io/wp-content/uploads/2016/09/logo_cncf.png)  \n\nNetdata is in the [Cloud Native Computing Foundation (CNCF) landscape](https://landscape.cncf.io/grouping=no\u0026sort=stars).\nCheck the [CNCF TOC Netdata presentation](https://docs.google.com/presentation/d/18C8bCTbtgKDWqPa57GXIjB2PbjjpjsUNkLtZEz6YK8s/edit?usp=sharing).  \n\n---\n\nPeople get **addicted to netdata**.\u003cbr/\u003e\nOnce you use it on your systems, **there is no going back**! *You have been warned...*\n\n![image](https://user-images.githubusercontent.com/2662304/48305662-9de82980-e537-11e8-9f5b-aa1a60fbb82f.png)\n\n[![Tweet about netdata!](https://img.shields.io/twitter/url/http/shields.io.svg?style=social\u0026label=Tweet%20about%20netdata)](https://twitter.com/intent/tweet?text=Netdata,%20real-time%20performance%20and%20health%20monitoring,%20done%20right!\u0026url=https://my-netdata.io/\u0026via=linuxnetdata\u0026hashtags=netdata,monitoring)\n\n\n## Contents\n\n1. [How it looks](#how-it-looks) - have a quick look at it\n2. [User base](#user-base) - who uses netdata?\n3. [Quick Start](#quick-start) - try it now on your systems\n4. [Why Netdata](#why-netdata) - why people love netdata, how it compares with other solutions\n5. [News](#news) - latest news about netdata\n6. [How it works](#how-it-works) - high level diagram of how netdata works\n7. [infographic](#infographic) - everything about netdata, in a page\n8. [Features](#features) - what features does it have\n9. [Visualization](#visualization) - unique visualization features\n10. [What does it monitor](#what-does-it-monitor) - which metrics it collects\n11. [Documentation](#documentation) - read the docs\n12. [Community](#community) - discuss with others and get support\n13. [License](#license) - check the license of netdata\n\n\n## How it looks\n\nThe following animated image, shows the top part of a typical netdata dashboard.\n\n![peek 2018-11-11 02-40](https://user-images.githubusercontent.com/2662304/48307727-9175c800-e55b-11e8-92d8-a581d60a4889.gif)\n\n*A typical netdata dashboard, in 1:1 timing. Charts can be panned by dragging them, zoomed in/out with `SHIFT` + `mouse wheel`, an area can be selected for zoom-in with `SHIFT` + `mouse selection`. Netdata is highly interactive and **real-time**, optimized to get the work done!*\n\n\u003e *We have a few online demos to experience it live: [https://my-netdata.io](https://my-netdata.io)*  \n\n## User base\n\nNetdata is used by hundreds of thousands of users all over the world.\nCheck our [GitHub watchers list](https://github.com/netdata/netdata/watchers).\nYou will find people working for **Amazon**, **Atos**, **Baidu**, **Cisco Systems**, **Citrix**, **Deutsche Telekom**, **DigitalOcean**, \n**Elastic**, **EPAM Systems**, **Ericsson**, **Google**, **Groupon**, **Hortonworks**, **HP**, **Huawei**,\n**IBM**, **Microsoft**, **NewRelic**, **Nvidia**, **Red Hat**, **SAP**, **Selectel**, **TicketMaster**,\n**Vimeo**, and many more!\n\n### Docker pulls\nWe provide docker images for the most common architectures. These are statistics reported by docker hub:\n\n[![netdata/netdata (official)](https://img.shields.io/docker/pulls/netdata/netdata.svg?label=netdata/netdata+%28official%29)](https://hub.docker.com/r/netdata/netdata/) [![firehol/netdata (deprecated)](https://img.shields.io/docker/pulls/firehol/netdata.svg?label=firehol/netdata+%28deprecated%29)](https://hub.docker.com/r/firehol/netdata/) [![titpetric/netdata (donated)](https://img.shields.io/docker/pulls/titpetric/netdata.svg?label=titpetric/netdata+%28third+party%29)](https://hub.docker.com/r/titpetric/netdata/)\n\n### Registry\nWhen you install multiple netdata, they are integrated into **one distributed application**, via a [netdata registry](registry/#registry). This is a web browser feature and it allows us to count the number of unique users and unique netdata servers installed. The following information comes from the global public netdata registry we run:\n\n[![User Base](https://registry.my-netdata.io/api/v1/badge.svg?chart=netdata.registry_entries\u0026dimensions=persons\u0026label=user%20base\u0026units=M\u0026value_color=blue\u0026precision=2\u0026divide=1000000\u0026v43)](https://registry.my-netdata.io/#menu_netdata_submenu_registry) [![Monitored Servers](https://registry.my-netdata.io/api/v1/badge.svg?chart=netdata.registry_entries\u0026dimensions=machines\u0026label=servers%20monitored\u0026units=k\u0026divide=1000\u0026value_color=orange\u0026precision=2\u0026v43)](https://registry.my-netdata.io/#menu_netdata_submenu_registry) [![Sessions Served](https://registry.my-netdata.io/api/v1/badge.svg?chart=netdata.registry_sessions\u0026label=sessions%20served\u0026units=M\u0026value_color=yellowgreen\u0026precision=2\u0026divide=1000000\u0026v43)](https://registry.my-netdata.io/#menu_netdata_submenu_registry)  \n  \n*in the last 24 hours:*\u003cbr/\u003e [![New Users Today](https://registry.my-netdata.io/api/v1/badge.svg?chart=netdata.registry_entries\u0026dimensions=persons\u0026after=-86400\u0026options=unaligned\u0026group=incremental-sum\u0026label=new%20users%20today\u0026units=null\u0026value_color=blue\u0026precision=0\u0026v42)](https://registry.my-netdata.io/#menu_netdata_submenu_registry) [![New Machines Today](https://registry.my-netdata.io/api/v1/badge.svg?chart=netdata.registry_entries\u0026dimensions=machines\u0026group=incremental-sum\u0026after=-86400\u0026options=unaligned\u0026label=servers%20added%20today\u0026units=null\u0026value_color=orange\u0026precision=0\u0026v42)](https://registry.my-netdata.io/#menu_netdata_submenu_registry) [![Sessions Today](https://registry.my-netdata.io/api/v1/badge.svg?chart=netdata.registry_sessions\u0026after=-86400\u0026group=incremental-sum\u0026options=unaligned\u0026label=sessions%20served%20today\u0026units=null\u0026value_color=yellowgreen\u0026precision=0\u0026v42)](https://registry.my-netdata.io/#menu_netdata_submenu_registry)  \n\n## Quick Start\n\nYou can quickly install netdata on a Linux box (physical, virtual, container, IoT) with the following command:\n\n```sh\n# make sure you run `bash` for your shell\nbash\n\n# install netdata, directly from github sources\nbash \u003c(curl -Ss https://my-netdata.io/kickstart.sh)\n```\n![](https://registry.my-netdata.io/api/v1/badge.svg?chart=web_log_nginx.requests_per_url\u0026options=unaligned\u0026dimensions=kickstart\u0026group=sum\u0026after=-3600\u0026label=last+hour\u0026units=installations\u0026value_color=orange\u0026precision=0) ![](https://registry.my-netdata.io/api/v1/badge.svg?chart=web_log_nginx.requests_per_url\u0026options=unaligned\u0026dimensions=kickstart\u0026group=sum\u0026after=-86400\u0026label=today\u0026units=installations\u0026precision=0)\n\nThe above command will:\n\n1. install any required packages on your system (it will ask you to confirm before doing so),\n2. download netdata source to `/usr/src/netdata.git`\n3. compile it, install it and start it\n\nMore installation methods and additional options can be found at the [installation page](installer/#installation).\n\nTo try netdata in a docker container, run this:\n\n```\ndocker run -d --name=netdata \\\n  -p 19999:19999 \\\n  -v /proc:/host/proc:ro \\\n  -v /sys:/host/sys:ro \\\n  -v /var/run/docker.sock:/var/run/docker.sock:ro \\\n  --cap-add SYS_PTRACE \\\n  --security-opt apparmor=unconfined \\\n  netdata/netdata\n```\n\nFor more information about running netdata in docker, check the [docker installation page](packaging/docker/).\n\n![image](https://user-images.githubusercontent.com/2662304/48304090-fd384080-e51b-11e8-80ae-eecb03118dda.png)\n\n## Why Netdata\n\nNetdata has a quite different approach to monitoring.\n\nNetdata is a monitoring agent you install on all your systems. It is:\n\n- a **metrics collector** - for system and application metrics (including web servers, databases, containers, etc)\n- a **time-series database** - all stored in memory (does not touch the disks while it runs)\n- a **metrics visualizer** - super fast, interactive, modern, optimized for anomaly detection\n- an **alarms notification engine** - an advanced watchdog for detecting performance and availability issues\n\nAll the above, are packaged together in a very flexible, extremely modular, distributed application.\n\nThis is how netdata compares to other monitoring solutions:\n\nnetdata|others (open-source and commercial)\n:---:|:---:\n**High resolution metrics** (1s granularity)|Low resolution metrics (10s granularity at best)\nMonitors everything, **thousands of metrics per node**|Monitor just a few metrics\nUI is super fast, optimized for **anomaly detection**|UI is good for just an abstract view\n**Meaningful presentation**, to help you understand the metrics|You have to know the metrics before you start\nInstall and get results **immediately**|Long preparation is required to get any useful results\nUse it for **troubleshooting** performance problems|Use them to get *statistics of past performance*\n**Kills the console** for tracing performance issues|The console is always required for troubleshooting\nRequires **zero dedicated resources**|Require large dedicated resources\n\nNetdata is **open-source**, **free**, super **fast**, very **easy**, completely **open**, extremely **efficient**,\n**flexible** and integrate-able.\n\nIt has been designed by **SysAdmins**, **DevOps** and **Developers** for troubleshooting performance problems,\nnot just visualize metrics.\n\n## News  \n  \n`Nov 22nd, 2018` - **[netdata v1.11.1 released!](https://github.com/netdata/netdata/releases)**  \n\n- Improved internal database to support values above 64bit.\n- New data collection plugins: [`openldap`](collectors/python.d.plugin/openldap/), [`tor`](collectors/python.d.plugin/tor/), [`nvidia_smi`](collectors/python.d.plugin/nvidia_smi/).\n- Improved data collection plugins: netdata now supports monitoring network interface aliases, [`smartd_log`](collectors/python.d.plugin/smartd_log/), [`cpufreq`](collectors/python.d.plugin/cpufreq/), [`sensors`](collectors/python.d.plugin/sensors/).\n- Health monitoring improvements: network interface congestion alarm restored, [`alerta.io`](health/notifications/alerta/), `conntrack_max`.\n- `my-netdata`menu has been refactored. \n- Packaging: `openrc` service definition got a few improvements.\n\n---  \n  \n`Sep 18, 2018` - **netdata has its own organization**  \n  \nNetdata used to be a [firehol.org](https://firehol.org) project, accessible as `firehol/netdata`.  \n  \nNetdata now has its own github organization `netdata`, so all github URLs are now `netdata/netdata`. The old github URLs, repo clones, forks, etc redirect automatically to the new repo.    \n\n## How it works\n\nNetdata is a highly efficient, highly modular, metrics management engine. Its lockless design makes it ideal for concurrent operations on the metrics.\n\n![image](https://user-images.githubusercontent.com/2662304/48323827-b4c17580-e636-11e8-842c-0ee72fcb4115.png)\n\nThis is how it works:\n\nFunction|Description|Documentation\n:---:|:---|:---:\n**Collect**|Multiple independent data collection workers are collecting metrics from their sources using the optimal protocol for each application and push the metrics to the database. Each data collection worker has lockless write access to the metrics it collects.|[`collectors`](collectors/#data-collection-plugins)\n**Store**|Metrics are stored in RAM in a round robin database (ring buffer), using a custom made floating point number for minimal footprint.|[`database`](database/#database)\n**Check**|A lockless independent watchdog is evaluating **health checks** on the collected metrics, triggers alarms, maintains a health transaction log and dispatches alarm notifications.|[`health`](health/#health-monitoring)\n**Stream**|An lockless independent worker is streaming metrics, in full detail and in real-time, to remote netdata servers, as soon as they are collected.|[`streaming`](streaming/#streaming-and-replication)\n**Archive**|A lockless independent worker is down-sampling the metrics and pushes them to **backend** time-series databases.|[`backends`](backends/)\n**Query**|Multiple independent workers are attached to the [internal web server](web/server/#web-server), servicing API requests, including [data queries](web/api/queries/#database-queries).|[`web/api`](web/api/#api)\n\nThe result is a highly efficient, low latency system, supporting multiple readers and one writer on each metric.\n\n## Infographic  \n  \nThis is a high level overview of netdata feature set and architecture.  \nClick it to to interact with it (it has direct links to documentation).  \n  \n[![image](https://user-images.githubusercontent.com/2662304/47672043-a47eb480-dbb9-11e8-92a4-fa422d053309.png)](https://my-netdata.io/infographic.html)  \n\n\n## Features\n\n![finger-video](https://user-images.githubusercontent.com/2662304/48346998-96cf3180-e685-11e8-9f4e-059d23aa3aa5.gif)\n\nThis is what you should expect from Netdata:\n\n### General\n- **1s granularity** - the highest possible resolution for all metrics.\n- **Unlimited metrics** - collects all the available metrics, the more the better.\n- **1% CPU utilization of a single core** - it is super fast, unbelievably optimized.\n- **A few MB of RAM** - by default it uses 25MB RAM. [You size it](database).\n- **Zero disk I/O** - while it runs, it does not load or save anything (except `error` and `access` logs).\n- **Zero configuration** - auto-detects everything, it can collect up to 10000 metrics per server out of the box.\n- **Zero maintenance** - You just run it, it does the rest.\n- **Zero dependencies** - it is even its own web server, for its static web files and its web API (though its plugins may require additional libraries, depending on the applications monitored).  \n- **Scales to infinity** - you can install it on all your servers, containers, VMs and IoTs. Metrics are not centralized by default, so there is no limit.\n- **Several operating modes** - Autonomous host monitoring (the default), headless data collector, forwarding proxy, store and forward proxy, central multi-host monitoring, in all possible configurations. Each node may have different metrics retention policy and run with or without health monitoring.\n\n### Health Monitoring \u0026 Alarms\n- **Sophisticated alerting** - comes with hundreds of alarms, **out of the box**! Supports dynamic thresholds, hysteresis, alarm templates, multiple role-based notification methods.\n- **Notifications**: [alerta.io](health/notifications/alerta/), [amazon sns](health/notifications/awssns/), [discordapp.com](health/notifications/discord/), [email](health/notifications/email/), [flock.com](health/notifications/flock/), [irs](health/notifications/irc/), [kavenegar.com](health/notifications/kavenegar/), [messagebird.com](health/notifications/messagebird/), [pagerduty.com](health/notifications/pagerduty/), [pushbullet.com](health/notifications/pushbullet/), [pushover.net](health/notifications/pushover/), [rocket.chat](health/notifications/rocketchat/), [slack.com](health/notifications/slack/), [syslog](health/notifications/syslog/), [telegram.org](health/notifications/telegram/), [twilio.com](health/notifications/twilio/), [web](health/notifications/web/).\n\n### Integrations\n- **time-series dbs** - can archive its metrics to `graphite`, `opentsdb`, `prometheus`, json document DBs, in the same or lower resolution (lower: to prevent it from congesting these servers due to the amount of data collected).\n\n## Visualization\n\n- **Stunning interactive dashboards** - mouse, touchpad and touch-screen friendly in 2 themes: `slate` (dark) and `white`.\n- **Amazingly fast visualization** - responds to all queries in less than 1 ms per metric, even on low-end hardware.\n- **Visual anomaly detection** - the dashboards are optimized for detecting anomalies visually.\n- **Embeddable** - its charts can be embedded on your web pages, wikis and blogs. You can even use [Atlassian's Confluence as a monitoring dashboard](web/gui/confluence/).\n- **Customizable** - custom dashboards can be built using simple HTML (no javascript necessary).\n\n### Positive and negative values\n\nTo improve clarity on charts, netdata dashboards present **positive** values for metrics representing `read`, `input`, `inbound`, `received` and **negative** values for metrics representing `write`, `output`, `outbound`, `sent`.\n\n![positive-and-negative-values](https://user-images.githubusercontent.com/2662304/48309090-7c5c6180-e57a-11e8-8e03-3a7538c14223.gif)\n\n*Netdata charts showing the bandwidth and packets of a network interface. `received` is positive and `sent` is negative.*\n\n### Autoscaled y-axis\n\nNetdata charts automatically zoom vertically, to visualize the variation of each metric within the visible time-frame.\n\n![non-zero-based](https://user-images.githubusercontent.com/2662304/48309139-3d2f1000-e57c-11e8-9a44-b91758134b00.gif)\n\n*A zero based `stacked` chart, automatically switches to an auto-scaled `area` chart when a single dimension is selected.*\n\n### Charts are synchronized\n\nCharts on netdata dashboards are synchronized to each other. There is no master chart. Any chart can be panned or zoomed at any time, and all other charts will follow.\n\n![charts-are-synchronized](https://user-images.githubusercontent.com/2662304/48309003-b4fb3b80-e578-11e8-86f6-f505c7059c15.gif)\n\n*Charts are panned by dragging them with the mouse. Charts can be zoomed in/out with`SHIFT` + `mouse wheel` while the mouse pointer is over a chart.*\n\n\u003e The visible time-frame (pan and zoom) is propagated from netdata server to netdata server, when navigating via the [`my-netdata` menu](registry#registry).\n\n\n### Highlighted time-frame\n\nTo improve visual anomaly detection across charts, the user can highlight a time-frame (by pressing `ALT` + `mouse selection`) on all charts.\n\n![highlighted-timeframe](https://user-images.githubusercontent.com/2662304/48311876-f9093300-e5ae-11e8-9c74-e3e291741990.gif)\n\n*A highlighted time-frame can be given by pressing `ALT` + `mouse selection` on any chart. Netdata will highlight the same range on all charts.*\n\n\u003e Highlighted ranges are propagated from netdata server to netdata server, when navigating via the [`my-netdata` menu](registry#registry).\n\n\n## What does it monitor  \n\nNetdata data collection is **extensible** - you can monitor anything you can get a metric for.\nIts [Plugin API](collectors/plugins.d/) supports all programing languages (anything can be a netdata plugin, BASH, python, perl, node.js, java, Go, ruby, etc).\n\n- For better performance, most system related plugins (cpu, memory, disks, filesystems, networking, etc) have been written in `C`.\n- For faster development and easier contributions, most application related plugins (databases, web servers, etc) have been written in `python`.\n\n#### APM (Application Performance Monitoring)\n- **[statsd](collectors/statsd.plugin/)** - netdata is a fully featured statsd server.\n- **[Go expvar](collectors/python.d.plugin/go_expvar/)** - collects metrics exposed by applications written in the Go programming language using the expvar package.\n- **[Spring Boot](collectors/python.d.plugin/springboot/)** - monitors running Java Spring Boot applications that expose their metrics with the use of the Spring Boot Actuator included in Spring Boot library.\n- **[uWSGI](collectors/python.d.plugin/uwsgi/)** - collects performance metrics from uWSGI applications.\n\n#### System Resources\n- **[CPU Utilization](collectors/proc.plugin/)** - total and per core CPU usage.\n- **[Interrupts](collectors/proc.plugin/)** - total and per core CPU interrupts.\n- **[SoftIRQs](collectors/proc.plugin/)** - total and per core SoftIRQs.\n- **[SoftNet](collectors/proc.plugin/)** - total and per core SoftIRQs related to network activity.\n- **[CPU Throttling](collectors/proc.plugin/)** - collects per core CPU throttling.\n- **[CPU Frequency](collectors/python.d.plugin/couchdb/)** - collects the current CPU frequency.\n- **[CPU Idle](collectors/python.d.plugin/cpuidle/)** - collects the time spent per processor state.\n- **[IdleJitter](collectors/idlejitter.plugin/)** - measures CPU latency.\n- **[Entropy](collectors/proc.plugin/)** - random numbers pool, using in cryptography.\n- **[Interprocess Communication - IPC](collectors/proc.plugin/)** - such as semaphores and semaphores arrays.\n\n#### Memory\n- **[ram](collectors/proc.plugin/)** - collects info about RAM usage.\n- **[swap](collectors/proc.plugin/)** - collects info about swap memory usage.\n- **[available memory](collectors/proc.plugin/)** - collects the amount of RAM available for userspace processes.\n- **[committed memory](collectors/proc.plugin/)** - collects the amount of RAM committed to userspace processes.\n- **[Page Faults](collectors/proc.plugin/)** - collects the system page faults (major and minor).\n- **[writeback memory](collectors/proc.plugin/)** - collects the system dirty memory and writeback activity.\n- **[huge pages](collectors/proc.plugin/)** - collects the amount of RAM used for huge pages.\n- **[KSM](collectors/proc.plugin/)** - collects info about Kernel Same Merging (memory dedupper).\n- **[Numa](collectors/proc.plugin/)** - collects Numa info on systems that support it.\n- **[slab](collectors/proc.plugin/)** - collects info about the Linux kernel memory usage.\n\n#### Disks\n- **[block devices](collectors/proc.plugin/)** - per disk: I/O, operations, backlog, utilization, space, etc.  \n- **[BCACHE](collectors/proc.plugin/)** - detailed performance of SSD caching devices.\n- **[DiskSpace](collectors/proc.plugin/)** - monitors disk space usage.\n- **[mdstat](collectors/python.d.plugin/mdstat/)** - software RAID.\n- **[hddtemp](collectors/python.d.plugin/hddtemp/)** - disk temperatures.\n- **[smartd](collectors/python.d.plugin/smartd_log/)** - disk S.M.A.R.T. values.\n- **[device mapper](collectors/proc.plugin/)** - naming disks.\n- **[Veritas Volume Manager](collectors/proc.plugin/)** - naming disks.\n- **[megacli](collectors/python.d.plugin/megacli/)** - adapter, physical drives and battery stats.\n- **[adaptec_raid](collectors/python.d.plugin/adaptec_raid/)** -  logical and physical devices health metrics.\n\n#### Filesystems\n- **[BTRFS](collectors/proc.plugin/)** - detailed disk space allocation and usage.\n- **[Ceph](collectors/python.d.plugin/ceph/)** - OSD usage, Pool usage, number of objects, etc.\n- **[NFS file servers and clients](collectors/proc.plugin/)** - NFS v2, v3, v4: I/O, cache, read ahead, RPC calls  \n- **[Samba](collectors/python.d.plugin/samba/)** - performance metrics of Samba SMB2 file sharing.\n- **[ZFS](collectors/proc.plugin/)** - detailed performance and resource usage.\n\n#### Networking\n- **[Network Stack](collectors/proc.plugin/)** - everything about the networking stack (both IPv4 and IPv6 for all protocols: TCP, UDP, SCTP, UDPLite, ICMP, Multicast, Broadcast, etc), and all network interfaces (per interface: bandwidth, packets, errors, drops).\n- **[Netfilter](collectors/proc.plugin/)** - everything about the netfilter connection tracker.\n- **[SynProxy](collectors/proc.plugin/)** - collects performance data about the linux SYNPROXY (DDoS).\n- **[NFacct](collectors/nfacct.plugin/)** - collects accounting data from iptables.\n- **[Network QoS](collectors/tc.plugin/)** - the only tool that visualizes network `tc` classes in real-time  \n- **[FPing](collectors/fping.plugin/)** - to measure latency and packet loss between any number of hosts.\n- **[ISC dhcpd](collectors/python.d.plugin/isc_dhcpd/)** - pools utilization, leases, etc.\n- **[AP](collectors/charts.d.plugin/ap/)** - collects Linux access point performance data (`hostapd`).\n- **[SNMP](collectors/node.d.plugin/snmp/)** - SNMP devices can be monitored too (although you will need to configure these).\n- **[port_check](collectors/python.d.plugin/portcheck/)** - checks TCP ports for availability and response time.\n\n#### Virtual Private Networks\n- **[OpenVPN](collectors/python.d.plugin/ovpn_status_log/)** - collects status per tunnel.\n- **[LibreSwan](collectors/charts.d.plugin/libreswan/)** - collects metrics per IPSEC tunnel.\n- **[Tor](collectors/python.d.plugin/tor/)** - collects Tor traffic statistics.\n\n#### Processes\n- **[System Processes](collectors/proc.plugin/)** - running, blocked, forks, active.\n- **[Applications](collectors/apps.plugin/)** - by grouping the process tree and reporting CPU, memory, disk reads, disk writes, swap, threads, pipes, sockets - per process group.  \n- **[systemd](collectors/cgroups.plugin/)** - monitors systemd services using CGROUPS.\n\n#### Users\n- **[Users and User Groups resource usage](collectors/apps.plugin/)** - by summarizing the process tree per user and group, reporting: CPU, memory, disk reads, disk writes, swap, threads, pipes, sockets\n- **[logind](collectors/python.d.plugin/logind/)** - collects sessions, users and seats connected.\n\n#### Containers and VMs\n- **[Containers](collectors/cgroups.plugin/)** - collects resource usage for all kinds of containers, using CGROUPS (systemd-nspawn, lxc, lxd, docker, kubernetes, etc).\n- **[libvirt VMs](collectors/cgroups.plugin/)** - collects resource usage for all kinds of VMs, using CGROUPS.\n- **[dockerd](collectors/python.d.plugin/dockerd/)** - collects docker health metrics.\n\n#### Web Servers\n- **[Apache and lighttpd](collectors/python.d.plugin/apache/)** - `mod-status` (v2.2, v2.4) and cache log statistics, for multiple servers.\n- **[IPFS](collectors/python.d.plugin/ipfs/)** - bandwidth, peers.\n- **[LiteSpeed](collectors/python.d.plugin/litespeed/)** - reads the litespeed rtreport files to collect metrics.\n- **[Nginx](collectors/python.d.plugin/nginx/)** - `stub-status`, for multiple servers.\n- **[Nginx+](collectors/python.d.plugin/nginx_plus/)** - connects to multiple nginx_plus servers (local or remote) to collect real-time performance metrics.\n- **[PHP-FPM](collectors/python.d.plugin/phpfpm/)** - multiple instances, each reporting connections, requests, performance, etc.\n- **[Tomcat](collectors/python.d.plugin/tomcat/)** - accesses, threads, free memory, volume, etc.\n- **[web server `access.log` files](collectors/python.d.plugin/web_log/)** - extracting in real-time, web server and proxy performance metrics and applying several health checks, etc.\n- **[HTTP check](collectors/python.d.plugin/httpcheck/)** - checks one or more web servers for HTTP status code and returned content.\n\n#### Proxies, Balancers, Accelerators\n- **[HAproxy](collectors/python.d.plugin/haproxy/)** - bandwidth, sessions, backends, etc.\n- **[Squid](collectors/python.d.plugin/squid/)** - multiple servers, each showing: clients bandwidth and requests, servers bandwidth and requests.\n- **[Traefik](collectors/python.d.plugin/traefik/)** - connects to multiple traefik instances (local or remote) to collect API metrics (response status code, response time, average response time and server uptime).\n- **[Varnish](collectors/python.d.plugin/varnish/)** - threads, sessions, hits, objects, backends, etc.\n- **[IPVS](collectors/proc.plugin/)** - collects metrics from the Linux IPVS load balancer.\n\n#### Database Servers\n- **[CouchDB](collectors/python.d.plugin/couchdb/)** - reads/writes, request methods, status codes, tasks, replication, per-db, etc.\n- **[MemCached](collectors/python.d.plugin/memcached/)** - multiple servers, each showing: bandwidth, connections, items, etc.\n- **[MongoDB](collectors/python.d.plugin/mongodb/)** - operations, clients, transactions, cursors, connections, asserts, locks, etc.\n- **[MySQL and mariadb](collectors/python.d.plugin/mysql/)** - multiple servers, each showing: bandwidth, queries/s, handlers, locks, issues, tmp operations, connections, binlog metrics, threads, innodb metrics, and more.\n- **[PostgreSQL](collectors/python.d.plugin/postgres/)** - multiple servers, each showing: per database statistics (connections, tuples read - written - returned, transactions, locks), backend processes, indexes, tables, write ahead, background writer and more.\n- **[Proxy SQL](collectors/python.d.plugin/proxysql/)** - collects Proxy SQL backend and frontend performance metrics.\n- **[Redis](collectors/python.d.plugin/redis/)** - multiple servers, each showing: operations, hit rate, memory, keys, clients, slaves.\n- **[RethinkDB](collectors/python.d.plugin/rethinkdbs/)** - connects to multiple rethinkdb servers (local or remote) to collect real-time metrics.\n\n#### Message Brokers\n- **[beanstalkd](collectors/python.d.plugin/beanstalk/)** - global and per tube monitoring.\n- **[RabbitMQ](collectors/python.d.plugin/rabbitmq/)** - performance and health metrics.\n\n#### Search and Indexing\n- **[ElasticSearch](collectors/python.d.plugin/elasticsearch/)** - search and index performance, latency, timings, cluster statistics, threads statistics, etc.\n\n#### DNS Servers\n- **[bind_rndc](collectors/python.d.plugin/bind_rndc/)** - parses `named.stats` dump file to collect real-time performance metrics. All versions of bind after 9.6 are supported.\n- **[dnsdist](collectors/python.d.plugin/dnsdist/)** - performance and health metrics.\n- **[ISC Bind (named)](collectors/node.d.plugin/named/)** - multiple servers, each showing: clients, requests, queries, updates, failures and several per view metrics. All versions of bind after 9.9.10 are supported.\n- **[NSD](collectors/python.d.plugin/nsd/)** - queries, zones, protocols, query types, transfers, etc.\n- **[PowerDNS](collectors/python.d.plugin/powerdns/)** - queries, answers, cache, latency, etc.\n- **[unbound](collectors/python.d.plugin/unbound/)** - performance and resource usage metrics.\n- **[dns_query_time](collectors/python.d.plugin/dns_query_time/)** - DNS query time statistics.\n\n#### Time Servers\n- **[chrony](collectors/python.d.plugin/chrony/)** - uses the `chronyc` command to collect chrony statistics (Frequency, Last offset, RMS offset, Residual freq, Root delay, Root dispersion, Skew, System time).  \n- **[ntpd](collectors/python.d.plugin/ntpd/)** - connects to multiple ntpd servers (local or remote) to provide statistics of system variables and optional also peer variables.\n\n#### Mail Servers\n- **[Dovecot](collectors/python.d.plugin/dovecot/)** - POP3/IMAP servers.\n- **[Exim](collectors/python.d.plugin/exim/)** - message queue (emails queued).\n- **[Postfix](collectors/python.d.plugin/postfix/)** - message queue (entries, size).\n\n#### Hardware Sensors\n- **[IPMI](collectors/freeipmi.plugin/)** - enterprise hardware sensors and events.\n- **[lm-sensors](collectors/python.d.plugin/sensors/)** - temperature, voltage, fans, power, humidity, etc.\n- **[Nvidia](collectors/python.d.plugin/nvidia_smi/)** - collects information for Nvidia GPUs.\n- **[RPi](collectors/charts.d.plugin/sensors/)** - Raspberry Pi temperature sensors.\n- **[w1sensor](collectors/python.d.plugin/w1sensor/)** - collects data from connected 1-Wire sensors.\n\n#### UPSes\n- **[apcupsd](collectors/charts.d.plugin/apcupsd/)** - load, charge, battery voltage, temperature, utility metrics, output metrics\n- **[NUT](collectors/charts.d.plugin/nut/)** - load, charge, battery voltage, temperature, utility metrics, output metrics\n- **[Linux Power Supply](collectors/python.d.plugin/linux_power_supply/)** - collects metrics reported by power supply drivers on Linux.\n\n#### Social Sharing Servers\n- **[RetroShare](collectors/python.d.plugin/retroshare/)** - connects to multiple retroshare servers (local or remote) to collect real-time performance metrics.\n\n#### Security\n- **[Fail2Ban](collectors/python.d.plugin/fail2ban/)** - monitors the fail2ban log file to check all bans for all active jails.\n\n#### Authentication, Authorization, Accounting (AAA, RADIUS, LDAP) Servers\n- **[FreeRadius](collectors/python.d.plugin/freeradius/)** - uses the `radclient` command to provide freeradius statistics (authentication, accounting, proxy-authentication, proxy-accounting).\n\n#### Telephony Servers\n- **[opensips](collectors/charts.d.plugin/opensips/)** - connects to an opensips server (localhost only) to collect real-time performance metrics.\n\n#### Household Appliances\n- **[SMA webbox](collectors/node.d.plugin/sma_webbox/)** - connects to multiple remote SMA webboxes to collect real-time performance metrics of the photovoltaic (solar) power generation.\n- **[Fronius](collectors/node.d.plugin/fronius/)** - connects to multiple remote Fronius Symo servers to collect real-time performance metrics of the photovoltaic (solar) power generation.\n- **[StiebelEltron](collectors/node.d.plugin/stiebeleltron/)** - collects the temperatures and other metrics from your Stiebel Eltron heating system using their Internet Service Gateway (ISG web).\n\n#### Game Servers\n- **[SpigotMC](collectors/python.d.plugin/spigotmc/)** - monitors Spigot Minecraft server ticks per second and number of online players using the Minecraft remote console.\n\n#### Distributed Computing\n- **[BOINC](collectors/python.d.plugin/boinc/)** - monitors task states for local and remote BOINC client software using the remote GUI RPC interface. Also provides alarms for a handful of error conditions.\n\n#### Media Streaming Servers\n- **[IceCast](collectors/python.d.plugin/icecast/)** - collects the number of listeners for active sources.\n\n### Monitoring Systems\n- **[Monit](collectors/python.d.plugin/monit/)** - collects metrics about monit targets (filesystems, applications, networks).\n\n#### Provisioning Systems\n- **[Puppet](collectors/python.d.plugin/puppet/)** - connects to multiple Puppet Server and Puppet DB instances (local or remote) to collect real-time status metrics.\n\nYou can easily extend Netdata, by writing plugins that collect data from any source, using any computer language.  \n  \n---  \n  \n## Documentation\n\nThe netdata documentation is at [https://docs.netdata.cloud](https://docs.netdata.cloud). But you can also find it inside the repo, so by just navigating the repo on github you can find all the documentation.\n\nHere is a quick list:\n\nDirectory|Description\n:---|:---\n[`installer`](installer/)|Instructions to install netdata on your systems.\n[`docker`](packaging/docker/)|Instructions to install netdata using docker.\n[`daemon`](daemon/)|Information about the netdata daemon and its configuration.\n[`collectors`](collectors/)|Information about data collection plugins.\n[`health`](health/)|How netdata's health monitoring works, how to create your own alarms and how to configure alarm notification methods.\n[`streaming`](streaming/)|How to build hierarchies of netdata servers, by streaming metrics between them.\n[`backends`](backends/)|Long term archiving of metrics to industry standard time-series databases, like `prometheus`, `graphite`, `opentsdb`.\n[`web/api`](web/api/)|Learn how to query the netdata API and the queries it supports.\n[`web/api/badges`](web/api/badges/)|Learn how to generate badges (SVG images) from live data.\n[`web/gui/custom`](web/gui/custom/)|Learn how to create custom netdata dashboards.\n[`web/gui/confluence`](web/gui/confluence/)|Learn how to create netdata dashboards on Atlassian's Confluence.\n\nYou can also check all the other directories. Most of them have plenty of documentation.\n\n## Community\n\nWe welcome [contributions](CONTRIBUTING.md). So, feel free to join the team.\n\nTo report bugs, or get help, use [GitHub Issues](https://github.com/netdata/netdata/issues).\n\nYou can also find netdata on:\n\n- [Facebook](https://www.facebook.com/linuxnetdata/)\n- [Twitter](https://twitter.com/linuxnetdata)\n- [OpenHub](https://www.openhub.net/p/netdata)\n- [Repology](https://repology.org/metapackage/netdata/versions)\n- [StackShare](https://stackshare.io/netdata)\n\n## License  \n  \nnetdata is [GPLv3+](LICENSE).  \n\nNetdata re-distributes other open-source tools and libraries. Please check the [third party licenses](REDISTRIBUTED.md).\n\n"
  },
  {
    "repo": "antirez/redis",
    "content": "This README is just a fast *quick start* document. You can find more detailed documentation at [redis.io](https://redis.io).\n\nWhat is Redis?\n--------------\n\nRedis is often referred as a *data structures* server. What this means is that Redis provides access to mutable data structures via a set of commands, which are sent using a *server-client* model with TCP sockets and a simple protocol. So different processes can query and modify the same data structures in a shared way.\n\nData structures implemented into Redis have a few special properties:\n\n* Redis cares to store them on disk, even if they are always served and modified into the server memory. This means that Redis is fast, but that is also non-volatile.\n* Implementation of data structures stress on memory efficiency, so data structures inside Redis will likely use less memory compared to the same data structure modeled using an high level programming language.\n* Redis offers a number of features that are natural to find in a database, like replication, tunable levels of durability, cluster, high availability.\n\nAnother good example is to think of Redis as a more complex version of memcached, where the operations are not just SETs and GETs, but operations to work with complex data types like Lists, Sets, ordered data structures, and so forth.\n\nIf you want to know more, this is a list of selected starting points:\n\n* Introduction to Redis data types. http://redis.io/topics/data-types-intro\n* Try Redis directly inside your browser. http://try.redis.io\n* The full list of Redis commands. http://redis.io/commands\n* There is much more inside the Redis official documentation. http://redis.io/documentation\n\nBuilding Redis\n--------------\n\nRedis can be compiled and used on Linux, OSX, OpenBSD, NetBSD, FreeBSD.\nWe support big endian and little endian architectures, and both 32 bit\nand 64 bit systems.\n\nIt may compile on Solaris derived systems (for instance SmartOS) but our\nsupport for this platform is *best effort* and Redis is not guaranteed to\nwork as well as in Linux, OSX, and \\*BSD there.\n\nIt is as simple as:\n\n    % make\n\nYou can run a 32 bit Redis binary using:\n\n    % make 32bit\n\nAfter building Redis, it is a good idea to test it using:\n\n    % make test\n\nFixing build problems with dependencies or cached build options\n---------\n\nRedis has some dependencies which are included into the `deps` directory.\n`make` does not automatically rebuild dependencies even if something in\nthe source code of dependencies changes.\n\nWhen you update the source code with `git pull` or when code inside the\ndependencies tree is modified in any other way, make sure to use the following\ncommand in order to really clean everything and rebuild from scratch:\n\n    make distclean\n\nThis will clean: jemalloc, lua, hiredis, linenoise.\n\nAlso if you force certain build options like 32bit target, no C compiler\noptimizations (for debugging purposes), and other similar build time options,\nthose options are cached indefinitely until you issue a `make distclean`\ncommand.\n\nFixing problems building 32 bit binaries\n---------\n\nIf after building Redis with a 32 bit target you need to rebuild it\nwith a 64 bit target, or the other way around, you need to perform a\n`make distclean` in the root directory of the Redis distribution.\n\nIn case of build errors when trying to build a 32 bit binary of Redis, try\nthe following steps:\n\n* Install the packages libc6-dev-i386 (also try g++-multilib).\n* Try using the following command line instead of `make 32bit`:\n  `make CFLAGS=\"-m32 -march=native\" LDFLAGS=\"-m32\"`\n\nAllocator\n---------\n\nSelecting a non-default memory allocator when building Redis is done by setting\nthe `MALLOC` environment variable. Redis is compiled and linked against libc\nmalloc by default, with the exception of jemalloc being the default on Linux\nsystems. This default was picked because jemalloc has proven to have fewer\nfragmentation problems than libc malloc.\n\nTo force compiling against libc malloc, use:\n\n    % make MALLOC=libc\n\nTo compile against jemalloc on Mac OS X systems, use:\n\n    % make MALLOC=jemalloc\n\nVerbose build\n-------------\n\nRedis will build with a user friendly colorized output by default.\nIf you want to see a more verbose output use the following:\n\n    % make V=1\n\nRunning Redis\n-------------\n\nTo run Redis with the default configuration just type:\n\n    % cd src\n    % ./redis-server\n\nIf you want to provide your redis.conf, you have to run it using an additional\nparameter (the path of the configuration file):\n\n    % cd src\n    % ./redis-server /path/to/redis.conf\n\nIt is possible to alter the Redis configuration by passing parameters directly\nas options using the command line. Examples:\n\n    % ./redis-server --port 9999 --replicaof 127.0.0.1 6379\n    % ./redis-server /etc/redis/6379.conf --loglevel debug\n\nAll the options in redis.conf are also supported as options using the command\nline, with exactly the same name.\n\nPlaying with Redis\n------------------\n\nYou can use redis-cli to play with Redis. Start a redis-server instance,\nthen in another terminal try the following:\n\n    % cd src\n    % ./redis-cli\n    redis\u003e ping\n    PONG\n    redis\u003e set foo bar\n    OK\n    redis\u003e get foo\n    \"bar\"\n    redis\u003e incr mycounter\n    (integer) 1\n    redis\u003e incr mycounter\n    (integer) 2\n    redis\u003e\n\nYou can find the list of all the available commands at http://redis.io/commands.\n\nInstalling Redis\n-----------------\n\nIn order to install Redis binaries into /usr/local/bin just use:\n\n    % make install\n\nYou can use `make PREFIX=/some/other/directory install` if you wish to use a\ndifferent destination.\n\nMake install will just install binaries in your system, but will not configure\ninit scripts and configuration files in the appropriate place. This is not\nneeded if you want just to play a bit with Redis, but if you are installing\nit the proper way for a production system, we have a script doing this\nfor Ubuntu and Debian systems:\n\n    % cd utils\n    % ./install_server.sh\n\nThe script will ask you a few questions and will setup everything you need\nto run Redis properly as a background daemon that will start again on\nsystem reboots.\n\nYou'll be able to stop and start Redis using the script named\n`/etc/init.d/redis_\u003cportnumber\u003e`, for instance `/etc/init.d/redis_6379`.\n\nCode contributions\n-----------------\n\nNote: by contributing code to the Redis project in any form, including sending\na pull request via Github, a code fragment or patch via private email or\npublic discussion groups, you agree to release your code under the terms\nof the BSD license that you can find in the [COPYING][1] file included in the Redis\nsource distribution.\n\nPlease see the [CONTRIBUTING][2] file in this source distribution for more\ninformation.\n\n[1]: https://github.com/antirez/redis/blob/unstable/COPYING\n[2]: https://github.com/antirez/redis/blob/unstable/CONTRIBUTING\n\nRedis internals\n===\n\nIf you are reading this README you are likely in front of a Github page\nor you just untarred the Redis distribution tar ball. In both the cases\nyou are basically one step away from the source code, so here we explain\nthe Redis source code layout, what is in each file as a general idea, the\nmost important functions and structures inside the Redis server and so forth.\nWe keep all the discussion at a high level without digging into the details\nsince this document would be huge otherwise and our code base changes\ncontinuously, but a general idea should be a good starting point to\nunderstand more. Moreover most of the code is heavily commented and easy\nto follow.\n\nSource code layout\n---\n\nThe Redis root directory just contains this README, the Makefile which\ncalls the real Makefile inside the `src` directory and an example\nconfiguration for Redis and Sentinel. You can find a few shell\nscripts that are used in order to execute the Redis, Redis Cluster and\nRedis Sentinel unit tests, which are implemented inside the `tests`\ndirectory.\n\nInside the root are the following important directories:\n\n* `src`: contains the Redis implementation, written in C.\n* `tests`: contains the unit tests, implemented in Tcl.\n* `deps`: contains libraries Redis uses. Everything needed to compile Redis is inside this directory; your system just needs to provide `libc`, a POSIX compatible interface and a C compiler. Notably `deps` contains a copy of `jemalloc`, which is the default allocator of Redis under Linux. Note that under `deps` there are also things which started with the Redis project, but for which the main repository is not `antirez/redis`. An exception to this rule is `deps/geohash-int` which is the low level geocoding library used by Redis: it originated from a different project, but at this point it diverged so much that it is developed as a separated entity directly inside the Redis repository.\n\nThere are a few more directories but they are not very important for our goals\nhere. We'll focus mostly on `src`, where the Redis implementation is contained,\nexploring what there is inside each file. The order in which files are\nexposed is the logical one to follow in order to disclose different layers\nof complexity incrementally.\n\nNote: lately Redis was refactored quite a bit. Function names and file\nnames have been changed, so you may find that this documentation reflects the\n`unstable` branch more closely. For instance in Redis 3.0 the `server.c`\nand `server.h` files were named `redis.c` and `redis.h`. However the overall\nstructure is the same. Keep in mind that all the new developments and pull\nrequests should be performed against the `unstable` branch.\n\nserver.h\n---\n\nThe simplest way to understand how a program works is to understand the\ndata structures it uses. So we'll start from the main header file of\nRedis, which is `server.h`.\n\nAll the server configuration and in general all the shared state is\ndefined in a global structure called `server`, of type `struct redisServer`.\nA few important fields in this structure are:\n\n* `server.db` is an array of Redis databases, where data is stored.\n* `server.commands` is the command table.\n* `server.clients` is a linked list of clients connected to the server.\n* `server.master` is a special client, the master, if the instance is a replica.\n\nThere are tons of other fields. Most fields are commented directly inside\nthe structure definition.\n\nAnother important Redis data structure is the one defining a client.\nIn the past it was called `redisClient`, now just `client`. The structure\nhas many fields, here we'll just show the main ones:\n\n    struct client {\n        int fd;\n        sds querybuf;\n        int argc;\n        robj **argv;\n        redisDb *db;\n        int flags;\n        list *reply;\n        char buf[PROTO_REPLY_CHUNK_BYTES];\n        ... many other fields ...\n    }\n\nThe client structure defines a *connected client*:\n\n* The `fd` field is the client socket file descriptor.\n* `argc` and `argv` are populated with the command the client is executing, so that functions implementing a given Redis command can read the arguments.\n* `querybuf` accumulates the requests from the client, which are parsed by the Redis server according to the Redis protocol and executed by calling the implementations of the commands the client is executing.\n* `reply` and `buf` are dynamic and static buffers that accumulate the replies the server sends to the client. These buffers are incrementally written to the socket as soon as the file descriptor is writable.\n\nAs you can see in the client structure above, arguments in a command\nare described as `robj` structures. The following is the full `robj`\nstructure, which defines a *Redis object*:\n\n    typedef struct redisObject {\n        unsigned type:4;\n        unsigned encoding:4;\n        unsigned lru:LRU_BITS; /* lru time (relative to server.lruclock) */\n        int refcount;\n        void *ptr;\n    } robj;\n\nBasically this structure can represent all the basic Redis data types like\nstrings, lists, sets, sorted sets and so forth. The interesting thing is that\nit has a `type` field, so that it is possible to know what type a given\nobject has, and a `refcount`, so that the same object can be referenced\nin multiple places without allocating it multiple times. Finally the `ptr`\nfield points to the actual representation of the object, which might vary\neven for the same type, depending on the `encoding` used.\n\nRedis objects are used extensively in the Redis internals, however in order\nto avoid the overhead of indirect accesses, recently in many places\nwe just use plain dynamic strings not wrapped inside a Redis object.\n\nserver.c\n---\n\nThis is the entry point of the Redis server, where the `main()` function\nis defined. The following are the most important steps in order to startup\nthe Redis server.\n\n* `initServerConfig()` setups the default values of the `server` structure.\n* `initServer()` allocates the data structures needed to operate, setup the listening socket, and so forth.\n* `aeMain()` starts the event loop which listens for new connections.\n\nThere are two special functions called periodically by the event loop:\n\n1. `serverCron()` is called periodically (according to `server.hz` frequency), and performs tasks that must be performed from time to time, like checking for timedout clients.\n2. `beforeSleep()` is called every time the event loop fired, Redis served a few requests, and is returning back into the event loop.\n\nInside server.c you can find code that handles other vital things of the Redis server:\n\n* `call()` is used in order to call a given command in the context of a given client.\n* `activeExpireCycle()` handles eviciton of keys with a time to live set via the `EXPIRE` command.\n* `freeMemoryIfNeeded()` is called when a new write command should be performed but Redis is out of memory according to the `maxmemory` directive.\n* The global variable `redisCommandTable` defines all the Redis commands, specifying the name of the command, the function implementing the command, the number of arguments required, and other properties of each command.\n\nnetworking.c\n---\n\nThis file defines all the I/O functions with clients, masters and replicas\n(which in Redis are just special clients):\n\n* `createClient()` allocates and initializes a new client.\n* the `addReply*()` family of functions are used by commands implementations in order to append data to the client structure, that will be transmitted to the client as a reply for a given command executed.\n* `writeToClient()` transmits the data pending in the output buffers to the client and is called by the *writable event handler* `sendReplyToClient()`.\n* `readQueryFromClient()` is the *readable event handler* and accumulates data from read from the client into the query buffer.\n* `processInputBuffer()` is the entry point in order to parse the client query buffer according to the Redis protocol. Once commands are ready to be processed, it calls `processCommand()` which is defined inside `server.c` in order to actually execute the command.\n* `freeClient()` deallocates, disconnects and removes a client.\n\naof.c and rdb.c\n---\n\nAs you can guess from the names these files implement the RDB and AOF\npersistence for Redis. Redis uses a persistence model based on the `fork()`\nsystem call in order to create a thread with the same (shared) memory\ncontent of the main Redis thread. This secondary thread dumps the content\nof the memory on disk. This is used by `rdb.c` to create the snapshots\non disk and by `aof.c` in order to perform the AOF rewrite when the\nappend only file gets too big.\n\nThe implementation inside `aof.c` has additional functions in order to\nimplement an API that allows commands to append new commands into the AOF\nfile as clients execute them.\n\nThe `call()` function defined inside `server.c` is responsible to call\nthe functions that in turn will write the commands into the AOF.\n\ndb.c\n---\n\nCertain Redis commands operate on specific data types, others are general.\nExamples of generic commands are `DEL` and `EXPIRE`. They operate on keys\nand not on their values specifically. All those generic commands are\ndefined inside `db.c`.\n\nMoreover `db.c` implements an API in order to perform certain operations\non the Redis dataset without directly accessing the internal data structures.\n\nThe most important functions inside `db.c` which are used in many commands\nimplementations are the following:\n\n* `lookupKeyRead()` and `lookupKeyWrite()` are used in order to get a pointer to the value associated to a given key, or `NULL` if the key does not exist.\n* `dbAdd()` and its higher level counterpart `setKey()` create a new key in a Redis database.\n* `dbDelete()` removes a key and its associated value.\n* `emptyDb()` removes an entire single database or all the databases defined.\n\nThe rest of the file implements the generic commands exposed to the client.\n\nobject.c\n---\n\nThe `robj` structure defining Redis objects was already described. Inside\n`object.c` there are all the functions that operate with Redis objects at\na basic level, like functions to allocate new objects, handle the reference\ncounting and so forth. Notable functions inside this file:\n\n* `incrRefcount()` and `decrRefCount()` are used in order to increment or decrement an object reference count. When it drops to 0 the object is finally freed.\n* `createObject()` allocates a new object. There are also specialized functions to allocate string objects having a specific content, like `createStringObjectFromLongLong()` and similar functions.\n\nThis file also implements the `OBJECT` command.\n\nreplication.c\n---\n\nThis is one of the most complex files inside Redis, it is recommended to\napproach it only after getting a bit familiar with the rest of the code base.\nIn this file there is the implementation of both the master and replica role\nof Redis.\n\nOne of the most important functions inside this file is `replicationFeedSlaves()` that writes commands to the clients representing replica instances connected\nto our master, so that the replicas can get the writes performed by the clients:\nthis way their data set will remain synchronized with the one in the master.\n\nThis file also implements both the `SYNC` and `PSYNC` commands that are\nused in order to perform the first synchronization between masters and\nreplicas, or to continue the replication after a disconnection.\n\nOther C files\n---\n\n* `t_hash.c`, `t_list.c`, `t_set.c`, `t_string.c` and `t_zset.c` contains the implementation of the Redis data types. They implement both an API to access a given data type, and the client commands implementations for these data types.\n* `ae.c` implements the Redis event loop, it's a self contained library which is simple to read and understand.\n* `sds.c` is the Redis string library, check http://github.com/antirez/sds for more information.\n* `anet.c` is a library to use POSIX networking in a simpler way compared to the raw interface exposed by the kernel.\n* `dict.c` is an implementation of a non-blocking hash table which rehashes incrementally.\n* `scripting.c` implements Lua scripting. It is completely self contained from the rest of the Redis implementation and is simple enough to understand if you are familar with the Lua API.\n* `cluster.c` implements the Redis Cluster. Probably a good read only after being very familiar with the rest of the Redis code base. If you want to read `cluster.c` make sure to read the [Redis Cluster specification][3].\n\n[3]: http://redis.io/topics/cluster-spec\n\nAnatomy of a Redis command\n---\n\nAll the Redis commands are defined in the following way:\n\n    void foobarCommand(client *c) {\n        printf(\"%s\",c-\u003eargv[1]-\u003eptr); /* Do something with the argument. */\n        addReply(c,shared.ok); /* Reply something to the client. */\n    }\n\nThe command is then referenced inside `server.c` in the command table:\n\n    {\"foobar\",foobarCommand,2,\"rtF\",0,NULL,0,0,0,0,0},\n\nIn the above example `2` is the number of arguments the command takes,\nwhile `\"rtF\"` are the command flags, as documented in the command table\ntop comment inside `server.c`.\n\nAfter the command operates in some way, it returns a reply to the client,\nusually using `addReply()` or a similar function defined inside `networking.c`.\n\nThere are tons of commands implementations inside the Redis source code\nthat can serve as examples of actual commands implementations. To write\na few toy commands can be a good exercise to familiarize with the code base.\n\nThere are also many other files not described here, but it is useless to\ncover everything. We want to just help you with the first steps.\nEventually you'll find your way inside the Redis code base :-)\n\nEnjoy!\n"
  },
  {
    "repo": "git/git",
    "content": "Git - fast, scalable, distributed revision control system\n=========================================================\n\nGit is a fast, scalable, distributed revision control system with an\nunusually rich command set that provides both high-level operations\nand full access to internals.\n\nGit is an Open Source project covered by the GNU General Public\nLicense version 2 (some parts of it are under different licenses,\ncompatible with the GPLv2). It was originally written by Linus\nTorvalds with help of a group of hackers around the net.\n\nPlease read the file [INSTALL][] for installation instructions.\n\nMany Git online resources are accessible from \u003chttps://git-scm.com/\u003e\nincluding full documentation and Git related tools.\n\nSee [Documentation/gittutorial.txt][] to get started, then see\n[Documentation/giteveryday.txt][] for a useful minimum set of commands, and\nDocumentation/git-\u003ccommandname\u003e.txt for documentation of each command.\nIf git has been correctly installed, then the tutorial can also be\nread with `man gittutorial` or `git help tutorial`, and the\ndocumentation of each command with `man git-\u003ccommandname\u003e` or `git help\n\u003ccommandname\u003e`.\n\nCVS users may also want to read [Documentation/gitcvs-migration.txt][]\n(`man gitcvs-migration` or `git help cvs-migration` if git is\ninstalled).\n\nThe user discussion and development of Git take place on the Git\nmailing list -- everyone is welcome to post bug reports, feature\nrequests, comments and patches to git@vger.kernel.org (read\n[Documentation/SubmittingPatches][] for instructions on patch submission).\nTo subscribe to the list, send an email with just \"subscribe git\" in\nthe body to majordomo@vger.kernel.org. The mailing list archives are\navailable at \u003chttps://public-inbox.org/git/\u003e,\n\u003chttp://marc.info/?l=git\u003e and other archival sites.\n\nIssues which are security relevant should be disclosed privately to\nthe Git Security mailing list \u003cgit-security@googlegroups.com\u003e.\n\nThe maintainer frequently sends the \"What's cooking\" reports that\nlist the current status of various development topics to the mailing\nlist.  The discussion following them give a good reference for\nproject status, development direction and remaining tasks.\n\nThe name \"git\" was given by Linus Torvalds when he wrote the very\nfirst version. He described the tool as \"the stupid content tracker\"\nand the name as (depending on your mood):\n\n - random three-letter combination that is pronounceable, and not\n   actually used by any common UNIX command.  The fact that it is a\n   mispronunciation of \"get\" may or may not be relevant.\n - stupid. contemptible and despicable. simple. Take your pick from the\n   dictionary of slang.\n - \"global information tracker\": you're in a good mood, and it actually\n   works for you. Angels sing, and a light suddenly fills the room.\n - \"goddamn idiotic truckload of sh*t\": when it breaks\n\n[INSTALL]: INSTALL\n[Documentation/gittutorial.txt]: Documentation/gittutorial.txt\n[Documentation/giteveryday.txt]: Documentation/giteveryday.txt\n[Documentation/gitcvs-migration.txt]: Documentation/gitcvs-migration.txt\n[Documentation/SubmittingPatches]: Documentation/SubmittingPatches\n"
  },
  {
    "repo": "Bilibili/ijkplayer",
    "content": "# ijkplayer\n\n Platform | Build Status\n -------- | ------------\n Android | [![Build Status](https://travis-ci.org/Bilibili/ci-ijk-ffmpeg-android.svg?branch=master)](https://travis-ci.org/Bilibili/ci-ijk-ffmpeg-android)\n iOS | [![Build Status](https://travis-ci.org/Bilibili/ci-ijk-ffmpeg-ios.svg?branch=master)](https://travis-ci.org/Bilibili/ci-ijk-ffmpeg-ios)\n\nVideo player based on [ffplay](http://ffmpeg.org)\n\n### Download\n\n- Android:\n - Gradle\n```\n# required\nallprojects {\n    repositories {\n        jcenter()\n    }\n}\n\ndependencies {\n    # required, enough for most devices.\n    compile 'tv.danmaku.ijk.media:ijkplayer-java:0.8.8'\n    compile 'tv.danmaku.ijk.media:ijkplayer-armv7a:0.8.8'\n\n    # Other ABIs: optional\n    compile 'tv.danmaku.ijk.media:ijkplayer-armv5:0.8.8'\n    compile 'tv.danmaku.ijk.media:ijkplayer-arm64:0.8.8'\n    compile 'tv.danmaku.ijk.media:ijkplayer-x86:0.8.8'\n    compile 'tv.danmaku.ijk.media:ijkplayer-x86_64:0.8.8'\n\n    # ExoPlayer as IMediaPlayer: optional, experimental\n    compile 'tv.danmaku.ijk.media:ijkplayer-exo:0.8.8'\n}\n```\n- iOS\n - in coming...\n\n### My Build Environment\n- Common\n - Mac OS X 10.11.5\n- Android\n - [NDK r10e](http://developer.android.com/tools/sdk/ndk/index.html)\n - Android Studio 2.1.3\n - Gradle 2.14.1\n- iOS\n - Xcode 7.3 (7D175)\n- [HomeBrew](http://brew.sh)\n - ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n - brew install git\n\n### Latest Changes\n- [NEWS.md](NEWS.md)\n\n### Features\n- Common\n - remove rarely used ffmpeg components to reduce binary size [config/module-lite.sh](config/module-lite.sh)\n - workaround for some buggy online video.\n- Android\n - platform: API 9~23\n - cpu: ARMv7a, ARM64v8a, x86 (ARMv5 is not tested on real devices)\n - api: [MediaPlayer-like](android/ijkplayer/ijkplayer-java/src/main/java/tv/danmaku/ijk/media/player/IMediaPlayer.java)\n - video-output: NativeWindow, OpenGL ES 2.0\n - audio-output: AudioTrack, OpenSL ES\n - hw-decoder: MediaCodec (API 16+, Android 4.1+)\n - alternative-backend: android.media.MediaPlayer, ExoPlayer\n- iOS\n - platform: iOS 7.0~10.2.x\n - cpu: armv7, arm64, i386, x86_64, (armv7s is obselete)\n - api: [MediaPlayer.framework-like](ios/IJKMediaPlayer/IJKMediaPlayer/IJKMediaPlayback.h)\n - video-output: OpenGL ES 2.0\n - audio-output: AudioQueue, AudioUnit\n - hw-decoder: VideoToolbox (iOS 8+)\n - alternative-backend: AVFoundation.Framework.AVPlayer, MediaPlayer.Framework.MPMoviePlayerControlelr (obselete since iOS 8)\n\n### NOT-ON-PLAN\n- obsolete platforms (Android: API-8 and below; iOS: pre-6.0)\n- obsolete cpu: ARMv5, ARMv6, MIPS (I don't even have these types of devices…)\n- native subtitle render\n- avfilter support\n\n### Before Build\n```\n# install homebrew, git, yasm\nruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\nbrew install git\nbrew install yasm\n\n# add these lines to your ~/.bash_profile or ~/.profile\n# export ANDROID_SDK=\u003cyour sdk path\u003e\n# export ANDROID_NDK=\u003cyour ndk path\u003e\n\n# on Cygwin (unmaintained)\n# install git, make, yasm\n```\n\n- If you prefer more codec/format\n```\ncd config\nrm module.sh\nln -s module-default.sh module.sh\ncd android/contrib\n# cd ios\nsh compile-ffmpeg.sh clean\n```\n\n- If you prefer less codec/format for smaller binary size (include hevc function)\n```\ncd config\nrm module.sh\nln -s module-lite-hevc.sh module.sh\ncd android/contrib\n# cd ios\nsh compile-ffmpeg.sh clean\n```\n\n- If you prefer less codec/format for smaller binary size (by default)\n```\ncd config\nrm module.sh\nln -s module-lite.sh module.sh\ncd android/contrib\n# cd ios\nsh compile-ffmpeg.sh clean\n```\n\n- For Ubuntu/Debian users.\n```\n# choose [No] to use bash\nsudo dpkg-reconfigure dash\n```\n\n- If you'd like to share your config, pull request is welcome.\n\n### Build Android\n```\ngit clone https://github.com/Bilibili/ijkplayer.git ijkplayer-android\ncd ijkplayer-android\ngit checkout -B latest k0.8.8\n\n./init-android.sh\n\ncd android/contrib\n./compile-ffmpeg.sh clean\n./compile-ffmpeg.sh all\n\ncd ..\n./compile-ijk.sh all\n\n# Android Studio:\n#     Open an existing Android Studio project\n#     Select android/ijkplayer/ and import\n#\n#     define ext block in your root build.gradle\n#     ext {\n#       compileSdkVersion = 23       // depending on your sdk version\n#       buildToolsVersion = \"23.0.0\" // depending on your build tools version\n#\n#       targetSdkVersion = 23        // depending on your sdk version\n#     }\n#\n# If you want to enable debugging ijkplayer(native modules) on Android Studio 2.2+: (experimental)\n#     sh android/patch-debugging-with-lldb.sh armv7a\n#     Install Android Studio 2.2(+)\n#     Preference -\u003e Android SDK -\u003e SDK Tools\n#     Select (LLDB, NDK, Android SDK Build-tools,Cmake) and install\n#     Open an existing Android Studio project\n#     Select android/ijkplayer\n#     Sync Project with Gradle Files\n#     Run -\u003e Edit Configurations -\u003e Debugger -\u003e Symbol Directories\n#     Add \"ijkplayer-armv7a/.externalNativeBuild/ndkBuild/release/obj/local/armeabi-v7a\" to Symbol Directories\n#     Run -\u003e Debug 'ijkplayer-example'\n#     if you want to reverse patches:\n#     sh patch-debugging-with-lldb.sh reverse armv7a\n#\n# Eclipse: (obselete)\n#     File -\u003e New -\u003e Project -\u003e Android Project from Existing Code\n#     Select android/ and import all project\n#     Import appcompat-v7\n#     Import preference-v7\n#\n# Gradle\n#     cd ijkplayer\n#     gradle\n\n```\n\n\n### Build iOS\n```\ngit clone https://github.com/Bilibili/ijkplayer.git ijkplayer-ios\ncd ijkplayer-ios\ngit checkout -B latest k0.8.8\n\n./init-ios.sh\n\ncd ios\n./compile-ffmpeg.sh clean\n./compile-ffmpeg.sh all\n\n# Demo\n#     open ios/IJKMediaDemo/IJKMediaDemo.xcodeproj with Xcode\n# \n# Import into Your own Application\n#     Select your project in Xcode.\n#     File -\u003e Add Files to ... -\u003e Select ios/IJKMediaPlayer/IJKMediaPlayer.xcodeproj\n#     Select your Application's target.\n#     Build Phases -\u003e Target Dependencies -\u003e Select IJKMediaFramework\n#     Build Phases -\u003e Link Binary with Libraries -\u003e Add:\n#         IJKMediaFramework.framework\n#\n#         AudioToolbox.framework\n#         AVFoundation.framework\n#         CoreGraphics.framework\n#         CoreMedia.framework\n#         CoreVideo.framework\n#         libbz2.tbd\n#         libz.tbd\n#         MediaPlayer.framework\n#         MobileCoreServices.framework\n#         OpenGLES.framework\n#         QuartzCore.framework\n#         UIKit.framework\n#         VideoToolbox.framework\n#\n#         ... (Maybe something else, if you get any link error)\n# \n```\n\n\n### Support (支持) ###\n- Please do not send e-mail to me. Public technical discussion on github is preferred.\n- 请尽量在 github 上公开讨论[技术问题](https://github.com/bilibili/ijkplayer/issues)，不要以邮件方式私下询问，恕不一一回复。\n\n\n### License\n\n```\nCopyright (c) 2017 Bilibili\nLicensed under LGPLv2.1 or later\n```\n\nijkplayer required features are based on or derives from projects below:\n- LGPL\n  - [FFmpeg](http://git.videolan.org/?p=ffmpeg.git)\n  - [libVLC](http://git.videolan.org/?p=vlc.git)\n  - [kxmovie](https://github.com/kolyvan/kxmovie)\n  - [soundtouch](http://www.surina.net/soundtouch/sourcecode.html)\n- zlib license\n  - [SDL](http://www.libsdl.org)\n- BSD-style license\n  - [libyuv](https://code.google.com/p/libyuv/)\n- ISC license\n  - [libyuv/source/x86inc.asm](https://code.google.com/p/libyuv/source/browse/trunk/source/x86inc.asm)\n\nandroid/ijkplayer-exo is based on or derives from projects below:\n- Apache License 2.0\n  - [ExoPlayer](https://github.com/google/ExoPlayer)\n\nandroid/example is based on or derives from projects below:\n- GPL\n  - [android-ndk-profiler](https://github.com/richq/android-ndk-profiler) (not included by default)\n\nios/IJKMediaDemo is based on or derives from projects below:\n- Unknown license\n  - [iOS7-BarcodeScanner](https://github.com/jpwiddy/iOS7-BarcodeScanner)\n\nijkplayer's build scripts are based on or derives from projects below:\n- [gas-preprocessor](http://git.libav.org/?p=gas-preprocessor.git)\n- [VideoLAN](http://git.videolan.org)\n- [yixia/FFmpeg-Android](https://github.com/yixia/FFmpeg-Android)\n- [kewlbear/FFmpeg-iOS-build-script](https://github.com/kewlbear/FFmpeg-iOS-build-script) \n\n### Commercial Use\nijkplayer is licensed under LGPLv2.1 or later, so itself is free for commercial use under LGPLv2.1 or later\n\nBut ijkplayer is also based on other different projects under various licenses, which I have no idea whether they are compatible to each other or to your product.\n\n[IANAL](https://en.wikipedia.org/wiki/IANAL), you should always ask your lawyer for these stuffs before use it in your product.\n"
  },
  {
    "repo": "php/php-src",
    "content": "The PHP Interpreter\n===================\n\nThis is the github mirror of the official PHP repository located at\nhttps://git.php.net.\n\n[![Build Status](https://secure.travis-ci.org/php/php-src.svg?branch=master)](http://travis-ci.org/php/php-src)\n[![Build status](https://ci.appveyor.com/api/projects/status/meyur6fviaxgdwdy?svg=true)](https://ci.appveyor.com/project/php/php-src)\n\nPull Requests\n=============\nPHP accepts pull requests via github. Discussions are done on github, but\ndepending on the topic can also be relayed to the official PHP developer\nmailing list internals@lists.php.net.\n\nNew features require an RFC and must be accepted by the developers.\nSee https://wiki.php.net/rfc and https://wiki.php.net/rfc/voting for more\ninformation on the process.\n\nBug fixes **do not** require an RFC, but require a bugtracker ticket. Always\nopen a ticket at https://bugs.php.net and reference the bug id using #NNNNNN.\n\n    Fix #55371: get_magic_quotes_gpc() throws deprecation warning\n\n    After removing magic quotes, the get_magic_quotes_gpc function caused\n    a deprecate warning. get_magic_quotes_gpc can be used to detected\n    the magic_quotes behavior and therefore should not raise a warning at any\n    time. The patch removes this warning\n\nWe do not merge pull requests directly on github. All PRs will be\npulled and pushed through https://git.php.net.\n\n\nGuidelines for contributors\n===========================\n- [CODING_STANDARDS](/CODING_STANDARDS)\n- [README.GIT-RULES](/README.GIT-RULES)\n- [README.MAILINGLIST_RULES](/README.MAILINGLIST_RULES)\n- [README.RELEASE_PROCESS](/README.RELEASE_PROCESS)\n\n## Testing\n\nTo run tests the `make test` is used after successful compilation of the sources.\n\nSee [Creating new test files](https://qa.php.net/write-test.php) chapter for more\ninformation about testing.\n"
  },
  {
    "repo": "wg/wrk",
    "content": "# wrk - a HTTP benchmarking tool\n\n  wrk is a modern HTTP benchmarking tool capable of generating significant\n  load when run on a single multi-core CPU. It combines a multithreaded\n  design with scalable event notification systems such as epoll and kqueue.\n\n  An optional LuaJIT script can perform HTTP request generation, response\n  processing, and custom reporting. Details are available in SCRIPTING and\n  several examples are located in [scripts/](scripts/).\n\n## Basic Usage\n\n    wrk -t12 -c400 -d30s http://127.0.0.1:8080/index.html\n\n  This runs a benchmark for 30 seconds, using 12 threads, and keeping\n  400 HTTP connections open.\n\n  Output:\n\n    Running 30s test @ http://127.0.0.1:8080/index.html\n      12 threads and 400 connections\n      Thread Stats   Avg      Stdev     Max   +/- Stdev\n        Latency   635.91us    0.89ms  12.92ms   93.69%\n        Req/Sec    56.20k     8.07k   62.00k    86.54%\n      22464657 requests in 30.00s, 17.76GB read\n    Requests/sec: 748868.53\n    Transfer/sec:    606.33MB\n\n## Command Line Options\n\n    -c, --connections: total number of HTTP connections to keep open with\n                       each thread handling N = connections/threads\n\n    -d, --duration:    duration of the test, e.g. 2s, 2m, 2h\n\n    -t, --threads:     total number of threads to use\n\n    -s, --script:      LuaJIT script, see SCRIPTING\n\n    -H, --header:      HTTP header to add to request, e.g. \"User-Agent: wrk\"\n\n        --latency:     print detailed latency statistics\n\n        --timeout:     record a timeout if a response is not received within\n                       this amount of time.\n\n## Benchmarking Tips\n\n  The machine running wrk must have a sufficient number of ephemeral ports\n  available and closed sockets should be recycled quickly. To handle the\n  initial connection burst the server's listen(2) backlog should be greater\n  than the number of concurrent connections being tested.\n\n  A user script that only changes the HTTP method, path, adds headers or\n  a body, will have no performance impact. Per-request actions, particularly\n  building a new HTTP request, and use of response() will necessarily reduce\n  the amount of load that can be generated.\n\n## Acknowledgements\n\n  wrk contains code from a number of open source projects including the\n  'ae' event loop from redis, the nginx/joyent/node.js 'http-parser',\n  and Mike Pall's LuaJIT. Please consult the NOTICE file for licensing\n  details.\n\n## Cryptography Notice\n\n  This distribution includes cryptographic software. The country in\n  which you currently reside may have restrictions on the import,\n  possession, use, and/or re-export to another country, of encryption\n  software. BEFORE using any encryption software, please check your\n  country's laws, regulations and policies concerning the import,\n  possession, or use, and re-export of encryption software, to see if\n  this is permitted. See \u003chttp://www.wassenaar.org/\u003e for more\n  information.\n\n  The U.S. Government Department of Commerce, Bureau of Industry and\n  Security (BIS), has classified this software as Export Commodity\n  Control Number (ECCN) 5D002.C.1, which includes information security\n  software using or performing cryptographic functions with symmetric\n  algorithms. The form and manner of this distribution makes it\n  eligible for export under the License Exception ENC Technology\n  Software Unrestricted (TSU) exception (see the BIS Export\n  Administration Regulations, Section 740.13) for both object code and\n  source code.\n"
  },
  {
    "repo": "SamyPesse/How-to-Make-a-Computer-Operating-System",
    "content": "How to Make a Computer Operating System\n=======================================\n\nOnline book about how to write a computer operating system in C/C++ from scratch.\n\n**Caution**: This repository is a remake of my old course. It was written several years ago [as one of my first projects when I was in High School](https://github.com/SamyPesse/devos), I'm still refactoring some parts. The original course was in French and I'm not an English native. I'm going to continue and improve this course in my free-time.\n\n**Book**: An online version is available at [http://samypesse.gitbooks.io/how-to-create-an-operating-system/](http://samypesse.gitbooks.io/how-to-create-an-operating-system/) (PDF, Mobi and ePub). It was generated using [GitBook](https://www.gitbook.com/).\n\n**Source Code**: All the system source code will be stored in the [src](https://github.com/SamyPesse/How-to-Make-a-Computer-Operating-System/tree/master/src) directory. Each step will contain links to the different related files.\n\n**Contributions**: This course is open to contributions, feel free to signal errors with issues or directly correct the errors with pull-requests.\n\n**Questions**: Feel free to ask any questions by adding issues or commenting sections.\n\nYou can follow me on Twitter [@SamyPesse](https://twitter.com/SamyPesse) or [GitHub](https://github.com/SamyPesse).\n\n### What kind of OS are we building?\n\nThe goal is to build a very simple UNIX-based operating system in C++, not just a \"proof-of-concept\". The OS should be able to boot, start a userland shell, and be extensible.\n\n![Screen](./preview.png)\n"
  },
  {
    "repo": "ggreer/the_silver_searcher",
    "content": "# The Silver Searcher\n\nA code searching tool similar to `ack`, with a focus on speed.\n\n[![Build Status](https://travis-ci.org/ggreer/the_silver_searcher.svg?branch=master)](https://travis-ci.org/ggreer/the_silver_searcher)\n\n[![Floobits Status](https://floobits.com/ggreer/ag.svg)](https://floobits.com/ggreer/ag/redirect)\n\n[![#ag on Freenode](https://img.shields.io/badge/Freenode-%23ag-brightgreen.svg)](https://webchat.freenode.net/?channels=ag)\n\nDo you know C? Want to improve ag? [I invite you to pair with me](http://geoff.greer.fm/2014/10/13/help-me-get-to-ag-10/).\n\n\n## What's so great about Ag?\n\n* It is an order of magnitude faster than `ack`.\n* It ignores file patterns from your `.gitignore` and `.hgignore`.\n* If there are files in your source repo you don't want to search, just add their patterns to a `.ignore` file. (\\*cough\\* `*.min.js` \\*cough\\*)\n* The command name is 33% shorter than `ack`, and all keys are on the home row!\n\nAg is quite stable now. Most changes are new features, minor bug fixes, or performance improvements. It's much faster than Ack in my benchmarks:\n\n    ack test_blah ~/code/  104.66s user 4.82s system 99% cpu 1:50.03 total\n\n    ag test_blah ~/code/  4.67s user 4.58s system 286% cpu 3.227 total\n\nAck and Ag found the same results, but Ag was 34x faster (3.2 seconds vs 110 seconds). My `~/code` directory is about 8GB. Thanks to git/hg/ignore, Ag only searched 700MB of that.\n\nThere are also [graphs of performance across releases](http://geoff.greer.fm/ag/speed/).\n\n## How is it so fast?\n\n* Ag uses [Pthreads](https://en.wikipedia.org/wiki/POSIX_Threads) to take advantage of multiple CPU cores and search files in parallel.\n* Files are `mmap()`ed instead of read into a buffer.\n* Literal string searching uses [Boyer-Moore strstr](https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string_search_algorithm).\n* Regex searching uses [PCRE's JIT compiler](http://sljit.sourceforge.net/pcre.html) (if Ag is built with PCRE \u003e=8.21).\n* Ag calls `pcre_study()` before executing the same regex on every file.\n* Instead of calling `fnmatch()` on every pattern in your ignore files, non-regex patterns are loaded into arrays and binary searched.\n\nI've written several blog posts showing how I've improved performance. These include how I [added pthreads](http://geoff.greer.fm/2012/09/07/the-silver-searcher-adding-pthreads/), [wrote my own `scandir()`](http://geoff.greer.fm/2012/09/03/profiling-ag-writing-my-own-scandir/), [benchmarked every revision to find performance regressions](http://geoff.greer.fm/2012/08/25/the-silver-searcher-benchmarking-revisions/), and profiled with [gprof](http://geoff.greer.fm/2012/02/08/profiling-with-gprof/) and [Valgrind](http://geoff.greer.fm/2012/01/23/making-programs-faster-profiling/).\n\n\n## Installing\n\n### macOS\n\n    brew install the_silver_searcher\n\nor\n\n    port install the_silver_searcher\n\n\n### Linux\n\n* Ubuntu \u003e= 13.10 (Saucy) or Debian \u003e= 8 (Jessie)\n\n        apt-get install silversearcher-ag\n* Fedora 21 and lower\n\n        yum install the_silver_searcher\n* Fedora 22+\n\n        dnf install the_silver_searcher\n* RHEL7+\n\n        yum install epel-release.noarch the_silver_searcher\n* Gentoo\n\n        emerge -a sys-apps/the_silver_searcher\n* Arch\n\n        pacman -S the_silver_searcher\n\n* Slackware\n\n        sbopkg -i the_silver_searcher\n\n* openSUSE:\n\n        zypper install the_silver_searcher\n\n* CentOS:\n        \n        yum install the_silver_searcher\n\n* SUSE Linux Enterprise: Follow [these simple instructions](https://software.opensuse.org/download.html?project=utilities\u0026package=the_silver_searcher).\n\n\n### BSD\n\n* FreeBSD\n\n        pkg install the_silver_searcher\n* OpenBSD/NetBSD\n\n        pkg_add the_silver_searcher\n\n### Windows\n\n* Win32/64\n\n  Unofficial daily builds are [available](https://github.com/k-takata/the_silver_searcher-win32).\n  \n* Chocolatey\n\n        choco install ag\n* MSYS2\n\n        pacman -S mingw-w64-{i686,x86_64}-ag\n* Cygwin\n\n  Run the relevant [`setup-*.exe`](https://cygwin.com/install.html), and select \"the\\_silver\\_searcher\" in the \"Utils\" category.\n\n## Building from source\n\n### Building master\n\n1. Install dependencies (Automake, pkg-config, PCRE, LZMA):\n    * macOS:\n\n            brew install automake pkg-config pcre xz\n        or\n\n            port install automake pkgconfig pcre xz\n    * Ubuntu/Debian:\n\n            apt-get install -y automake pkg-config libpcre3-dev zlib1g-dev liblzma-dev\n    * Fedora:\n\n            yum -y install pkgconfig automake gcc zlib-devel pcre-devel xz-devel\n    * CentOS:\n\n            yum -y groupinstall \"Development Tools\"\n            yum -y install pcre-devel xz-devel zlib-devel\n    * openSUSE:\n\n            zypper source-install --build-deps-only the_silver_searcher\n\n    * Windows: It's complicated. See [this wiki page](https://github.com/ggreer/the_silver_searcher/wiki/Windows).\n2. Run the build script (which just runs aclocal, automake, etc):\n\n        ./build.sh\n\n   On Windows (inside an msys/MinGW shell):\n\n        make -f Makefile.w32\n3. Make install:\n\n        sudo make install\n\n\n### Building a release tarball\n\nGPG-signed releases are available [here](http://geoff.greer.fm/ag).\n\nBuilding release tarballs requires the same dependencies, except for automake and pkg-config. Once you've installed the dependencies, just run:\n\n    ./configure\n    make\n    make install\n\nYou may need to use `sudo` or run as root for the make install.\n\n\n## Editor Integration\n\n### Vim\n\nYou can use Ag with [ack.vim](https://github.com/mileszs/ack.vim) by adding the following line to your `.vimrc`:\n\n    let g:ackprg = 'ag --nogroup --nocolor --column'\n\nor:\n\n    let g:ackprg = 'ag --vimgrep'\n\nWhich has the same effect but will report every match on the line.\n\n### Emacs\n\nYou can use [ag.el][] as an Emacs front-end to Ag. See also: [helm-ag].\n\n[ag.el]: https://github.com/Wilfred/ag.el\n[helm-ag]: https://github.com/syohex/emacs-helm-ag\n\n### TextMate\n\nTextMate users can use Ag with [my fork](https://github.com/ggreer/AckMate) of the popular AckMate plugin, which lets you use both Ack and Ag for searching. If you already have AckMate you just want to replace Ack with Ag, move or delete `\"~/Library/Application Support/TextMate/PlugIns/AckMate.tmplugin/Contents/Resources/ackmate_ack\"` and run `ln -s /usr/local/bin/ag \"~/Library/Application Support/TextMate/PlugIns/AckMate.tmplugin/Contents/Resources/ackmate_ack\"`\n\n## Other stuff you might like\n\n* [Ack](https://github.com/petdance/ack2) - Better than grep. Without Ack, Ag would not exist.\n* [ack.vim](https://github.com/mileszs/ack.vim)\n* [Exuberant Ctags](http://ctags.sourceforge.net/) - Faster than Ag, but it builds an index beforehand. Good for *really* big codebases.\n* [Git-grep](http://git-scm.com/docs/git-grep) - As fast as Ag but only works on git repos.\n* [ripgrep](https://github.com/BurntSushi/ripgrep)\n* [Sack](https://github.com/sampson-chen/sack) - A utility that wraps Ack and Ag. It removes a lot of repetition from searching and opening matching files.\n"
  },
  {
    "repo": "kripken/emscripten",
    "content": "![emscripten logo](media/switch_logo.png)\n\n[![Build Status](https://travis-ci.org/kripken/emscripten.svg?branch=incoming)](https://travis-ci.org/kripken/emscripten/branches)\n[![CircleCI](https://circleci.com/gh/kripken/emscripten.svg?style=svg)](https://circleci.com/gh/kripken/emscripten/tree/incoming)\n\nEmscripten is an [LLVM](https://en.wikipedia.org/wiki/LLVM)-to-JavaScript compiler. It takes LLVM bitcode - which can be generated\nfrom C/C++, using `llvm-gcc` (DragonEgg) or `clang`, or any other language that can be\nconverted into LLVM - and compiles that into JavaScript, which can be run on the web (or\nanywhere else JavaScript can run).\n\nLinks to **demos**, **tutorial**, **FAQ**, etc: \u003chttps://github.com/kripken/emscripten/wiki\u003e\n\nMain project page: \u003chttp://emscripten.org\u003e\n\nLicense\n-------\n\nEmscripten is available under 2 licenses, the MIT license and the\nUniversity of Illinois/NCSA Open Source License.\n\nBoth are permissive open source licenses, with little if any\npractical difference between them.\n\nThe reason for offering both is that (1) the MIT license is\nwell-known, while (2) the University of Illinois/NCSA Open Source\nLicense allows Emscripten's code to be integrated upstream into\nLLVM, which uses that license, should the opportunity arise.\n\nSee `LICENSE` for the full content of the licenses.\n"
  },
  {
    "repo": "julycoding/The-Art-Of-Programming-By-July",
    "content": "## About\n\n**July的新书《编程之法：面试和算法心得》纸质版在本github上的基础上做了极大彻底的改进、优化，无论是完整度、还是最新度、或质量上，都远非博客、github所能相比。换言之，新书《编程之法》的质量远高于博客、github。**\n\n**此外，散落在网上其他任何地方的“编程之法”电子材料均是盗版自本github，更无质量可言。所以，July只唯一推荐《编程之法》纸质版。**\n\n《编程之法》纸质版已于2015年10月14日陆续开卖！目前，京东、当当、亚马逊等各大网店均已有现货销售。比如京东：http://item.jd.com/11786791.html 。\n\n另，新书《编程之法：面试和算法心得》的勘误在此文最末：http://blog.csdn.net/v_july_v/article/details/49302193\n\n看过[结构之法算法之道blog](http://blog.csdn.net/v_july_v)的朋友可能知道，从2010年10月起，[July](http://weibo.com/julyweibo) 开始整理一个微软面试100题的系列，他在整理这个系列的过程当中，越来越强烈的感觉到，可以从那100题中精选一些更为典型的题，每一题详细阐述成章，不断优化，于此，便成了程序员编程艺术系列。\n\n原编程艺术系列从2011年4月至今，写了42个编程问题，在创作的过程当中，得到了很多朋友的支持，特别是博客上随时都会有朋友不断留言，或提出改进建议，或show出自己的思路、代码，或指正bug。为更好的改进、优化、增补编程艺术系列，特把博客上的这个**程序员编程艺术系列和博客内其它部分经典文章**同步到此，成立本项目。\n\n若发现任何问题、错误、bug，或可以优化的每一段代码，欢迎随时pull request或发issue反馈，thanks。\n\nupdate\n\nJuly于2015年正式创业，任七月在线创始人兼CEO，公司官网为：https://www.julyedu.com/ ，致力于培养100万AI人才。\n\n## Start Reading\n * [中文目录](ebook/zh/Readme.md) Enhancement in progress\n * [English Contents](ebook/en/Readme.md) Translation in progress\n\n\n## How To Contribute\n * 邀请大家帮忙把github上的文章导出到word上，欢迎到这里认领：https://github.com/julycoding/The-Art-Of-Programming-By-July/issues/337 」\n * 一章一章的测试所有代码，指正 bug，修正错误。 「必选，可到这里认领：https://github.com/julycoding/The-Art-Of-Programming-By-July/issues/210 」\n * 优化原文章上的C/C++ 代码，优化后的代码可以放到[ebook/code](ebook/code/)文件夹内，并注意代码命名规范的问题：https://github.com/julycoding/The-Art-Of-Programming-By-July/issues/234 。 「必选」\n * 添加其它语言如Java、python、go 的代码，放在[ebook/code](ebook/code/)文件夹内，同样如上，注意代码命名规范的问题。 「可选」\n * 重绘所有的图片：https://github.com/julycoding/The-Art-Of-Programming-by-July/issues/80\n * 翻译成英文版，参考[中文目录](ebook/zh/Readme.md)，把翻译后的文章编辑到这[English Version](ebook/en/Readme.md),注：不必逐字翻译，精简大气即可（如有兴趣翻译，请到这里领取感兴趣的章节翻译：https://github.com/julycoding/The-Art-Of-Programming-by-July/issues/84 )\n * 自己主导续写新的章节；\n * 任何你想做的事情，包括痛批你觉得写的烂的章节，所有你的意见都将改进此系列。\n\n你可以做以上任何一件或几件事情，如遇到任何问题或疑惑，咱们可以随时讨论：\n\u003chttps://github.com/julycoding/The-Art-Of-Programming-by-July/issues?state=open\u003e。\n「如不知如何在github上提交及同步作者的更新，可参考此文：http://www.cnblogs.com/rubylouvre/archive/2013/01/24/2874694.html 」\n\n## Code Style\n本项目暂约定以下代码风格(不断逐条添加中)：\n关于空格\n- 所有代码使用4个空格缩进\n- 运算符后使用一个空格\n- \",\" 和for循环语句中的\";\" 后面跟上一个空格\n- 条件、分支保留字，如 if for while else switch 后留出一个空格\n- \"[]\", \".\"和\"-\u003e\" 前后不留空格\n - 用空行把大块代码分成逻辑上的“段落\n \n关于括号 \n- 大括号另起一行\n- 即便只有一行代码也加大括号\n - C 指针中的指针符靠近类型名，如写成int* p，而不写成int *p\n \n关于标点\n- 中文表述，使用中文全角的标点符号，如：（）、。，？\n- 数学公式（包括文中混排的公式）和英文代码，使用英文半角的标点符号，如：(),.?…\n\n关于注释\n- 注释统一用中文\n- 尽量统一用\"//\"，一般不用\"/\\*...\\*/\"\n\n关于命名\n- 类名为大写字母开头的单词组合\n- 函数名比较长，由多个单词组成的，每个单词的首字母大写，如int MaxSubArray()；函数名很短，由一个单词组成，首字母小写，比如int swap()\n- 变量名比较长，由多个单词组成的，首个单词的首字母小写，后面紧跟单词的首字母大写，如maxEnd；变量名很短，由一个单词组成，首字母小写，如left\n- 变量尽量使用全名，能够描述所要实现的功能，如 highestTemprature；对于已经公认了的写法才使用缩写，如 tmp mid prev next\n- 变量名能“望文生义”，如v1, v2不如area, height\n- 常量的命名都是大写字母的单词，之间用下划线隔开，比如MY_CONSTANT\n- il \u003c 4384 和 inputLength \u003c MAX_INPUT_LENGTH，后一种写法更好\n\n - 一个函数只专注做一件事\n - 时间复杂度小写表示，如O(nlogn)，而不写成O(N*logN)\n - 正文中绝大部分采用C实现，少量C++代码，即以C为主，但不去刻意排斥回避C++；\n \n关于的地得\n- 形容词（代词） + 的 + 名词，例如：我的小苹果\n- 副词 + 地 + 动词，例如：慢慢地走\n- 动词 + 得 + 副词，例如：走得很快\n\n关于参考文献\n- 格式：主要责任者.书名〔文献类型标识 ] .其他责任者.版本.出版地：出版者，出版年.文献数量.丛编项.附注项.文献标准编号。例子：1 刘少奇.论共产党员的修养.修订 2 版.北京：人民出版社，1962.76 页.\n - 专业术语\n- 统一一律用“树结点”，而不是“树节点”。\n- 用左子树、右子树表示树的左右子树没问题，但是否用左孩子、右孩子表示树或子树的左右结点？\n - ..\n - 此外，更多C++ 部分可参考Google C++ Style Guide，中文版见：http://zh-google-styleguide.readthedocs.org/en/latest/contents/ ；\n\n有何问题或补充意见，咱们可以随时到这里讨论：https://github.com/julycoding/The-Art-Of-Programming-By-July/issues/81 。\n\n## Ver Note\n - 2010年10月11日，在CSDN上正式开博，感谢博客上所有读者的访问、浏览、关注、支持、留言、评论、批评、指正；\n - 2011年1月，在学校的时候，第一家出版社联系出书，因“时机未到，尚需积累”的原因婉拒，随后第二家、第三家出版社陆续联系，因总感觉写书的时机还没到，一律婉拒；\n - 2011年10月， 当时在图灵教育的杨海玲老师（现在人民邮电信息技术分社）再度联系出书，再度认为“时机未到”；\n - 2013年12月30日，本项目在众多朋友的努力之下，冲到github流行趋势排行榜全球第一，自己也在众人助推下冲到全球开发者第一。\n - 2014年1月18日，想通了一件事：如果什么都不去尝试，那么将年年一事无成，所以元旦一过，便正式确认今2014年之内要把拖了近3年之久的书出版出来；\n - 2013年12月-2014年3月，本github的Contributors 转移结构之法算法之道blog的部分经典文章到本github上，感谢这近100位Contributors，包括但不限于：\n- Boshen（除我之外，贡献本github的次数最多）\n- sallen450\n- marchtea（专门为本github书稿弄了一个HTML网页）\n- nateriver520（劝我把书稿放在github上，才有了本github）\n - 2014年3月，通读全部文章，修正明显错误，并邀请部分朋友review本github上的全部文章，包括cherry、王威扬、邬勇、高增琪、武博文、杨忠宝等；\n \n2014年4月\n- 整个4月，精简篇幅，调整目录，Contributors 贡献其它语言代码，并翻译部分文章；\n- 4月25日，跟人民邮电出版社信息技术分社签订合同，书名暂定《程序员编程艺术：面试和算法心得》，有更好的名字再替换。\n - 2014年5月，逐章逐节逐行逐字优化文字描述，测试重写优化每一段每一行每一个代码，确定代码基本风格；\n \n2014年6月\n- 第一周，压缩篇幅，宁愿量少，但求质精；\n- 第二周，全面 review；\n- 第三周，本github的部分Contributors 把全部文章从github转到word上，这部分contributors 包括包括：zhou1989、qiwsir、DogK、x140yu、ericxk、zhanglin0129、idouba.net、gaohua、kelvinkuo等；\n- 第四周，继续在Word 上做出最后彻底的改进，若未发现bug或pull request，本github将暂不再改动；\n- 6月30日，与出版社约定的交稿日期延期，理由：目前版本不是所能做到的最好的版本。\n2014年7月，邀请部分好友进行第一轮审稿，包括曹鹏、邹伟、林奔、王婷、何欢，其中，曹鹏重写优化了部分代码。此外，葛立娜对书稿中的语言描述做了不少改进；\n \n2014年8月\n- 8月上旬，新增KMP一节内容；\n- 8月下旬，重点修改SVM一节内容；\n\n2014年9月\n- 9月上旬，和一些朋友一起重绘稿件中的部分图和公式，这部分朋友包括顾运（@陈笙）、mastermay、在山东大学读研二的丰俊丙、厦门大学电子工程系陈友和等等；\n- 9月下旬，再度邀请另一部分好友进行第二轮审稿，包括许利杰、王亮、陈赢、李祥老师、litaoye等，并在微博上公开征集部分读者审稿，包括李元超、刘琪等等；\n\n2014年10月\n- 10月8日起，开始一章一章陆续交Word 稿给出版社初审\n- 10月9日，第一章、字符串完成修改；\n- 10月10日，第二章、数组完成修改；\n- 10月22日，第三章、树完成修改；\n\n2014年11月\n- 11月5日，第三章、树完成第二版修改，主要修正部分图片、公式、语言描述的错误；\n\n2014年12月\n- 12月1日，第四章、查找完成修改。至此，前4 章的修改稿交付出版社。 \n- 12月8日，第五章、动态规划完成修改，等出版社反馈中。一个人坚持有点枯燥。\n- 12月31日，第六章仍未修改完。\n\n2015年1月\n- 1月12日凌晨，第六章、海量数据处理完成修改，交付出版社。\n\n2015年4月\n- 4月27日凌晨，交完第七章初稿，接下来编辑老师反馈，我修改审阅反馈稿。且书名由原来的《程序员编程艺术：面试和算法心得》暂时改为《编程之法：面试和算法心得》。\n\n2015年5月\n- 5月2日，开始写书的前言，大致是：为何要写这本书，写的过程是怎样的；这是本什么书，有何特色，内容是什么，为什么这么写；写给谁看，怎么看更好。当然我还会加一些自己觉得比较个性化的内容。\n- 5月5日，审阅完编辑老师的第一章反馈，并合并。\n- 5月6日，审阅完第二章的一半。海玲姐两位老师给出了大量细致、详尽的修改建议，包括文字表述、语言表达、标点符号、字体格式、出版规范，尤其是正斜体、大小写、上下角。\n- 5月15日，和海玲姐审完第一、二章，标点、术语、表述、逻辑、图片、代码等一切细节。书稿进入一审阶段。\n\n2015年6月\n- 6月28日，经过反复修改、确认，书稿第一、二、三章基本定稿。 \n\n2015年7月\n- 7月10日，书稿全部七章基本定稿，即将进入二审。\n- 7月23日，补齐前言、封底、内容提要、邀请曹鹏、邹伟两位博士写推荐序，书稿进入二审，出版社重绘全部图片和公式。\n\n2015年8月\n- 8月6日，三审结束。书稿取得阶段性的胜利。 \n- 8月下旬，发稿审批。\n\n2015年9月\n- 9月上旬，排版校对，出胶片、印刷、装订成书 \n- 9月21日，几经易稿，终于敲定新书封面。\n- 9月22日，开始印刷。\n\n2015年10月\n- 进入10月份，万众期待的《编程之法》，终于终于要来了！\n- 10月13日晚，终于拿到第一批样书。\n- 10月14日下午三点半，我的新书《编程之法》终于在异步社区上首发开卖！\n- 10月28日，新书正式上架京东。目前京东、当当、亚马逊等各大网店均已有现货销售。\n\n## Contributors\n感谢所有贡献的朋友：https://github.com/julycoding/The-Art-Of-Programming-by-July/graphs/contributors ，因为有各位之力，本项目才能于13年年底冲到github流行趋势排行榜全球第一。非常期待你的加入，thanks。\n\n同时，欢迎加入《编程之法》讨论交流QQ群：74631723，需要写验证信息。\n\n孤军奋战的时代早已远去，我们只有团结起来，才能帮助到更多人。[@研究者July](http://weibo.com/julyweibo)，始于二零一三年十二月十四日。\n\n## Expressing Thanks\n\n- 感谢我博客上所有读者的访问、浏览、关注、支持、留言、评论、批评、指正，仅以本书献给我博客的所有读者。\n- 感谢Boshen、sallen450、marchtea、nateriver520等朋友帮我把博客上的部分经典文章移到GitHub上。\n- 感谢zhou1989、qiwsir、DogK、x140yu、ericxk、zhanglin0129、idouba.net、gaohua、kelvinkuo等朋友帮我把GitHub上的文章转为Word文件。\n- 感谢顾运、mastermay、丰俊丙、陈友和等朋友帮忙重绘书中的部分图和重录书中的部分公式。\n- 感谢cherry、王威扬、邬勇、高增琪、武博文、杨忠宝、葛立娜、林奔、王婷、何欢、许利杰JerryLead、王亮、陈赢、李祥老师、litaoye、李元超、刘琪、weedge、Frankie等众多朋友帮忙审校书稿。\n- 特别感谢曹鹏、邹伟两位博士。感谢他们非常认真细致地看完了全部书稿，给出了非常多的建设性意见，并为本书作序。\n- 最后，再次感谢杨海玲老师以及出版社的编辑们。感谢杨海玲老师给出了大量细致的修改建议，并且非常耐心地与我一轮一轮讨论和修改书稿。\n\n感谢以上诸位，正因为他们的帮助，纸书《编程之法：面试和算法心得》的质量才不断提升，从而给广大读者呈现的是更好的作品。\n\n## Copyright\n本电子书的版权属于July 本人，严禁他人出版或用于商业用途，违者必究法律责任。July、二零一四年五月十一日晚。\n\n## July' PDF\nJuly’ PDF\n - 《支持向量机通俗导论（理解SVM的三层境界）》Latex排版精细版：http://vdisk.weibo.com/s/zrFL6OXKgnlcp ；Latex版本②：https://raw.githubusercontent.com/liuzheng712/Intro2SVM/master/Intro2SVM.pdf 。\n - 原《程序员编程艺术第一~三十七章PDF》：http://download.csdn.net/detail/v_july_v/6694053 ，本github上的文章已经对此PDF进行了极大的优化和改进。\n - 《微软面试100题系列之PDF》：http://download.csdn.net/detail/v_july_v/4583815\n - 《十五个经典算法研究与总结之PDF》：http://download.csdn.net/detail/v_july_v/4478027\n - 编程艺术HTML网页版：http://taop.marchtea.com/\n - 截止到2014年12.9日，结构之法算法之道blog所有155篇博文集锦CHM文件下载地址：http://pan.baidu.com/s/1gdrJndp\n \nJuly团队高校讲座PPT\n - 2014年4月29日《武汉华科大第5次面试\u0026算法讲座PPT》：http://pan.baidu.com/s/1hqh1E9e ；\n - 2014年9月3日西电第8次面试\u0026算法讲座视频：http://v.youku.com/v_show/id_XNzc2MDYzNDg4.html ；PPT：http://pan.baidu.com/s/1pJ9HFqb ；\n - 北京10月机器学习班的所有上课PPT：http://yun.baidu.com/share/home?uk=4214456744\u0026view=share#category/type=0；\n- July新书初稿的4个PDF\n - B树的PDF：http://yun.baidu.com/s/1jGwup5k ；\n - 海量数据处理的PDF：http://yun.baidu.com/s/1dDreICL ；\n - 支持向量机的PDF：http://yun.baidu.com/s/1ntwof7j ；\n - KMP的PDF：http://yun.baidu.com/s/1eQel3PK ；\n \n七月在线经典课程\n - 《机器学习与量化交易项目班》：https://www.julyedu.com/course/getDetail/46 ，项目驱动、真实数据，市场上极为难得的项目班，16年9.24日开班。\n - 《kaggle案例实战班 [100%纯实战]》：https://www.julyedu.com/course/getDetail/54 ，本周六12.24日起在线直播\n - ¥2499包揽2018年全套AI课程和全年GPU：https://www.julyedu.com/vip/vip_18 ，包含机器学习、深度学习、迁移学习、强化学习、TensorFlow框架实战班等等..\n - 《机器学习 第八期》：https://www.julyedu.com/course/getDetail/65 ，BAT工业实战、kaggle案例，国内最好ML课程。\n - 七月在线「机器学习集训营」，线上线下项目实训，线下BAT专家面对面、手把手教学，尤为突出BAT级工业项目实战辅导 + 一对一面试求职辅导：https://www.julyedu.com/category/index/2/32 。且提供3个月GPU实战，更精讲面试常见考点。且第4期ML集训营4增加入学测评，增加每周在线考试，增加作业和考试1v1批改，增大ML DL内容的所占比重，更增加到了北上深广杭沈济郑成等9大线下点。\n \nJuly’ blog\n - CNN笔记：通俗理解卷积神经网络 http://blog.csdn.net/v_july_v/article/details/51812459\n  - 结构之法 算法之道ML/DL系列博客：http://blog.csdn.net/v_JULY_v/article/category/1061301\n - 教你从头到尾利用DL学梵高作画 ① CPU版教程：http://blog.csdn.net/v_july_v/article/details/52683959 ；② GPU版教程：http://blog.csdn.net/v_july_v/article/details/52658965\n - 学汪峰作词 MAC torch char-rnn 教程：https://ask.julyedu.com/question/7405\n - 基于torch学汪峰写歌词、聊天机器人、图像着色/生成、看图说话、字幕生成：http://blog.csdn.net/v_july_v/article/details/52796239\n - 教你从头到尾利用DQN自动玩flappy bird（全程命令提示，GPU+CPU版）：http://blog.csdn.net/v_july_v/article/details/52810219\n - beautiful soup和网络爬虫初步：http://mp.weixin.qq.com/s?__biz=MzI4MTQ2NjU5NA==\u0026mid=2247483666\u0026idx=1\u0026sn=af802b39d42d9073e24c086790457004\u0026chksm=eba9829fdcde0b8929ae724f0b41d49091c88fc1e07f697c5030516f33b1bcfcffdace83bcf7\u0026mpshare=1\u0026scene=23\u0026srcid=1102C9RJJ1RXE6bHwFQzLy6W#rd\n - 16年度最火课程TOP 10免费试听：https://www.julyedu.com/sale/christmas\n - 如何学习数据科学、机器学习与深度学习（附学习路线）：http://blog.csdn.net/v_july_v/article/details/54561427\n - 《机器学习集训营第5期》：http://www.julyedu.com/weekend/train5 ，学习机器学习的最佳选择，三个月挑战起薪四十万。\n - 国内首个AI题库陆续发布，「BAT机器学习面试1000题」：http://blog.csdn.net/column/details/17609.html\n - 迄今为止，这几篇是我见过阐述最为通俗易懂的文章：①KMP http://blog.csdn.net/v_july_v/article/details/7041827 ，②SVM http://blog.csdn.net/v_july_v/article/details/7624837 ，③CNN https://www.julyedu.com/question/big/kp_id/26/ques_id/2084 ，④RNN https://www.julyedu.com/question/big/kp_id/26/ques_id/1717 ，⑤LSTM https://www.julyedu.com/question/big/kp_id/26/ques_id/1851 ， ⑥物体检测 https://www.julyedu.com/question/big/kp_id/26/ques_id/2103 ， ⑦区块链 https://www.julyedu.com/question/big/kp_id/29/ques_id/2110 。看过无数资料，可能都不如这些好懂。 ​​​​\n- 持续更新..\n"
  },
  {
    "repo": "vim/vim",
    "content": "![Vim Logo](https://github.com/vim/vim/blob/master/runtime/vimlogo.gif)\n\n[![Build Status](https://travis-ci.org/vim/vim.svg?branch=master)](https://travis-ci.org/vim/vim)\n[![Coverage Status](https://codecov.io/gh/vim/vim/coverage.svg?branch=master)](https://codecov.io/gh/vim/vim?branch=master)\n[![Coverage Status](https://coveralls.io/repos/github/vim/vim/badge.svg?branch=master)](https://coveralls.io/github/vim/vim?branch=master)\n[![Appveyor Build status](https://ci.appveyor.com/api/projects/status/o2qht2kjm02sgghk?svg=true)](https://ci.appveyor.com/project/chrisbra/vim)\n[![Coverity Scan](https://scan.coverity.com/projects/241/badge.svg)](https://scan.coverity.com/projects/vim)\n[![Language Grade: C/C++](https://img.shields.io/lgtm/grade/cpp/g/vim/vim.svg?logo=lgtm\u0026logoWidth=18)](https://lgtm.com/projects/g/vim/vim/context:cpp)\n[![Debian CI](https://badges.debian.net/badges/debian/testing/vim/version.svg)](https://buildd.debian.org/vim)\n\n\n## What is Vim? ##\n\nVim is a greatly improved version of the good old UNIX editor Vi.  Many new\nfeatures have been added: multi-level undo, syntax highlighting, command line\nhistory, on-line help, spell checking, filename completion, block operations,\nscript language, etc.  There is also a Graphical User Interface (GUI)\navailable.  Still, Vi compatibility is maintained, those who have Vi \"in the\nfingers\" will feel at home.  See `runtime/doc/vi_diff.txt` for differences with\nVi.\n\nThis editor is very useful for editing programs and other plain text files.\nAll commands are given with normal keyboard characters, so those who can type\nwith ten fingers can work very fast.  Additionally, function keys can be\nmapped to commands by the user, and the mouse can be used.\n\nVim runs under MS-Windows (NT, 2000, XP, Vista, 7, 8, 10), Macintosh, VMS and\nalmost all flavours of UNIX.  Porting to other systems should not be very\ndifficult.  Older versions of Vim run on MS-DOS, MS-Windows 95/98/Me, Amiga\nDOS, Atari MiNT, BeOS, RISC OS and OS/2.  These are no longer maintained.\n\n\n## Distribution ##\n\nYou can often use your favorite package manager to install Vim.  On Mac and\nLinux a small version of Vim is pre-installed, you still need to install Vim\nif you want more features.\n\nThere are separate distributions for Unix, PC, Amiga and some other systems.\nThis `README.md` file comes with the runtime archive.  It includes the\ndocumentation, syntax files and other files that are used at runtime.  To run\nVim you must get either one of the binary archives or a source archive.\nWhich one you need depends on the system you want to run it on and whether you\nwant or must compile it yourself.  Check http://www.vim.org/download.php for\nan overview of currently available distributions.\n\nSome popular places to get the latest Vim:\n* Check out the git repository from [github](https://github.com/vim/vim).\n* Get the source code as an [archive](https://github.com/vim/vim/releases).\n* Get a Windows executable from the\n[vim-win32-installer](https://github.com/vim/vim-win32-installer/releases) repository.\n\n\n\n## Compiling ##\n\nIf you obtained a binary distribution you don't need to compile Vim.  If you\nobtained a source distribution, all the stuff for compiling Vim is in the\n`src` directory.  See `src/INSTALL` for instructions.\n\n\n## Installation ##\n\nSee one of these files for system-specific instructions.  Either in the\nREADMEdir directory (in the repository) or the top directory (if you unpack an\narchive):\n\n\tREADME_ami.txt\t\tAmiga\n\tREADME_unix.txt\t\tUnix\n\tREADME_dos.txt\t\tMS-DOS and MS-Windows\n\tREADME_mac.txt\t\tMacintosh\n\tREADME_vms.txt\t\tVMS\n\nThere are other `README_*.txt` files, depending on the distribution you used.\n\n\n## Documentation ##\n\nThe Vim tutor is a one hour training course for beginners.  Often it can be\nstarted as `vimtutor`.  See `:help tutor` for more information.\n\nThe best is to use `:help` in Vim.  If you don't have an executable yet, read\n`runtime/doc/help.txt`.  It contains pointers to the other documentation\nfiles.  The User Manual reads like a book and is recommended to learn to use\nVim.  See `:help user-manual`.\n\n\n## Copying ##\n\nVim is Charityware.  You can use and copy it as much as you like, but you are\nencouraged to make a donation to help orphans in Uganda.  Please read the file\n`runtime/doc/uganda.txt` for details (do `:help uganda` inside Vim).\n\nSummary of the license: There are no restrictions on using or distributing an\nunmodified copy of Vim.  Parts of Vim may also be distributed, but the license\ntext must always be included.  For modified versions a few restrictions apply.\nThe license is GPL compatible, you may compile Vim with GPL libraries and\ndistribute it.\n\n\n## Sponsoring ##\n\nFixing bugs and adding new features takes a lot of time and effort.  To show\nyour appreciation for the work and motivate Bram and others to continue\nworking on Vim please send a donation.\n\nSince Bram is back to a paid job the money will now be used to help children\nin Uganda.  See `runtime/doc/uganda.txt`.  But at the same time donations\nincrease Bram's motivation to keep working on Vim!\n\nFor the most recent information about sponsoring look on the Vim web site:\n\thttp://www.vim.org/sponsor/\n\n\n## Contributing ##\n\nIf you would like to help making Vim better, see the [CONTRIBUTING.md](https://github.com/vim/vim/blob/master/CONTRIBUTING.md) file.\n\n\n## Information ##\n\nThe latest news about Vim can be found on the Vim home page:\n\thttp://www.vim.org/\n\nIf you have problems, have a look at the Vim documentation or tips:\n\thttp://www.vim.org/docs.php\n\thttp://vim.wikia.com/wiki/Vim_Tips_Wiki\n\nIf you still have problems or any other questions, use one of the mailing\nlists to discuss them with Vim users and developers:\n\thttp://www.vim.org/maillist.php\n\nIf nothing else works, report bugs directly:\n\tBram Moolenaar \u003cBram@vim.org\u003e\n\n\n## Main author ##\n\nSend any other comments, patches, flowers and suggestions to:\n\tBram Moolenaar \u003cBram@vim.org\u003e\n\n\nThis is `README.md` for version 8.1 of Vim: Vi IMproved.\n"
  },
  {
    "repo": "cfenollosa/os-tutorial",
    "content": "os-tutorial\n===========\n\nHow to create an OS from scratch!\n\nI have always wanted to learn how to make an OS from scratch. In college I was taught\nhow to implement advanced features (pagination, semaphores, memory management, etc)\nbut:\n\n- I never got to start from my own boot sector\n- College is hard so I don't remember most of it.\n- I'm fed up with people who think that reading an already existing kernel, even if small, is \na good idea to learn operating systems.\n\nInspired by [this document](http://www.cs.bham.ac.uk/~exr/lectures/opsys/10_11/lectures/os-dev.pdf)\nand the [OSDev wiki](http://wiki.osdev.org/), I'll try to make short step-by-step READMEs and\ncode samples for anybody to follow. Honestly, this tutorial is basically the first document but\nsplit into smaller pieces and without the theory.\n\nUpdated: more sources: [the little book about OS development](https://littleosbook.github.io),\n[JamesM's kernel development tutorials](https://web.archive.org/web/20160412174753/http://www.jamesmolloy.co.uk/tutorial_html/index.html)\n\n\nFeatures\n--------\n\n- This course is a code tutorial aimed at people who are comfortable with low level computing. For example,\nprogrammers who have curiosity on how an OS works but don't have the time or willpower to start reading the Linux kernel\ntop to bottom.\n- There is little theory. Yes, this is a feature. Google is your theory lecturer. Once you pass college, \nexcessive theory is worse than no theory because it makes things seem more difficult than they really are.\n- The lessons are tiny and may take 5-15 minutes to complete. Trust me and trust yourself. You can do it!\n\n\nHow to use this tutorial\n------------------------\n\n1. Start with the first folder and go down in order. They build on previous code, so if \nyou jump right to folder 05 and don't know why there is a `mov ah, 0x0e`, it's because you missed lecture 02.\nReally, just go in order. You can always skip stuff you already know.\n\n2. Open the README and read the first line, which details the concepts you should be familiar with\nbefore reading the code. Google concepts you are not familiar with. The second line states the goals for each lesson. \nRead them, because they explain why we do what we do. The \"why\" is as important as the \"how\".\n \n3. Read the rest of the README. It is **very concise**.\n\n4. (Optional) Try to write the code files by yourself after reading the README.\n\n5. Look at the code examples. They are extremely well commented.\n\n6. (Optional) Experiment with them and try to break things. The only way to make sure you understood something is\ntrying to break it or replicate it with different commands.\n\n\nTL;DR: First read the README on each folder, then the code files. If you're brave, try to code them yourself.\n\n\nStrategy\n--------\n\nWe will want to do many things with our OS:\n\n- Boot from scratch, without GRUB - DONE!\n- Enter 32-bit mode - DONE\n- Jump from Assembly to C - DONE!\n- Interrupt handling - DONE!\n- Screen output and keyboard input - DONE!\n- A tiny, basic `libc` which grows to suit our needs - DONE!\n- Memory management\n- Write a filesystem to store files\n- Create a very simple shell\n- User mode\n- Maybe we will write a simple text editor\n- Multiple processes and scheduling\n\nProbably we will go through them in that order, however it's soon to tell.\n\nIf we feel brave enough:\n\n- A BASIC interpreter, like in the 70s!\n- A GUI\n- Networking\n\n\n\nContributing\n------------\n\nThis is a personal learning project, and even though it hasn't been updated for a long time, I still have hopes to get into it at some point.\n\nI'm thankful to all those who have pointed out bugs and submitted pull requests. I will need some time to review everything and I cannot guarantee that at this moment.\n\nPlease feel free to fork this repo. If many of you are interested in continuing the project, let me know and I'll link the \"main fork\" from here.\n"
  },
  {
    "repo": "FFmpeg/FFmpeg",
    "content": "FFmpeg README\n=============\n\nFFmpeg is a collection of libraries and tools to process multimedia content\nsuch as audio, video, subtitles and related metadata.\n\n## Libraries\n\n* `libavcodec` provides implementation of a wider range of codecs.\n* `libavformat` implements streaming protocols, container formats and basic I/O access.\n* `libavutil` includes hashers, decompressors and miscellaneous utility functions.\n* `libavfilter` provides a mean to alter decoded Audio and Video through chain of filters.\n* `libavdevice` provides an abstraction to access capture and playback devices.\n* `libswresample` implements audio mixing and resampling routines.\n* `libswscale` implements color conversion and scaling routines.\n\n## Tools\n\n* [ffmpeg](https://ffmpeg.org/ffmpeg.html) is a command line toolbox to\n  manipulate, convert and stream multimedia content.\n* [ffplay](https://ffmpeg.org/ffplay.html) is a minimalistic multimedia player.\n* [ffprobe](https://ffmpeg.org/ffprobe.html) is a simple analysis tool to inspect\n  multimedia content.\n* Additional small tools such as `aviocat`, `ismindex` and `qt-faststart`.\n\n## Documentation\n\nThe offline documentation is available in the **doc/** directory.\n\nThe online documentation is available in the main [website](https://ffmpeg.org)\nand in the [wiki](https://trac.ffmpeg.org).\n\n### Examples\n\nCoding examples are available in the **doc/examples** directory.\n\n## License\n\nFFmpeg codebase is mainly LGPL-licensed with optional components licensed under\nGPL. Please refer to the LICENSE file for detailed information.\n\n## Contributing\n\nPatches should be submitted to the ffmpeg-devel mailing list using\n`git format-patch` or `git send-email`. Github pull requests should be\navoided because they are not part of our review process and will be ignored.\n"
  },
  {
    "repo": "stedolan/jq",
    "content": "jq\n==\n\njq is a lightweight and flexible command-line JSON processor.\n\n[![Coverage Status](https://coveralls.io/repos/stedolan/jq/badge.svg?branch=master\u0026service=github)](https://coveralls.io/github/stedolan/jq?branch=master),\nUnix: [![Build Status](https://travis-ci.org/stedolan/jq.svg?branch=master)](https://travis-ci.org/stedolan/jq),\nWindows: [![Windows build status](https://ci.appveyor.com/api/projects/status/mi816811c9e9mx29?svg=true)](https://ci.appveyor.com/project/stedolan/jq)\n\n\nIf you want to learn to use jq, read the documentation at\n[https://stedolan.github.io/jq](https://stedolan.github.io/jq).  This\ndocumentation is generated from the docs/ folder of this repository.\nYou can also try it online at [jqplay.org](https://jqplay.org).\n\nIf you want to hack on jq, feel free, but be warned that its internals\nare not well-documented at the moment. Bring a hard hat and a\nshovel.  Also, read the wiki: https://github.com/stedolan/jq/wiki, where\nyou will find cookbooks, discussion of advanced topics, internals,\nrelease engineering, and more.\n\nSource tarball and built executable releases can be found on the\nhomepage and on the github release page, https://github.com/stedolan/jq/releases\n\nIf you're building directly from the latest git, you'll need flex,\nbison (3.0 or newer), libtool, make, automake, and autoconf installed.\nTo get regexp support you'll also need to install Oniguruma or clone it as a\ngit submodule as per the instructions below.\n(note that jq's tests require regexp support to pass).  To build, run:\n\n    git submodule update --init # if building from git to get oniguruma\n    autoreconf -fi              # if building from git\n    ./configure --with-oniguruma=builtin\n    make -j8\n    make check\n\nTo build without bison or flex, add `--disable-maintainer-mode` to the\n./configure invocation:\n\n    ./configure --with-oniguruma=builtin --disable-maintainer-mode\n\n(Developers must not use `--disable-maintainer-mode`, not when making\nchanges to the jq parser and/or lexer.)\n\nTo build a statically linked version of jq, run:\n\n    make LDFLAGS=-all-static\n\nAfter make finishes, you'll be able to use `./jq`.  You can also\ninstall it using:\n\n    sudo make install\n\nIf you're not using the latest git version but instead building a\nreleased tarball (available on the website), then you won't need to\nrun `autoreconf` (and shouldn't), and you won't need flex or bison.\n\nTo cross-compile for OS X and Windows, see docs/Rakefile's build task\nand scripts/crosscompile.  You'll need a cross-compilation environment,\nsuch as Mingw for cross-compiling for Windows.\n\nCross-compilation requires a clean workspace, then:\n\n    # git clean ...\n    autoreconf -i\n    ./configure\n    make distclean\n    scripts/crosscompile \u003cname-of-build\u003e \u003cconfigure-options\u003e\n\nUse the --host= and --target= ./configure options to select a\ncross-compilation environment.  See also the wiki.\n\nSend questions to https://stackoverflow.com/questions/tagged/jq or to the #jq channel (http://irc.lc/freenode/%23jq/) on Freenode (https://webchat.freenode.net/).\n"
  },
  {
    "repo": "tmux/tmux",
    "content": "Welcome to tmux!\n\ntmux is a terminal multiplexer: it enables a number of terminals to be created,\naccessed, and controlled from a single screen. tmux may be detached from a\nscreen and continue running in the background, then later reattached.\n\nThis release runs on OpenBSD, FreeBSD, NetBSD, Linux, OS X and Solaris.\n\ntmux depends on libevent 2.x. Download it from:\n\n\thttp://libevent.org\n\nIt also depends on ncurses, available from:\n\n\thttp://invisible-island.net/ncurses/\n\nTo build and install tmux from a release tarball, use:\n\n\t$ ./configure \u0026\u0026 make\n\t$ sudo make install\n\ntmux can use the utempter library to update utmp(5), if it is installed - run\nconfigure with --enable-utempter to enable this.\n\nTo get and build the latest from version control:\n\n\t$ git clone https://github.com/tmux/tmux.git\n\t$ cd tmux\n\t$ sh autogen.sh\n\t$ ./configure \u0026\u0026 make\n\n(Note that this requires at least a working C compiler, make, autoconf,\nautomake, pkg-config as well as libevent and ncurses libraries and headers.)\n\nFor more information see http://git-scm.com. Patches should be sent by email to\nthe mailing list at tmux-users@googlegroups.com or submitted through GitHub at\nhttps://github.com/tmux/tmux/issues.\n\nFor documentation on using tmux, see the tmux.1 manpage. It can be viewed from\nthe source tree with:\n\n\t$ nroff -mdoc tmux.1|less\n\nA small example configuration in example_tmux.conf.\n\nAnd a bash(1) completion file at:\n\n\thttps://github.com/imomaliev/tmux-bash-completion\n\nFor debugging, running tmux with -v or -vv will generate server and client log\nfiles in the current directory.\n\ntmux mailing lists are available. For general discussion and bug reports:\n\n\thttps://groups.google.com/forum/#!forum/tmux-users\n\nAnd for Git commit emails:\n\n\thttps://groups.google.com/forum/#!forum/tmux-git\n\nSubscribe by sending an email to \u003ctmux-users+subscribe@googlegroups.com\u003e.\n\nBug reports, feature suggestions and especially code contributions are most\nwelcome. Please send by email to:\n\n\ttmux-users@googlegroups.com\n\nThis file and the CHANGES, FAQ, SYNCING and TODO files are licensed under the\nISC license. All other files have a license and copyright notice at their start.\n\n-- Nicholas Marriott \u003cnicholas.marriott@gmail.com\u003e\n"
  },
  {
    "repo": "swoole/swoole-src",
    "content": "English | [中文](./README-CN.md)\n\nSwoole\n======\n[![Latest Version](https://img.shields.io/github/release/swoole/swoole-src.svg?style=flat-square)](https://github.com/swoole/swoole-src/releases)\n[![Build Status](https://api.travis-ci.org/swoole/swoole-src.svg)](https://travis-ci.org/swoole/swoole-src)\n[![License](https://img.shields.io/badge/license-apache2-blue.svg)](LICENSE)\n[![Join the chat at https://gitter.im/swoole/swoole-src](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/swoole/swoole-src?utm_source=badge\u0026utm_medium=badge\u0026utm_campaign=pr-badge\u0026utm_content=badge)\n[![Coverity Scan Build Status](https://scan.coverity.com/projects/11654/badge.svg)](https://scan.coverity.com/projects/swoole-swoole-src)\n[![Backers on Open Collective](https://opencollective.com/swoole-src/backers/badge.svg)](#backers) \n[![Sponsors on Open Collective](https://opencollective.com/swoole-src/sponsors/badge.svg)](#sponsors) \n\n![](./mascot.png)\n\n**Swoole is an event-driven asynchronous \u0026 coroutine-based concurrency networking communication engine with high performance written in C and C++ for PHP.**\n\n## ✨Event-based\n\nThe network layer in Swoole is event-based and takes full advantage of the underlying epoll/kqueue implementation, making it really easy to serve millions of requests.\n\nSwoole4 use a brand new engine kernel and now it has a full-time developer team, so we are entering an unprecedented period in PHP history which offers a unique possibility for a rapid evolution in performance.\n\n## ⚡️Coroutine\n\nSwoole4 or later supports the built-in coroutine with high availability, and you can use fully synchronized code to implement asynchronous performance. PHP code without any additional keywords, the underlying automatic coroutine-scheduling.\n\nDevelopers can understand coroutines as ultra-lightweight threads, and you can easily create thousands of coroutines in a single process.\n\n### MySQL\n\nConcurrency 10k requests to read data from MySQL takes only 0.2s!\n\n```php\n$s = microtime(true);\nfor ($c = 100; $c--;) {\n    go(function () {\n        $mysql = new Swoole\\Coroutine\\MySQL;\n        $mysql-\u003econnect([\n            'host' =\u003e '127.0.0.1',\n            'user' =\u003e 'root',\n            'password' =\u003e 'root',\n            'database' =\u003e 'test'\n        ]);\n        $statement = $mysql-\u003eprepare('SELECT * FROM `user`');\n        for ($n = 100; $n--;) {\n            $result = $statement-\u003eexecute();\n            assert(count($result) \u003e 0);\n        }\n    });\n}\nSwoole\\Event::wait();\necho 'use ' . (microtime(true) - $s) . ' s';\n```\n\n### Mixed Server\n\nYou can create multiple services on the single event loop: TCP, HTTP, Websocket and HTTP2, and easily handle thousands of requests.\n\n```php\nfunction tcp_pack(string $data): string\n{\n    return pack('n', strlen($data)) . $data;\n}\nfunction tcp_unpack(string $data): string\n{\n    return substr($data, 2, unpack('n', substr($data, 0, 2))[1]);\n}\n$tcp_options = [\n    'open_length_check' =\u003e true,\n    'package_length_type' =\u003e 'n',\n    'package_length_offset' =\u003e 0,\n    'package_body_offset' =\u003e 2\n];\n```\n\n```php\n$server = new Swoole\\WebSocket\\Server('127.0.0.1', 9501, SWOOLE_BASE);\n$server-\u003eset(['open_http2_protocol' =\u003e true]);\n// http \u0026\u0026 http2\n$server-\u003eon('request', function (Swoole\\Http\\Request $request, Swoole\\Http\\Response $response) {\n    $response-\u003eend('Hello ' . $request-\u003erawcontent());\n});\n// websocket\n$server-\u003eon('message', function (Swoole\\WebSocket\\Server $server, Swoole\\WebSocket\\Frame $frame) {\n    $server-\u003epush($frame-\u003efd, 'Hello ' . $frame-\u003edata);\n});\n// tcp\n$tcp_server = $server-\u003elisten('127.0.0.1', 9502, SWOOLE_TCP);\n$tcp_server-\u003eset($tcp_options);\n$tcp_server-\u003eon('receive', function (Swoole\\Server $server, int $fd, int $reactor_id, string $data) {\n    $server-\u003esend($fd, tcp_pack('Hello ' . tcp_unpack($data)));\n});\n$server-\u003estart();\n```\n\n### Kinds of Clients\n\nWhether you DNS query or send requests or receive responses, all of these are scheduled by coroutine automatically.\n\n```php\ngo(function () {\n    // http\n    $http_client = new Swoole\\Coroutine\\Http\\Client('127.0.0.1', 9501);\n    assert($http_client-\u003epost('/', 'Swoole Http'));\n    var_dump($http_client-\u003ebody);\n    // websocket\n    $http_client-\u003eupgrade('/');\n    $http_client-\u003epush('Swoole Websocket');\n    var_dump($http_client-\u003erecv()-\u003edata);\n});\ngo(function () {\n    // http2\n    $http2_client = new Swoole\\Coroutine\\Http2\\Client('localhost', 9501);\n    $http2_client-\u003econnect();\n    $http2_request = new Swoole\\Http2\\Reuqest;\n    $http2_request-\u003emethod = 'POST';\n    $http2_request-\u003edata = 'Swoole Http2';\n    $http2_client-\u003esend($http2_request);\n    $http2_response = $http2_client-\u003erecv();\n    var_dump($http2_response-\u003edata);\n});\ngo(function () use ($tcp_options) {\n    // tcp\n    $tcp_client = new Swoole\\Coroutine\\Client(SWOOLE_TCP);\n    $tcp_client-\u003eset($tcp_options);\n    $tcp_client-\u003econnect('127.0.0.1', 9502);\n    $tcp_client-\u003esend(tcp_pack('Swoole Tcp'));\n    var_dump(tcp_unpack($tcp_client-\u003erecv()));\n});\n```\n\n### Channel\n\nChannel is the only way for exchanging data between coroutines, the development combination of the `Coroutine + Channel` is the famous CSP programming model.\n\nIn Swoole development, Channel is usually used for implementing connection pool or scheduling coroutine concurrent.\n\n#### The simplest example of a connection pool\n\nIn the following example, we have a thousand concurrently requests to redis. Normally, this has exceeded the maximum number of Redis connections setting and will throw a connection exception, but the connection pool based on Channel can perfectly schedule requests. We don't have to worry about connection overload.\n\n```php\nclass RedisPool\n{\n    /**@var \\Swoole\\Coroutine\\Channel */\n    protected $pool;\n\n    /**\n     * RedisPool constructor.\n     * @param int $size max connections\n     */\n    public function __construct(int $size = 100)\n    {\n        $this-\u003epool = new \\Swoole\\Coroutine\\Channel($size);\n        for ($i = 0; $i \u003c $size; $i++) {\n            $redis = new Swoole\\Coroutine\\Redis();\n            $res = $redis-\u003econnect('127.0.0.1', 6379);\n            if ($res == false) {\n                throw new \\RuntimeException(\"failed to connect redis server.\");\n            } else {\n                $this-\u003eput($redis);\n            }\n        }\n    }\n\n    public function get(): \\Swoole\\Coroutine\\Redis\n    {\n        return $this-\u003epool-\u003epop();\n    }\n\n    public function put(\\Swoole\\Coroutine\\Redis $redis)\n    {\n        $this-\u003epool-\u003epush($redis);\n    }\n\n    public function close(): void\n    {\n        $this-\u003epool-\u003eclose();\n        $this-\u003epool = null;\n    }\n}\n\ngo(function () {\n    $pool = new RedisPool();\n    // max concurrency num is more than max connections\n    // but it's no problem, channel will help you with scheduling\n    for ($c = 0; $c \u003c 1000; $c++) {\n        go(function () use ($pool, $c) {\n            for ($n = 0; $n \u003c 100; $n++) {\n                $redis = $pool-\u003eget();\n                assert($redis-\u003eset(\"awesome-{$c}-{$n}\", 'swoole'));\n                assert($redis-\u003eget(\"awesome-{$c}-{$n}\") === 'swoole');\n                assert($redis-\u003edelete(\"awesome-{$c}-{$n}\"));\n                $pool-\u003eput($redis);\n            }\n        });\n    }\n});\n```\n\n#### Production \u0026 Consumption\n\nSome Swoole's clients implement the defer mode for concurrency, but you can still implement it flexible with a combination of coroutines and channels.\n\n```php\ngo(function () {\n    // User: I need you to bring me some information back.\n    // Channel: OK! I will be responsible for scheduling.\n    $channel = new Swoole\\Coroutine\\Channel;\n    go(function () use ($channel) {\n        // Coroutine A: Ok! I will show you the github addr info\n        $addr_info = Co::getaddrinfo('github.com');\n        $channel-\u003epush(['A', json_encode($addr_info, JSON_PRETTY_PRINT)]);\n    });\n    go(function () use ($channel) {\n        // Coroutine B: Ok! I will show you what your code look like\n        $mirror = Co::readFile(__FILE__);\n        $channel-\u003epush(['B', $mirror]);\n    });\n    go(function () use ($channel) {\n        // Coroutine C: Ok! I will show you the date\n        $channel-\u003epush(['C', date(DATE_W3C)]);\n    });\n    for ($i = 3; $i--;) {\n        list($id, $data) = $channel-\u003epop();\n        echo \"From {$id}:\\n {$data}\\n\";\n    }\n    // User: Amazing, I got every information at earliest time!\n});\n```\n\n### Timer\n\n```php\n$id = Swoole\\Timer::tick(100, function () {\n    echo \"⚙️ Do something...\\n\";\n});\nSwoole\\Timer::after(500, function () use ($id) {\n    Swoole\\Timer::clear($id);\n    echo \"⏰ Done\\n\";\n});\nSwoole\\Timer::after(1000, function () use ($id) {\n    if (!Swoole\\Timer::exists($id)) {\n        echo \"✅ All right!\\n\";\n    }\n});\n```\n#### Use coroutine way\n\n```php\ngo(function () {\n    $i = 0;\n    while (true) {\n        Co::sleep(0.1);\n        echo \"📝 Do something...\\n\";\n        if (++$i === 5) {\n            echo \"🛎 Done\\n\";\n            break;\n        }\n    }\n    echo \"🎉 All right!\\n\";\n});\n```\n\n## 🔥 Amazing Runtime Hook\n\n**In the latest version of Swoole, we add a new feature to make sync network libs to be a coroutine lib with only one step.**\n\nJust call `Swoole\\Runtime::enableCoroutine()` method on the top of the script and use php-redis, concurrency 10k requests to read data from Redis takes only 0.1s!\n\n```php\nSwoole\\Runtime::enableCoroutine();\n$s = microtime(true);\nfor ($c = 100; $c--;) {\n    go(function () {\n        ($redis = new Redis)-\u003econnect('127.0.0.1', 6379);\n        for ($n = 100; $n--;) {\n            assert($redis-\u003eget('awesome') === 'swoole');\n        }\n    });\n}\nSwoole\\Event::wait();\necho 'use ' . (microtime(true) - $s) . ' s';\n```\n\nAfter you call it, the Swoole kernel will replace the function pointers of streams in ZendVM, if you use `php_stream` based extensions, all socket operations can be dynamically converted to asynchronous IO scheduled by coroutine at runtime.\n\n### How many things you can do in 1s?\n\nSleep 10k times, read, write, check and delete files 10k times, use PDO and MySQLi to communicate with the database 10k times, create a TCP server and multiple clients to communicate with each other 10k times, create a UDP server and multiple clients to communicate with each other 10k times...Everything works well in one process!\n\nJust see what the Swoole brings, just imagine...\n\n```php\nSwoole\\Runtime::enableCoroutine();\n$s = microtime(true);\n\n// i just want to sleep...\nfor ($c = 100; $c--;) {\n    go(function () {\n        for ($n = 100; $n--;) {\n            usleep(1000);\n        }\n    });\n}\n\n// 10k file read and write\nfor ($c = 100; $c--;) {\n    go(function () use ($c) {\n        $tmp_filename = \"/tmp/test-{$c}.php\";\n        for ($n = 100; $n--;) {\n            $self = file_get_contents(__FILE__);\n            file_put_contents($tmp_filename, $self);\n            assert(file_get_contents($tmp_filename) === $self);\n        }\n        unlink($tmp_filename);\n    });\n}\n\n// 10k pdo and mysqli read\nfor ($c = 50; $c--;) {\n    go(function () {\n        $pdo = new PDO('mysql:host=127.0.0.1;dbname=test;charset=utf8', 'root', 'root');\n        $statement = $pdo-\u003eprepare('SELECT * FROM `user`');\n        for ($n = 100; $n--;) {\n            $statement-\u003eexecute();\n            assert(count($statement-\u003efetchAll()) \u003e 0);\n        }\n    });\n}\nfor ($c = 50; $c--;) {\n    go(function () {\n        $mysqli = new Mysqli('127.0.0.1', 'root', 'root', 'test');\n        $statement = $mysqli-\u003eprepare('SELECT `id` FROM `user`');\n        for ($n = 100; $n--;) {\n            $statement-\u003ebind_result($id);\n            $statement-\u003eexecute();\n            $statement-\u003efetch();\n            assert($id \u003e 0);\n        }\n    });\n}\n\n// php_stream tcp server \u0026 client with 12.8k requests in single process\nfunction tcp_pack(string $data): string\n{\n    return pack('n', strlen($data)) . $data;\n}\n\nfunction tcp_length(string $head): int\n{\n    return unpack('n', $head)[1];\n}\n\ngo(function () {\n    $ctx = stream_context_create(['socket' =\u003e ['so_reuseaddr' =\u003e true, 'backlog' =\u003e 128]]);\n    $socket = stream_socket_server(\n        'tcp://0.0.0.0:9502',\n        $errno, $errstr, STREAM_SERVER_BIND | STREAM_SERVER_LISTEN, $ctx\n    );\n    if (!$socket) {\n        echo \"$errstr ($errno)\\n\";\n    } else {\n        $i = 0;\n        while ($conn = stream_socket_accept($socket, 1)) {\n            stream_set_timeout($conn, 5);\n            for ($n = 100; $n--;) {\n                $data = fread($conn, tcp_length(fread($conn, 2)));\n                assert($data === \"Hello Swoole Server #{$n}!\");\n                fwrite($conn, tcp_pack(\"Hello Swoole Client #{$n}!\"));\n            }\n            if (++$i === 128) {\n                fclose($socket);\n                break;\n            }\n        }\n    }\n});\nfor ($c = 128; $c--;) {\n    go(function () {\n        $fp = stream_socket_client(\"tcp://127.0.0.1:9502\", $errno, $errstr, 1);\n        if (!$fp) {\n            echo \"$errstr ($errno)\\n\";\n        } else {\n            stream_set_timeout($fp, 5);\n            for ($n = 100; $n--;) {\n                fwrite($fp, tcp_pack(\"Hello Swoole Server #{$n}!\"));\n                $data = fread($fp, tcp_length(fread($fp, 2)));\n                assert($data === \"Hello Swoole Client #{$n}!\");\n            }\n            fclose($fp);\n        }\n    });\n}\n\n// udp server \u0026 client with 12.8k requests in single process\ngo(function () {\n    $socket = new Swoole\\Coroutine\\Socket(AF_INET, SOCK_DGRAM, 0);\n    $socket-\u003ebind('127.0.0.1', 9503);\n    $client_map = [];\n    for ($c = 128; $c--;) {\n        for ($n = 0; $n \u003c 100; $n++) {\n            $recv = $socket-\u003erecvfrom($peer);\n            $client_uid = \"{$peer['address']}:{$peer['port']}\";\n            $id = $client_map[$client_uid] = ($client_map[$client_uid] ?? -1) + 1;\n            assert($recv === \"Client: Hello #{$id}!\");\n            $socket-\u003esendto($peer['address'], $peer['port'], \"Server: Hello #{$id}!\");\n        }\n    }\n    $socket-\u003eclose();\n});\nfor ($c = 128; $c--;) {\n    go(function () {\n        $fp = stream_socket_client(\"udp://127.0.0.1:9503\", $errno, $errstr, 1);\n        if (!$fp) {\n            echo \"$errstr ($errno)\\n\";\n        } else {\n            for ($n = 0; $n \u003c 100; $n++) {\n                fwrite($fp, \"Client: Hello #{$n}!\");\n                $recv = fread($fp, 1024);\n                list($address, $port) = explode(':', (stream_socket_get_name($fp, true)));\n                assert($address === '127.0.0.1' \u0026\u0026 (int)$port === 9503);\n                assert($recv === \"Server: Hello #{$n}!\");\n            }\n            fclose($fp);\n        }\n    });\n}\n\nSwoole\\Event::wait();\necho 'use ' . (microtime(true) - $s) . ' s';\n```\n\n## ⌛️ Installation\n\n\u003e As with any open source project, Swoole always provides the most reliable stability and the most powerful features in **the latest released version**. Please ensure as much as possible that you are using the latest version.\n\n### Requirements\n\n- Linux, OS X and basic Windows support (thanks to Cygwin)\n- PHP 7.0.0 or later (The higher the version, the better the performance.)\n- GCC 4.8 or later\n\n### 1. Install via pecl (beginners)\n\n```shell\npecl install swoole\n```\n\n### 2. Install from source (recommand)\n\n```shell\ngit clone https://github.com/swoole/swoole-src.git \u0026\u0026 \\\ncd swoole-src \u0026\u0026 \\\nphpize \u0026\u0026 \\\n./configure \u0026\u0026 \\\nmake \u0026\u0026 make install\n```\n\n#### Enable extension in PHP\n\nAfter compiling and installing to the system successfully, you need to add a new line `extension=swoole.so` to `php.ini` to enable Swoole extension.\n\n#### Extra Compiler Configurations\n\n\u003e for example: `./configure --enable-openssl --enable-sockets`\n\n- `--enable-openssl`\n- `--enable-sockets`\n- `--enable-http2`, `--with-nghttp2-dir=/path/to` (need nghttp2)\n- `--enable-mysqlnd` (need mysqlnd)\n- `--enable-async-redis`, `--with-hiredis-dir=/path/to` (need hiredis, build-in in v4.2.6 or later)\n\n### Upgrade\n\n\u003e  ⚠️ If you upgrade from source, don't forget to `make clean` before you upgrade your swoole\n\n1. `pecl upgrade swoole`\n2. `git pull \u0026\u0026 cd swoole-src \u0026\u0026 make clean \u0026\u0026 make \u0026\u0026 sudo make install`\n3. if you change your PHP version, please re-run `phpize clean \u0026\u0026 phpize` then try to compile\n\n## 💎 Frameworks \u0026 Components\n\n- [**Swoft**](https://github.com/swoft-cloud) is a modern, high-performance AOP and coroutine PHP framework.\n- [**Easyswoole**](https://www.easyswoole.com) is a simple, high-performance PHP framework, based on Swoole, which makes using Swoole as easy as `echo \"hello world\"`.\n- [**Saber**](https://github.com/swlib/saber) Is a human-friendly, high-performance HTTP client component that has almost everything you can imagine.\n\n## 🛠 Develop \u0026 Discussion\n\n* __中文文档__: \u003chttp://wiki.swoole.com\u003e\n* __Document__: \u003chttps://www.swoole.co.uk/docs\u003e\n* __IDE Helper \u0026 API__: \u003chttps://github.com/swoole/ide-helper\u003e\n* __中文社区__: \u003chttps://wiki.swoole.com/wiki/page/p-discussion.html\u003e\n* __Twitter__: \u003chttps://twitter.com/php_swoole\u003e\n* __Slack Group__: \u003chttps://swoole.slack.com\u003e\n\n## 🍭 Benchmark\n\n+ On the open source [Techempower Web Framework benchmarks](https://www.techempower.com/benchmarks/#section=data-r17) Swoole used MySQL database benchmark to rank first, and all performance tests ranked in the first echelon.\n+ You can just run [Benchmark Script](./benchmark/benchmark.php) to quickly test the maximum QPS of Swoole-HTTP-Server on your machine.\n\n## 🖊️ Contribution\n\nYour contribution to Swoole development is very welcome!\n\nYou may contribute in the following ways:\n\n* [Report issues and feedback](https://github.com/swoole/swoole-src/issues)\n* Submit fixes, features via Pull Request\n* Write/polish documentation\n\n## ❤️ Contributors\n\nThis project exists thanks to all the people who contribute. [[Contributor](https://github.com/swoole/swoole-src/graphs/contributors)].\n\u003ca href=\"https://github.com/swoole/swoole-src/graphs/contributors\"\u003e\u003cimg src=\"https://opencollective.com/swoole-src/contributors.svg?width=890\u0026button=false\" /\u003e\u003c/a\u003e\n\n## 📃 License\n\nApache License Version 2.0 see http://www.apache.org/licenses/LICENSE-2.0.html\n"
  },
  {
    "repo": "curl/curl",
    "content": "![curl logo](https://curl.haxx.se/logo/curl-logo.svg)\n\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/63/badge)](https://bestpractices.coreinfrastructure.org/projects/63)\n[![Coverity passed](https://scan.coverity.com/projects/curl/badge.svg)](https://scan.coverity.com/projects/curl)\n[![Travis-CI Build Status](https://travis-ci.org/curl/curl.svg?branch=master)](https://travis-ci.org/curl/curl)\n[![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/l1vv31029huhf4g4?svg=true)](https://ci.appveyor.com/project/curlorg/curl)\n[![Coverage Status](https://coveralls.io/repos/github/curl/curl/badge.svg)](https://coveralls.io/github/curl/curl)\n[![Backers on Open Collective](https://opencollective.com/curl/backers/badge.svg)](#backers)\n[![Sponsors on Open Collective](https://opencollective.com/curl/sponsors/badge.svg)](#sponsors)\n[![Language Grade: C/C++](https://img.shields.io/lgtm/grade/cpp/g/curl/curl.svg?logo=lgtm\u0026logoWidth=18)](https://lgtm.com/projects/g/curl/curl/context:cpp)\n\nCurl is a command-line tool for transferring data specified with URL\nsyntax. Find out how to use curl by reading [the curl.1 man\npage](https://curl.haxx.se/docs/manpage.html) or [the MANUAL\ndocument](https://curl.haxx.se/docs/manual.html). Find out how to install Curl\nby reading [the INSTALL document](https://curl.haxx.se/docs/install.html).\n\nlibcurl is the library curl is using to do its job. It is readily available to\nbe used by your software. Read [the libcurl.3 man\npage](https://curl.haxx.se/libcurl/c/libcurl.html) to learn how!\n\nYou find answers to the most frequent questions we get in [the FAQ\ndocument](https://curl.haxx.se/docs/faq.html).\n\nStudy [the COPYING file](https://curl.haxx.se/docs/copyright.html) for\ndistribution terms and similar. If you distribute curl binaries or other\nbinaries that involve libcurl, you might enjoy [the LICENSE-MIXING\ndocument](https://curl.haxx.se/legal/licmix.html).\n\n## Contact\n\nIf you have problems, questions, ideas or suggestions, please contact us by\nposting to a suitable [mailing list](https://curl.haxx.se/mail/).\n\nAll contributors to the project are listed in [the THANKS\ndocument](https://curl.haxx.se/docs/thanks.html).\n\n## Website\n\nVisit the [curl web site](https://curl.haxx.se/) for the latest news and\ndownloads.\n\n## Git\n\nTo download the very latest source from the Git server do this:\n\n    git clone https://github.com/curl/curl.git\n\n(you'll get a directory named curl created, filled with the source code)\n\n## Notice\n\nCurl contains pieces of source code that is Copyright (c) 1998, 1999 Kungliga\nTekniska Högskolan. This notice is included here to comply with the\ndistribution terms.\n\n## Backers\n\nThank you to all our backers! 🙏 [[Become a backer](https://opencollective.com/curl#backer)]\n\n\u003ca href=\"https://opencollective.com/curl#backers\" target=\"_blank\"\u003e\u003cimg src=\"https://opencollective.com/curl/backers.svg?width=890\"\u003e\u003c/a\u003e\n\n\n## Sponsors\n\nSupport this project by becoming a sponsor. Your logo will show up here with a\nlink to your website. [[Become a\nsponsor](https://opencollective.com/curl#sponsor)]\n\n\u003ca href=\"https://opencollective.com/curl/sponsor/0/website\" target=\"_blank\"\u003e\u003cimg src=\"https://opencollective.com/curl/sponsor/0/avatar.svg\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opencollective.com/curl/sponsor/1/website\" target=\"_blank\"\u003e\u003cimg src=\"https://opencollective.com/curl/sponsor/1/avatar.svg\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opencollective.com/curl/sponsor/2/website\" target=\"_blank\"\u003e\u003cimg src=\"https://opencollective.com/curl/sponsor/2/avatar.svg\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opencollective.com/curl/sponsor/3/website\" target=\"_blank\"\u003e\u003cimg src=\"https://opencollective.com/curl/sponsor/3/avatar.svg\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opencollective.com/curl/sponsor/4/website\" target=\"_blank\"\u003e\u003cimg src=\"https://opencollective.com/curl/sponsor/4/avatar.svg\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opencollective.com/curl/sponsor/5/website\" target=\"_blank\"\u003e\u003cimg src=\"https://opencollective.com/curl/sponsor/5/avatar.svg\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opencollective.com/curl/sponsor/6/website\" target=\"_blank\"\u003e\u003cimg src=\"https://opencollective.com/curl/sponsor/6/avatar.svg\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opencollective.com/curl/sponsor/7/website\" target=\"_blank\"\u003e\u003cimg src=\"https://opencollective.com/curl/sponsor/7/avatar.svg\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opencollective.com/curl/sponsor/8/website\" target=\"_blank\"\u003e\u003cimg src=\"https://opencollective.com/curl/sponsor/8/avatar.svg\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opencollective.com/curl/sponsor/9/website\" target=\"_blank\"\u003e\u003cimg src=\"https://opencollective.com/curl/sponsor/9/avatar.svg\"\u003e\u003c/a\u003e\n"
  },
  {
    "repo": "obsproject/obs-studio",
    "content": "OBS Studio \u003chttps://obsproject.com\u003e\n===================================\n\n.. image:: https://travis-ci.org/obsproject/obs-studio.svg?branch=master\n   :alt: OBS Studio Build Status - Travis CI\n   :target: https://travis-ci.org/obsproject/obs-studio\n\n.. image:: https://ci.appveyor.com/api/projects/status/github/obsproject/obs-studio?branch=master\u0026svg=true\n   :alt: OBS Studio Build Status - AppVeyor CI\n   :target: https://ci.appveyor.com/project/jp9000/obs-studio/branch/master\n\nWhat is OBS Studio?\n-------------------\n\n  OBS Studio is software designed for capturing, compositing, encoding,\n  recording, and streaming video content, efficiently.\n\n  It's distributed under the GNU General Public License v2 - see the\n  accompanying COPYING file for more details.\n\nQuick Links\n-----------\n\n - Website: https://obsproject.com\n\n - Help/Documentation/Guides: https://github.com/obsproject/obs-studio/wiki\n\n - Forums: https://obsproject.com/forum/\n\n - Build Instructions: https://github.com/obsproject/obs-studio/wiki/Install-Instructions\n\n - Developer/API Documentation: https://obsproject.com/docs\n\n - Bug Tracker: https://obsproject.com/mantis/\n\n   (Note: The bug tracker is linked to forum accounts.  To use the bug\n   tracker, log in to a forum account)\n\nContributing\n------------\n\n - If you wish to contribute code to the project, please make sure to\n   read the coding and commit guidelines:\n   https://github.com/obsproject/obs-studio/blob/master/CONTRIBUTING.rst\n\n - Developer/API documentation can be found here:\n   https://obsproject.com/docs\n\n - If you wish to contribute translations, do not submit pull requests.\n   Instead, please use Crowdin.  For more information read this thread:\n   https://obsproject.com/forum/threads/how-to-contribute-translations-for-obs.16327/\n\n - Other ways to contribute are by helping people out with support on\n   our forums or in our community chat.  Please limit support to topics\n   you fully understand -- bad advice is worse than no advice.  When it\n   comes to something that you don't fully know or understand, please\n   defer to the official help or official channels.\n\nCredits\n-------\n - Icons made by `Freepik \u003chttps://www.freepik.com\u003e`_ from\n   `Flaticon \u003chttps://www.flaticon.com/\u003e`_ are licensed under\n   `CC 3.0 BY \u003chttps://creativecommons.org/licenses/by/3.0/\u003e`_.\n"
  },
  {
    "repo": "vurtun/nuklear",
    "content": "# Nuklear\n\n[![Build Status](https://travis-ci.org/vurtun/nuklear.svg)](https://travis-ci.org/vurtun/nuklear)\n\nThis is a minimal state immediate mode graphical user interface toolkit\nwritten in ANSI C and licensed under public domain. It was designed as a simple\nembeddable user interface for application and does not have any dependencies,\na default render backend or OS window and input handling but instead provides a very modular\nlibrary approach by using simple input state for input and draw\ncommands describing primitive shapes as output. So instead of providing a\nlayered library that tries to abstract over a number of platform and\nrender backends it only focuses on the actual UI.\n\n## Features\n\n- Immediate mode graphical user interface toolkit\n- Single header library\n- Written in C89 (ANSI C)\n- Small codebase (~18kLOC)\n- Focus on portability, efficiency and simplicity\n- No dependencies (not even the standard library if not wanted)\n- Fully skinnable and customizable\n- Low memory footprint with total memory control if needed or wanted\n- UTF-8 support\n- No global or hidden state\n- Customizable library modules (you can compile and use only what you need)\n- Optional font baker and vertex buffer output\n- [Documentation](https://rawgit.com/vurtun/nuklear/master/doc/nuklear.html)\n\n## Building\n\nThis library is self contained in one single header file and can be used either\nin header only mode or in implementation mode. The header only mode is used\nby default when included and allows including this header in other headers\nand does not contain the actual implementation.\n\nThe implementation mode requires to define  the preprocessor macro\n`NK_IMPLEMENTATION` in *one* .c/.cpp file before `#include`ing this file, e.g.:\n```c\n#define NK_IMPLEMENTATION\n#include \"nuklear.h\"\n```\nIMPORTANT: Every time you include \"nuklear.h\" you have to define the same optional flags.\nThis is very important not doing it either leads to compiler errors or even worse stack corruptions.\n\n## Gallery\n\n![screenshot](https://cloud.githubusercontent.com/assets/8057201/11761525/ae06f0ca-a0c6-11e5-819d-5610b25f6ef4.gif)\n![screen](https://cloud.githubusercontent.com/assets/8057201/13538240/acd96876-e249-11e5-9547-5ac0b19667a0.png)\n![screen2](https://cloud.githubusercontent.com/assets/8057201/13538243/b04acd4c-e249-11e5-8fd2-ad7744a5b446.png)\n![node](https://cloud.githubusercontent.com/assets/8057201/9976995/e81ac04a-5ef7-11e5-872b-acd54fbeee03.gif)\n![skinning](https://cloud.githubusercontent.com/assets/8057201/15991632/76494854-30b8-11e6-9555-a69840d0d50b.png)\n![gamepad](https://cloud.githubusercontent.com/assets/8057201/14902576/339926a8-0d9c-11e6-9fee-a8b73af04473.png)\n\n## Example\n\n```c\n/* init gui state */\nstruct nk_context ctx;\nnk_init_fixed(\u0026ctx, calloc(1, MAX_MEMORY), MAX_MEMORY, \u0026font);\n\nenum {EASY, HARD};\nstatic int op = EASY;\nstatic float value = 0.6f;\nstatic int i =  20;\n\nif (nk_begin(\u0026ctx, \"Show\", nk_rect(50, 50, 220, 220),\n    NK_WINDOW_BORDER|NK_WINDOW_MOVABLE|NK_WINDOW_CLOSABLE)) {\n    /* fixed widget pixel width */\n    nk_layout_row_static(\u0026ctx, 30, 80, 1);\n    if (nk_button_label(\u0026ctx, \"button\")) {\n        /* event handling */\n    }\n\n    /* fixed widget window ratio width */\n    nk_layout_row_dynamic(\u0026ctx, 30, 2);\n    if (nk_option_label(\u0026ctx, \"easy\", op == EASY)) op = EASY;\n    if (nk_option_label(\u0026ctx, \"hard\", op == HARD)) op = HARD;\n\n    /* custom widget pixel width */\n    nk_layout_row_begin(\u0026ctx, NK_STATIC, 30, 2);\n    {\n        nk_layout_row_push(\u0026ctx, 50);\n        nk_label(\u0026ctx, \"Volume:\", NK_TEXT_LEFT);\n        nk_layout_row_push(\u0026ctx, 110);\n        nk_slider_float(\u0026ctx, 0, \u0026value, 1.0f, 0.1f);\n    }\n    nk_layout_row_end(\u0026ctx);\n}\nnk_end(\u0026ctx);\n```\n![example](https://cloud.githubusercontent.com/assets/8057201/10187981/584ecd68-675c-11e5-897c-822ef534a876.png)\n\n## Bindings\nThere are a number of nuklear bindings for different languges created by other authors.\nI cannot atest for their quality since I am not necessarily proficient in either of these\nlanguages. Furthermore there are no guarantee that all bindings will always be kept up to date:\n\n- [Java](https://github.com/glegris/nuklear4j) by Guillaume Legris\n- [Golang](https://github.com/golang-ui/nuklear) by golang-ui@github.com\n- [Rust](https://github.com/snuk182/nuklear-rust) by snuk182@github.com\n- [Chicken](https://github.com/wasamasa/nuklear) by wasamasa@github.com\n- [Nim](https://github.com/zacharycarter/nuklear-nim) by zacharycarter@github.com\n- Lua\n  - [LÖVE-Nuklear](https://github.com/keharriso/love-nuklear) by Kevin Harrison\n  - [MoonNuklear](https://github.com/stetre/moonnuklear) by Stefano Trettel\n- Python\n  - [pyNuklear](https://github.com/billsix/pyNuklear) by William Emerison Six (ctypes-based wrapper)\n  - [pynk](https://github.com/nathanrw/nuklear-cffi) by nathanrw@github.com (cffi binding)\n- [CSharp/.NET](https://github.com/cartman300/NuklearDotNet) by cartman300@github.com\n\n## Credits\nDeveloped by Micha Mettke and every direct or indirect contributor to the GitHub.\n\n\nEmbeds `stb_texedit`, `stb_truetype` and `stb_rectpack` by Sean Barrett (public domain)\nEmbeds `ProggyClean.ttf` font by Tristan Grimmer (MIT license).\n\n\nBig thank you to Omar Cornut (ocornut@github) for his [imgui](https://github.com/ocornut/imgui) library and\ngiving me the inspiration for this library, Casey Muratori for handmade hero\nand his original immediate mode graphical user interface idea and Sean\nBarrett for his amazing single header [libraries](https://github.com/nothings/stb) which restored my faith\nin libraries and brought me to create some of my own. Finally Apoorva Joshi for his singe-header [file packer](http://apoorvaj.io/single-header-packer.html).\n\n## License\n```\n------------------------------------------------------------------------------\nThis software is available under 2 licenses -- choose whichever you prefer.\n------------------------------------------------------------------------------\nALTERNATIVE A - MIT License\nCopyright (c) 2017 Micha Mettke\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies\nof the Software, and to permit persons to whom the Software is furnished to do\nso, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n------------------------------------------------------------------------------\nALTERNATIVE B - Public Domain (www.unlicense.org)\nThis is free and unencumbered software released into the public domain.\nAnyone is free to copy, modify, publish, use, compile, sell, or distribute this\nsoftware, either in source code form or as a compiled binary, for any purpose,\ncommercial or non-commercial, and by any means.\nIn jurisdictions that recognize copyright laws, the author or authors of this\nsoftware dedicate any and all copyright interest in the software to the public\ndomain. We make this dedication for the benefit of the public at large and to\nthe detriment of our heirs and successors. We intend this dedication to be an\novert act of relinquishment in perpetuity of all present and future rights to\nthis software under copyright law.\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN\nACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n-----------------------------------------------------------------------------\n```\n"
  },
  {
    "repo": "libuv/libuv",
    "content": "![libuv][libuv_banner]\n\n## Overview\n\nlibuv is a multi-platform support library with a focus on asynchronous I/O. It\nwas primarily developed for use by [Node.js][], but it's also\nused by [Luvit](http://luvit.io/), [Julia](http://julialang.org/),\n[pyuv](https://github.com/saghul/pyuv), and [others](https://github.com/libuv/libuv/wiki/Projects-that-use-libuv).\n\n## Feature highlights\n\n * Full-featured event loop backed by epoll, kqueue, IOCP, event ports.\n\n * Asynchronous TCP and UDP sockets\n\n * Asynchronous DNS resolution\n\n * Asynchronous file and file system operations\n\n * File system events\n\n * ANSI escape code controlled TTY\n\n * IPC with socket sharing, using Unix domain sockets or named pipes (Windows)\n\n * Child processes\n\n * Thread pool\n\n * Signal handling\n\n * High resolution clock\n\n * Threading and synchronization primitives\n\n## Versioning\n\nStarting with version 1.0.0 libuv follows the [semantic versioning](http://semver.org/)\nscheme. The API change and backwards compatibility rules are those indicated by\nSemVer. libuv will keep a stable ABI across major releases.\n\nThe ABI/API changes can be tracked [here](http://abi-laboratory.pro/tracker/timeline/libuv/).\n\n## Licensing\n\nlibuv is licensed under the MIT license. Check the [LICENSE file](LICENSE).\nThe documentation is licensed under the CC BY 4.0 license. Check the [LICENSE-docs file](LICENSE-docs).\n\n## Community\n\n * [Support](https://github.com/libuv/help)\n * [Mailing list](http://groups.google.com/group/libuv)\n * [IRC chatroom (#libuv@irc.freenode.org)](http://webchat.freenode.net?channels=libuv\u0026uio=d4)\n\n## Documentation\n\n### Official documentation\n\nLocated in the docs/ subdirectory. It uses the [Sphinx](http://sphinx-doc.org/)\nframework, which makes it possible to build the documentation in multiple\nformats.\n\nShow different supported building options:\n\n```bash\n$ make help\n```\n\nBuild documentation as HTML:\n\n```bash\n$ make html\n```\n\nBuild documentation as HTML and live reload it when it changes (this requires\nsphinx-autobuild to be installed and is only supported on Unix):\n\n```bash\n$ make livehtml\n```\n\nBuild documentation as man pages:\n\n```bash\n$ make man\n```\n\nBuild documentation as ePub:\n\n```bash\n$ make epub\n```\n\nNOTE: Windows users need to use make.bat instead of plain 'make'.\n\nDocumentation can be browsed online [here](http://docs.libuv.org).\n\nThe [tests and benchmarks](https://github.com/libuv/libuv/tree/master/test)\nalso serve as API specification and usage examples.\n\n### Other resources\n\n * [LXJS 2012 talk](http://www.youtube.com/watch?v=nGn60vDSxQ4)\n   \u0026mdash; High-level introductory talk about libuv.\n * [libuv-dox](https://github.com/thlorenz/libuv-dox)\n   \u0026mdash; Documenting types and methods of libuv, mostly by reading uv.h.\n * [learnuv](https://github.com/thlorenz/learnuv)\n   \u0026mdash; Learn uv for fun and profit, a self guided workshop to libuv.\n\nThese resources are not handled by libuv maintainers and might be out of\ndate. Please verify it before opening new issues.\n\n## Downloading\n\nlibuv can be downloaded either from the\n[GitHub repository](https://github.com/libuv/libuv)\nor from the [downloads site](http://dist.libuv.org/dist/).\n\nStarting with libuv 1.7.0, binaries for Windows are also provided. This is to\nbe considered EXPERIMENTAL.\n\nBefore verifying the git tags or signature files, importing the relevant keys\nis necessary. Key IDs are listed in the\n[MAINTAINERS](https://github.com/libuv/libuv/blob/master/MAINTAINERS.md)\nfile, but are also available as git blob objects for easier use.\n\nImporting a key the usual way:\n\n```bash\n$ gpg --keyserver pool.sks-keyservers.net --recv-keys AE9BC059\n```\n\nImporting a key from a git blob object:\n\n```bash\n$ git show pubkey-saghul | gpg --import\n```\n\n### Verifying releases\n\nGit tags are signed with the developer's key, they can be verified as follows:\n\n```bash\n$ git verify-tag v1.6.1\n```\n\nStarting with libuv 1.7.0, the tarballs stored in the\n[downloads site](http://dist.libuv.org/dist/) are signed and an accompanying\nsignature file sit alongside each. Once both the release tarball and the\nsignature file are downloaded, the file can be verified as follows:\n\n```bash\n$ gpg --verify libuv-1.7.0.tar.gz.sign\n```\n\n## Build Instructions\n\nFor GCC there are two build methods: via autotools or via [GYP][].\nGYP is a meta-build system which can generate MSVS, Makefile, and XCode\nbackends. It is best used for integration into other projects.\n\nTo build with autotools:\n\n```bash\n$ sh autogen.sh\n$ ./configure\n$ make\n$ make check\n$ make install\n```\n\nTo build with [CMake](https://cmake.org/):\n\n```bash\n$ mkdir -p out/cmake ; cd out/cmake ; cmake -DBUILD_TESTING=ON ../..\n$ make all test\n# Or manually:\n$ ./uv_run_tests    # shared library build\n$ ./uv_run_tests_a  # static library build\n```\n\nTo build with GYP, first run:\n\n```bash\n$ git clone https://chromium.googlesource.com/external/gyp build/gyp\n```\n\n### Windows\n\nPrerequisites:\n\n* [Python 2.6 or 2.7][] as it is required\n  by [GYP][].\n  If python is not in your path, set the environment variable `PYTHON` to its\n  location. For example: `set PYTHON=C:\\Python27\\python.exe`\n* One of:\n  * [Visual C++ Build Tools][]\n  * [Visual Studio 2015 Update 3][], all editions\n    including the Community edition (remember to select\n    \"Common Tools for Visual C++ 2015\" feature during installation).\n  * [Visual Studio 2017][], any edition (including the Build Tools SKU).\n    **Required Components:** \"MSbuild\", \"VC++ 2017 v141 toolset\" and one of the\n    Windows SDKs (10 or 8.1).\n* Basic Unix tools required for some tests,\n  [Git for Windows][] includes Git Bash\n  and tools which can be included in the global `PATH`.\n\nTo build, launch a git shell (e.g. Cmd or PowerShell), run `vcbuild.bat`\n(to build with VS2017 you need to explicitly add a `vs2017` argument),\nwhich will checkout the GYP code into `build/gyp`, generate `uv.sln`\nas well as the necesery related project files, and start building.\n\n```console\n\u003e vcbuild\n```\n\nOr:\n\n```console\n\u003e vcbuild vs2017\n```\n\nTo run the tests:\n\n```console\n\u003e vcbuild test\n```\n\nTo see all the options that could passed to `vcbuild`:\n\n```console\n\u003e vcbuild help\nvcbuild.bat [debug/release] [test/bench] [clean] [noprojgen] [nobuild] [vs2017] [x86/x64] [static/shared]\nExamples:\n  vcbuild.bat              : builds debug build\n  vcbuild.bat test         : builds debug build and runs tests\n  vcbuild.bat release bench: builds release build and runs benchmarks\n```\n\n\n### Unix\n\nFor Debug builds (recommended) run:\n\n```bash\n$ ./gyp_uv.py -f make\n$ make -C out\n```\n\nFor Release builds run:\n\n```bash\n$ ./gyp_uv.py -f make\n$ BUILDTYPE=Release make -C out\n```\n\nRun `./gyp_uv.py -f make -Dtarget_arch=x32` to build [x32][] binaries.\n\n### OS X\n\nRun:\n\n```bash\n$ ./gyp_uv.py -f xcode\n$ xcodebuild -ARCHS=\"x86_64\" -project out/uv.xcodeproj -configuration Release -alltargets\n```\n\nUsing Homebrew:\n\n```bash\n$ brew install --HEAD libuv\n```\n\nNote to OS X users:\n\nMake sure that you specify the architecture you wish to build for in the\n\"ARCHS\" flag. You can specify more than one by delimiting with a space\n(e.g. \"x86_64 i386\").\n\n### Android\n\nRun:\n\nFor arm\n\n```bash\n$ source ./android-configure-arm NDK_PATH gyp [API_LEVEL]\n$ make -C out\n```\n\nor for arm64\n\n```bash\n$ source ./android-configure-arm64 NDK_PATH gyp [API_LEVEL]\n$ make -C out\n```\n\nor for x86\n\n```bash\n$ source ./android-configure-x86 NDK_PATH gyp [API_LEVEL]\n$ make -C out\n```\n\nor for x86_64\n\n```bash\n$ source ./android-configure-x86_64 NDK_PATH gyp [API_LEVEL]\n$ make -C out\n```\n\nThe default API level is 24, but a different one can be selected as follows:\n\n```bash\n$ source ./android-configure ~/android-ndk-r15b gyp 21\n$ make -C out\n```\n\nNote for UNIX users: compile your project with `-D_LARGEFILE_SOURCE` and\n`-D_FILE_OFFSET_BITS=64`. GYP builds take care of that automatically.\n\n### Using Ninja\n\nTo use ninja for build on ninja supported platforms, run:\n\n```bash\n$ ./gyp_uv.py -f ninja\n$ ninja -C out/Debug     #for debug build OR\n$ ninja -C out/Release\n```\n\n\n### Running tests\n\n#### Build\n\nBuild (includes tests):\n\n```bash\n$ ./gyp_uv.py -f make\n$ make -C out\n```\n\n#### Run all tests\n\n```bash\n$ ./out/Debug/run-tests\n```\n\n#### Run one test\n\nThe list of all tests is in `test/test-list.h`.\n\nThis invocation will cause the `run-tests` driver to fork and execute `TEST_NAME` in a child process:\n\n```bash\n$ ./out/Debug/run-tests TEST_NAME\n```\n\nThis invocation will cause the `run-tests` driver to execute the test within the `run-tests` process:\n\n```bash\n$ ./out/Debug/run-tests TEST_NAME TEST_NAME\n```\n\n#### Debugging tools\n\nWhen running the test from within the `run-tests` process (`run-tests TEST_NAME TEST_NAME`), tools like gdb and valgrind work normally.\nWhen running the test from a child of the `run-tests` process (`run-tests TEST_NAME`), use these tools in a fork-aware manner.\n\n##### Fork-aware gdb\n\nUse the [follow-fork-mode](https://sourceware.org/gdb/onlinedocs/gdb/Forks.html) setting:\n\n```\n$ gdb --args out/Debug/run-tests TEST_NAME\n\n(gdb) set follow-fork-mode child\n...\n```\n\n##### Fork-aware valgrind\n\nUse the `--trace-children=yes` parameter:\n\n```bash\n$ valgrind --trace-children=yes -v --tool=memcheck --leak-check=full --track-origins=yes --leak-resolution=high --show-reachable=yes --log-file=memcheck.log out/Debug/run-tests TEST_NAME\n```\n\n### Running benchmarks\n\nSee the section on running tests.\nThe benchmark driver is `out/Debug/run-benchmarks` and the benchmarks are listed in `test/benchmark-list.h`.\n\n## Supported Platforms\n\nCheck the [SUPPORTED_PLATFORMS file](SUPPORTED_PLATFORMS.md).\n\n### AIX Notes\n\nAIX support for filesystem events requires the non-default IBM `bos.ahafs`\npackage to be installed.  This package provides the AIX Event Infrastructure\nthat is detected by `autoconf`.\n[IBM documentation](http://www.ibm.com/developerworks/aix/library/au-aix_event_infrastructure/)\ndescribes the package in more detail.\n\nAIX support for filesystem events is not compiled when building with `gyp`.\n\n### z/OS Notes\n\nz/OS creates System V semaphores and message queues. These persist on the system\nafter the process terminates unless the event loop is closed.\n\nUse the `ipcrm` command to manually clear up System V resources.\n\n## Patches\n\nSee the [guidelines for contributing][].\n\n[node.js]: http://nodejs.org/\n[GYP]: http://code.google.com/p/gyp/\n[guidelines for contributing]: https://github.com/libuv/libuv/blob/master/CONTRIBUTING.md\n[libuv_banner]: https://raw.githubusercontent.com/libuv/libuv/master/img/banner.png\n[x32]: https://en.wikipedia.org/wiki/X32_ABI\n[Python 2.6 or 2.7]: https://www.python.org/downloads/\n[Visual C++ Build Tools]: https://visualstudio.microsoft.com/visual-cpp-build-tools/\n[Visual Studio 2015 Update 3]: https://www.visualstudio.com/vs/older-downloads/\n[Visual Studio 2017]: https://www.visualstudio.com/downloads/\n[Git for Windows]: http://git-scm.com/download/win\n"
  },
  {
    "repo": "pjreddie/darknet",
    "content": "![Darknet Logo](http://pjreddie.com/media/files/darknet-black-small.png)\n\n# Darknet #\nDarknet is an open source neural network framework written in C and CUDA. It is fast, easy to install, and supports CPU and GPU computation.\n\nFor more information see the [Darknet project website](http://pjreddie.com/darknet).\n\nFor questions or issues please use the [Google Group](https://groups.google.com/forum/#!forum/darknet).\n"
  },
  {
    "repo": "Genymobile/scrcpy",
    "content": "# scrcpy (v1.5)\n\nThis application provides display and control of Android devices connected on\nUSB (or [over TCP/IP][article-tcpip]). It does not require any _root_ access.\nIt works on _GNU/Linux_, _Windows_ and _MacOS_.\n\n![screenshot](assets/screenshot-debian-600.jpg)\n\n\n## Requirements\n\nThe Android part requires at least API 21 (Android 5.0).\n\nMake sure you [enabled adb debugging][enable-adb] on your device(s).\n\n[enable-adb]: https://developer.android.com/studio/command-line/adb.html#Enabling\n\nOn some devices, you also need to enable [an additional option][control] to\ncontrol it using keyboard and mouse.\n\n[control]: https://github.com/Genymobile/scrcpy/issues/70#issuecomment-373286323\n\n\n## Get the app\n\n\n### Linux\n\nOn Linux, you typically need to [build the app manually][BUILD]. Don't worry,\nit's not that hard.\n\nFor Arch Linux, two [AUR] packages have been created by users:\n\n - [`scrcpy`](https://aur.archlinux.org/packages/scrcpy/)\n - [`scrcpy-prebuiltserver`](https://aur.archlinux.org/packages/scrcpy-prebuiltserver/)\n\n[AUR]: https://wiki.archlinux.org/index.php/Arch_User_Repository\n\nFor Gentoo, an [Ebuild] is available: [`scrcpy/`][ebuild-link].\n\n[Ebuild]: https://wiki.gentoo.org/wiki/Ebuild\n[ebuild-link]: https://github.com/maggu2810/maggu2810-overlay/tree/master/app-mobilephone/scrcpy\n\n\n### Windows\n\nFor Windows, for simplicity, prebuilt archives with all the dependencies\n(including `adb`) are available:\n\n - [`scrcpy-win32-v1.5.zip`][direct-win32]  \n   _(SHA-256: 46ae0d4c1c6bd049ec4a30080d2ad91a32b31d3f758afdca2c3a915ecabf02c1)_\n - [`scrcpy-win64-v1.5.zip`][direct-win64]  \n   _(SHA-256: 89daa07325129617cf943a84bc4e304ee5e57118416fe265b9b5d4a1bf87c501)_\n\n[direct-win32]: https://github.com/Genymobile/scrcpy/releases/download/v1.5-fixversion/scrcpy-win32-v1.5.zip\n[direct-win64]: https://github.com/Genymobile/scrcpy/releases/download/v1.5-fixversion/scrcpy-win64-v1.5.zip\n\nYou can also [build the app manually][BUILD].\n\n\n### Mac OS\n\nThe application is available in [Homebrew]. Just install it:\n\n[Homebrew]: https://brew.sh/\n\n```bash\nbrew install scrcpy\n```\n\nYou need `adb`, accessible from your `PATH`. If you don't have it yet:\n\n```bash\nbrew cask install android-platform-tools\n```\n\nYou can also [build the app manually][BUILD].\n\n\n## Run\n\nPlug an Android device, and execute:\n\n```bash\nscrcpy\n```\n\nIt accepts command-line arguments, listed by:\n\n```bash\nscrcpy --help\n```\n\n## Features\n\n\n### Reduce size\n\nSometimes, it is useful to mirror an Android device at a lower definition to\nincrease performances.\n\nTo limit both width and height to some value (e.g. 1024):\n\n```bash\nscrcpy --max-size 1024\nscrcpy -m 1024  # short version\n```\n\nThe other dimension is computed to that the device aspect-ratio is preserved.\nThat way, a device in 1920×1080 will be mirrored at 1024×576.\n\n\n### Change bit-rate\n\nThe default bit-rate is 8Mbps. To change the video bitrate (e.g. to 2Mbps):\n\n```bash\nscrcpy --bit-rate 2M\nscrcpy -b 2M  # short version\n```\n\n\n### Crop\n\nThe device screen may be cropped to mirror only part of the screen.\n\nThis is useful for example to mirror only 1 eye of the Oculus Go:\n\n```bash\nscrcpy --crop 1224:1440:0:0   # 1224x1440 at offset (0,0)\nscrcpy -c 1224:1440:0:0       # short version\n```\n\nIf `--max-size` is also specified, resizing is applied after cropping.\n\n\n### Wireless\n\n_Scrcpy_ uses `adb` to communicate with the device, and `adb` can [connect] to a\ndevice over TCP/IP:\n\n1. Connect the device to the same Wi-Fi as your computer.\n2. Get your device IP address (in Settings → About phone → Status).\n3. Enable adb over TCP/IP on your device: `adb tcpip 5555`.\n4. Unplug your device.\n5. Connect to your device: `adb connect DEVICE_IP:5555` _(replace `DEVICE_IP`)_.\n6. Run `scrcpy` as usual.\n\nIt may be useful to decrease the bit-rate and the definition:\n\n```bash\nscrcpy --bit-rate 2M --max-size 800\nscrcpy -b2M -m800  # short version\n```\n\n[connect]: https://developer.android.com/studio/command-line/adb.html#wireless\n\n\n### Record screen\n\nIt is possible to record the screen while mirroring:\n\n```bash\nscrcpy --record file.mp4\nscrcpy -r file.mp4\n```\n\n\"Skipped frames\" are recorded, even if they are not displayed in real time (for\nperformance reasons). Frames are _timestamped_ on the device, so [packet delay\nvariation] does not impact the recorded file.\n\n[packet delay variation]: https://en.wikipedia.org/wiki/Packet_delay_variation\n\n\n### Multi-devices\n\nIf several devices are listed in `adb devices`, you must specify the _serial_:\n\n```bash\nscrcpy --serial 0123456789abcdef\nscrcpy -s 0123456789abcdef  # short version\n```\n\nYou can start several instances of _scrcpy_ for several devices.\n\n\n### Fullscreen\n\nThe app may be started directly in fullscreen:\n\n```bash\nscrcpy --fullscreen\nscrcpy -f  # short version\n```\n\nFullscreen can then be toggled dynamically with `Ctrl`+`f`.\n\n\n### Show touches\n\nFor presentations, it may be useful to show physical touches (on the physical\ndevice).\n\nAndroid provides this feature in _Developers options_.\n\n_Scrcpy_ provides an option to enable this feature on start and disable on exit:\n\n```bash\nscrcpy --show-touches\nscrcpy -t\n```\n\nNote that it only shows _physical_ touches (with the finger on the device).\n\n\n### Install APK\n\nTo install an APK, drag \u0026 drop an APK file (ending with `.apk`) to the _scrcpy_\nwindow.\n\nThere is no visual feedback, a log is printed to the console.\n\n\n### Push file to device\n\nTo push a file to `/sdcard/` on the device, drag \u0026 drop a (non-APK) file to the\n_scrcpy_ window.\n\nThere is no visual feedback, a log is printed to the console.\n\n\n### Forward audio\n\nAudio is not forwarded by _scrcpy_.\n\nThere is a limited solution using [AOA], implemented in the [`audio`] branch. If\nyou are interested, see [issue 14].\n\n\n[AOA]: https://source.android.com/devices/accessories/aoa2\n[`audio`]: https://github.com/Genymobile/scrcpy/commits/audio\n[issue 14]: https://github.com/Genymobile/scrcpy/issues/14\n\n\n## Shortcuts\n\n | Action                                 |   Shortcut                    |\n | -------------------------------------- |:----------------------------  |\n | switch fullscreen mode                 | `Ctrl`+`f`                    |\n | resize window to 1:1 (pixel-perfect)   | `Ctrl`+`g`                    |\n | resize window to remove black borders  | `Ctrl`+`x` \\| _Double-click¹_ |\n | click on `HOME`                        | `Ctrl`+`h` \\| _Middle-click_  |\n | click on `BACK`                        | `Ctrl`+`b` \\| _Right-click²_  |\n | click on `APP_SWITCH`                  | `Ctrl`+`s`                    |\n | click on `MENU`                        | `Ctrl`+`m`                    |\n | click on `VOLUME_UP`                   | `Ctrl`+`↑` _(up)_   (`Cmd`+`↑` on MacOS) |\n | click on `VOLUME_DOWN`                 | `Ctrl`+`↓` _(down)_ (`Cmd`+`↓` on MacOS) |\n | click on `POWER`                       | `Ctrl`+`p`                    |\n | turn screen on                         | _Right-click²_                |\n | paste computer clipboard to device     | `Ctrl`+`v`                    |\n | enable/disable FPS counter (on stdout) | `Ctrl`+`i`                    |\n\n_¹Double-click on black borders to remove them._  \n_²Right-click turns the screen on if it was off, presses BACK otherwise._\n\n\n## Why _scrcpy_?\n\nA colleague challenged me to find a name as unpronounceable as [gnirehtet].\n\n[`strcpy`] copies a **str**ing; `scrcpy` copies a **scr**een.\n\n[gnirehtet]: https://github.com/Genymobile/gnirehtet\n[`strcpy`]: http://man7.org/linux/man-pages/man3/strcpy.3.html\n\n\n## How to build?\n\nSee [BUILD].\n\n[BUILD]: BUILD.md\n\n\n## Common issues\n\nSee the [FAQ](FAQ.md).\n\n\n## Developers\n\nRead the [developers page].\n\n[developers page]: DEVELOP.md\n\n\n## Licence\n\n    Copyright (C) 2018 Genymobile\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n        http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n## Articles\n\n- [Introducing scrcpy][article-intro]\n- [Scrcpy now works wirelessly][article-tcpip]\n\n[article-intro]: https://blog.rom1v.com/2018/03/introducing-scrcpy/\n[article-tcpip]: https://www.genymotion.com/blog/open-source-project-scrcpy-now-works-wirelessly/\n"
  },
  {
    "repo": "robertdavidgraham/masscan",
    "content": "# MASSCAN: Mass IP port scanner\n\nThis is the fastest Internet port scanner. It can scan the entire Internet\nin under 6 minutes, transmitting 10 million packets per second.\n\nIt produces results similar to `nmap`, the most famous port scanner.\nInternally, it operates more like `scanrand`, `unicornscan`, and `ZMap`, using\nasynchronous transmission. The major difference is that it's faster than these\nother scanners. In addition, it's more flexible, allowing arbitrary address\nranges and port ranges.\n\nNOTE: masscan uses a **custom TCP/IP stack**. Anything other than simple port\nscans will cause conflict with the local TCP/IP stack. This means you need to\neither use the `-S` option to use a separate IP address, or configure your\noperating system to firewall the ports that masscan uses.\n\nThis tool is free, but consider funding it here:\nBitcoin wallet address: 1MASSCANaHUiyTtR3bJ2sLGuMw5kDBaj4T\n\n\n# Building\n\nOn Debian/Ubuntu, it goes something like this:\n\n\t$ sudo apt-get install git gcc make libpcap-dev\n\t$ git clone https://github.com/robertdavidgraham/masscan\n\t$ cd masscan\n\t$ make\n\nThis puts the program in the `masscan/bin` subdirectory. You'll have to\nmanually copy it to something like `/usr/local/bin` if you want to\ninstall it elsewhere on the system.\n\nThe source consists of a lot of small files, so building goes a lot faster\nby using the multi-threaded build:\n\n\t$ make -j\n\nWhile Linux is the primary target platform, the code runs well on many other\nsystems. Here's some additional build info:\n* Windows w/ Visual Studio: use the VS10 project\n* Windows w/ MingGW: just type `make`\n* Windows w/ cygwin: won't work\n* Mac OS X /w XCode: use the XCode4 project\n* Mac OS X /w cmdline: just type `make`\n* FreeBSD: type `gmake`\n* other: I don't know, don't care\n\n\n## PF_RING\n\nTo get beyond 2 million packets/second, you need an Intel 10-gbps Ethernet\nadapter and a special driver known as [\"PF_RING ZC\" from ntop](http://www.ntop.org/products/packet-capture/pf_ring/pf_ring-zc-zero-copy/). Masscan doesn't need to be rebuilt in order to use PF_RING. To use PF_RING,\nyou need to build the following components:\n\n* `libpfring.so` (installed in /usr/lib/libpfring.so)\n* `pf_ring.ko` (their kernel driver)\n* `ixgbe.ko` (their version of the Intel 10-gbps Ethernet driver)\n\nYou don't need to build their version of `libpcap.so`.\n\nWhen Masscan detects that an adapter is named something like `zc:enp1s0` instead\nof something like `enp1s0`, it'll automatically switch to PF_RING ZC mode.\n\n## Regression testing\n\nThe project contains a built-in self-test:\n\n\t$ make regress\n\tbin/masscan --regress\n\tselftest: success!\n\nThis tests a lot of tricky bits of the code. You should do this after building.\n\n\n## Performance testing\n\nTo test performance, run something like the following:\n\n\t$ bin/masscan 0.0.0.0/4 -p80 --rate 100000000 --router-mac 66-55-44-33-22-11\n\nThe bogus `--router-mac` keeps packets on the local network segments so that\nthey won't go out to the Internet.\n\nYou can also test in \"offline\" mode, which is how fast the program runs\nwithout the transmit overhead:\n\n\t$ bin/masscan 0.0.0.0/4 -p80 --rate 100000000 --offline\n    \nThis second benchmark shows roughly how fast the program would run if it were\nusing PF_RING, which has near zero overhead.\n\n\n# Usage\n\nUsage is similar to `nmap`. To scan a network segment for some ports:\n\n\t# masscan -p80,8000-8100 10.0.0.0/8\n\nThis will:\n* scan the 10.x.x.x subnet, all 16 million addresses\n* scans port 80 and the range 8000 to 8100, or 102 addresses total\n* print output to `\u003cstdout\u003e` that can be redirected to a file\n\nTo see the complete list of options, use the `--echo` feature. This\ndumps the current configuration and exits. This output can be used as input back\ninto the program:\n\n\t# masscan -p80,8000-8100 10.0.0.0/8 --echo \u003e xxx.conf\n\t# masscan -c xxx.conf --rate 1000\n\n\n## Banner checking\n\nMasscan can do more than just detect whether ports are open. It can also\ncomplete the TCP connection and interaction with the application at that\nport in order to grab simple \"banner\" information.\n\nThe problem with this is that masscan contains its own TCP/IP stack\nseparate from the system you run it on. When the local system receives\na SYN-ACK from the probed target, it responds with a RST packet that kills\nthe connection before masscan can grab the banner.\n\nThe easiest way to prevent this is to assign masscan a separate IP\naddress. This would look like the following:\n\n\t# masscan 10.0.0.0/8 -p80 --banners --source-ip 192.168.1.200\n\nThe address you choose has to be on the local subnet and not otherwise\nbe used by another system.\n\nIn some cases, such as WiFi, this isn't possible. In those cases, you can\nfirewall the port that masscan uses. This prevents the local TCP/IP stack\nfrom seeing the packet, but masscan still sees it since it bypasses the\nlocal stack. For Linux, this would look like:\n\n\t# iptables -A INPUT -p tcp --dport 61000 -j DROP\n\t# masscan 10.0.0.0/8 -p80 --banners --source-port 61000\n\nYou probably want to pick ports that don't conflict with ports Linux might otherwise\nchoose for source-ports. You can see the range Linux uses, and reconfigure\nthat range, by looking in the file:\n\n    /proc/sys/net/ipv4/ip_local_port_range\n\nOn the latest version of Kali Linux (2018-August), that range is  32768  to  60999, so\nyou should choose ports either below 32768 or 61000 and above.\n\nSetting an `iptables` rule only lasts until the next reboot. You need to lookup how to\nsave the configuration depending upon your distro, such as using `iptables-save` \nand/or `iptables-persistant`.\n\nOn Mac OS X and BSD, there are similar steps. To find out the ranges to avoid,\nuse a command like the following:\n\n    # sysctl net.inet.ip.portrange.first net.inet.ip.portrange.last\n\nOn FreeBSD and older MacOS, use an `ipfw` command: \n\n\t# sudo ipfw add 1 deny tcp from any to any 40000 in\n\t# masscan 10.0.0.0/8 -p80 --banners --source-port 40000\n\nOn newer MacOS and OpenBSD, use the `pf` packet-filter utility. \nEdit the file `/etc/pf.conf` to add a line like the following:\n\n    block in proto tcp from any to any port 40000\n    \nThen to enable the firewall, run the command:\n    \n    # pfctrl -E    \n\nIf the firewall is already running, then either reboot or reload the rules\nwith the following command:\n\n    # pfctl -f /etc/pf.conf\n\nWindows doesn't respond with RST packets, so neither of these techniques\nare necessary. However, masscan is still designed to work best using its\nown IP address, so you should run that way when possible, even when its\nnot strictly necessary.\n\nThe same thing is needed for other checks, such as the `--heartbleed` check,\nwhich is just a form of banner checking.\n\n\n## How to scan the entire Internet\n\nWhile useful for smaller, internal networks, the program is really designed\nwith the entire Internet in mind. It might look something like this:\n\n\t# masscan 0.0.0.0/0 -p0-65535\n\nScanning the entire Internet is bad. For one thing, parts of the Internet react\nbadly to being scanned. For another thing, some sites track scans and add you\nto a ban list, which will get you firewalled from useful parts of the Internet.\nTherefore, you want to exclude a lot of ranges. To blacklist or exclude ranges,\nyou want to use the following syntax:\n\n\t# masscan 0.0.0.0/0 -p0-65535 --excludefile exclude.txt\n\nThis just prints the results to the command-line. You probably want them\nsaved to a file instead. Therefore, you want something like:\n\n\t# masscan 0.0.0.0/0 -p0-65535 -oX scan.xml\n\nThis saves the results in an XML file, allowing you to easily dump the\nresults in a database or something.\n\nBut, this only goes at the default rate of 100 packets/second, which will\ntake forever to scan the Internet. You need to speed it up as so:\n\n\t# masscan 0.0.0.0/0 -p0-65535 --max-rate 100000\n\nThis increases the rate to 100,000 packets/second, which will scan the\nentire Internet (minus excludes) in about 10 hours per port (or 655,360 hours\nif scanning all ports).\n\nThe thing to notice about this command-line is that these are all `nmap`\ncompatible options. In addition, \"invisible\" options compatible with `nmap`\nare also set for you: `-sS -Pn -n --randomize-hosts --send-eth`. Likewise,\nthe format of the XML file is inspired by `nmap`. There are, of course, a\nlot of differences, because the *asynchronous* nature of the program\nleads to a fundamentally different approach to the problem.\n\nThe above command-line is a bit cumbersome. Instead of putting everything\non the command-line, it can be stored in a file instead. The above settings\nwould look like this:\n\n\t# My Scan\n\trate =  100000.00\n\toutput-format = xml\n\toutput-status = all\n\toutput-filename = scan.xml\n\tports = 0-65535\n\trange = 0.0.0.0-255.255.255.255\n\texcludefile = exclude.txt\n\nTo use this configuration file, use the `-c`:\n\n\t# masscan -c myscan.conf\n\nThis also makes things easier when you repeat a scan.\n\nBy default, masscan first loads the configuration file \n`/etc/masscan/masscan.conf`. Any later configuration parameters override what's\nin this default configuration file. That's where I put my \"excludefile\" \nparameter, so that I don't ever forget it. It just works automatically.\n\n## Getting output\n\nBy default, masscan produces fairly large text files, but it's easy \nto convert them into any other format. There are five supported output formats:\n\n1. xml:  Just use the parameter `-oX \u003cfilename\u003e`. \n\tOr, use the parameters `--output-format xml` and `--output-filename \u003cfilename\u003e`.\n\n2. binary: This is the masscan builtin format. It produces much smaller files, so that\nwhen I scan the Internet my disk doesn't fill up. They need to be parsed,\nthough. The command line option `--readscan` will read binary scan files.\nUsing `--readscan` with the `-oX` option will produce a XML version of the \nresults file.\n\n3. grepable: This is an implementation of the Nmap -oG\noutput that can be easily parsed by command-line tools. Just use the\nparameter `-oG \u003cfilename\u003e`. Or, use the parameters `--output-format grepable` and\n`--output-filename \u003cfilename\u003e`.\n\n4. json: This saves the results in JSON format. Just use the\nparameter `-oJ \u003cfilename\u003e`. Or, use the parameters `--output-format json` and\n`--output-filename \u003cfilename\u003e`.\n\n5. list: This is a simple list with one host and port pair \nper line. Just use the parameter `-oL \u003cfilename\u003e`. Or, use the parameters \n`--output-format list` and `--output-filename \u003cfilename\u003e`. The format is:\n\n\t```\n\t\u003cport state\u003e \u003cprotocol\u003e \u003cport number\u003e \u003cIP address\u003e \u003cPOSIX timestamp\u003e  \n\topen tcp 80 XXX.XXX.XXX.XXX 1390380064\n\t```\t\n\n## Comparison with Nmap\n\nWhere reasonable, every effort has been taken to make the program familiar\nto `nmap` users, even though it's fundamentally different. Two important\ndifferences are:\n\n* no default ports to scan, you must specify `-p \u003cports\u003e`\n* target hosts are IP addresses or simple ranges, not DNS names, nor \n  the funky subnet ranges `nmap` can use (like `10.0.0-255.0-255`).\n\nYou can think of `masscan` as having the following settings permanently\nenabled:\n* `-sS`: this does SYN scan only (currently, will change in the future)\n* `-Pn`: doesn't ping hosts first, which is fundamental to the async operation\n* `-n`: no DNS resolution happens\n* `--randomize-hosts`: scan completely randomized\n* `--send-eth`: sends using raw `libpcap`\n\nIf you want a list of additional `nmap` compatible settings, use the following\ncommand:\n\n\t# masscan --nmap\n\n\n## Transmit rate (IMPORTANT!!)\n\nThis program spews out packets very fast. On Windows, or from VMs,\nit can do 300,000 packets/second. On Linux (no virtualization) it'll\ndo 1.6 million packets-per-second. That's fast enough to melt most networks.\n\nNote that it'll only melt your own network. It randomizes the target\nIP addresses so that it shouldn't overwhelm any distant network.\n\nBy default, the rate is set to 100 packets/second. To increase the rate to\na million use something like `--rate 1000000`.\n\n\n\n# Design\n\nThis section describes the major design issues of the program.\n\n## Code Layout\n\nThe file `main.c` contains the `main()` function, as you'd expect. It also\ncontains the `transmit_thread()` and `receive_thread()` functions. These\nfunctions have been deliberately flattened and heavily commented so that you\ncan read the design of the program simply by stepping line-by-line through\neach of these.\n\n## Asynchronous\n\nThis is an *asynchronous* design. In other words, it is to `nmap` what\nthe `nginx` web-server is to `Apache`. It has separate transmit and receive\nthreads that are largely independent from each other. It's the same sort of\ndesign found in `scanrand`, `unicornscan`, and `ZMap`.\n\nBecause it's asynchronous, it runs as fast as the underlying packet transmit\nallows.\n\n\n## Randomization\n\nA key difference between Masscan and other scanners is the way it randomizes\ntargets.\n\nThe fundamental principle is to have a single index variable that starts at\nzero and is incremented by one for every probe. In C code, this is expressed\nas:\n\n    for (i = 0; i \u003c range; i++) {\n        scan(i);\n    }\n\nWe have to translate the index into an IP address. Let's say that you want to\nscan all \"private\" IP addresses. That would be the table of ranges like:\n    \n    192.168.0.0/16\n    10.0.0.0/8\n    172.16.0.0/12\n\nIn this example, the first 64k indexes are appended to 192.168.x.x to form\nthe target address. Then, the next 16-million are appended to 10.x.x.x.\nThe remaining indexes in the range are applied to 172.16.x.x.\n\nIn this example, we only have three ranges. When scanning the entire Internet,\nwe have in practice more than 100 ranges. That's because you have to blacklist\nor exclude a lot of sub-ranges. This chops up the desired range into hundreds\nof smaller ranges.\n\nThis leads to one of the slowest parts of the code. We transmit 10 million\npackets per second, and have to convert an index variable to an IP address\nfor each and every probe. We solve this by doing a \"binary search\" in a small\namount of memory. At this packet rate, cache efficiencies start to dominate\nover algorithm efficiencies. There are a lot of more efficient techniques in\ntheory, but they all require so much memory as to be slower in practice.\n\nWe call the function that translates from an index into an IP address\nthe `pick()` function. In use, it looks like:\n\n    for (i = 0; i \u003c range; i++) {\n        ip = pick(addresses, i);\n        scan(ip);\n    }\n\nMasscan supports not only IP address ranges, but also port ranges. This means\nwe need to pick from the index variable both an IP address and a port. This\nis fairly straightforward:\n\n    range = ip_count * port_count;\n    for (i = 0; i \u003c range; i++) {\n        ip   = pick(addresses, i / port_count);\n        port = pick(ports,     i % port_count);\n        scan(ip, port);\n    }\n\nThis leads to another expensive part of the code. The division/modulus\ninstructions are around 90 clock cycles, or 30 nanoseconds, on x86 CPUs. When\ntransmitting at a rate of 10 million packets/second, we have only\n100 nanoseconds per packet. I see no way to optimize this any better. Luckily,\nthough, two such operations can be executed simultaneously, so doing two \nof these as shown above is no more expensive than doing one.\n\nThere are actually some easy optimizations for the above performance problems,\nbut they all rely upon `i++`, the fact that the index variable increases one\nby one through the scan. Actually, we need to randomize this variable. We\nneed to randomize the order of IP addresses that we scan or we'll blast the\nheck out of target networks that aren't built for this level of speed. We \nneed to spread our traffic evenly over the target.\n\nThe way we randomize is simply by encrypting the index variable. By definition,\nencryption is random, and creates a 1-to-1 mapping between the original index\nvariable and the output. This means that while we linearly go through the\nrange, the output IP addresses are completely random. In code, this looks like:\n\n    range = ip_count * port_count;\n    for (i = 0; i \u003c range; i++) {\n        x = encrypt(i);\n        ip   = pick(addresses, x / port_count);\n        port = pick(ports,     x % port_count);\n        scan(ip, port);\n    }\n\nThis also has a major cost. Since the range is an unpredictable size instead\nof a nice even power of 2, we can't use cheap binary techniques like\nAND (\u0026) and XOR (^). Instead, we have to use expensive operations like \nMODULUS (%). In my current benchmarks, it's taking 40 nanoseconds to\nencrypt the variable.\n\nThis architecture allows for lots of cool features. For example, it supports\n\"shards\". You can setup 5 machines each doing a fifth of the scan, or\n`range / shard_count`. Shards can be multiple machines, or simply multiple\nnetwork adapters on the same machine, or even (if you want) multiple IP\nsource addresses on the same network adapter.\n\nOr, you can use a 'seed' or 'key' to the encryption function, so that you get\na different order each time you scan, like `x = encrypt(seed, i)`.\n\nWe can also pause the scan by exiting out of the program, and simply\nremembering the current value of `i`, and restart it later. I do that a lot\nduring development. I see something going wrong with my Internet scan, so\nI hit \u003cctrl-c\u003e to stop the scan, then restart it after I've fixed the bug.\n\nAnother feature is retransmits/retries. Packets sometimes get dropped on the\nInternet, so you can send two packets back-to-back. However, something that\ndrops one packet may drop the immediately following packet. Therefore, you\nwant to send the copy about 1 second apart. This is simple. We already have\na 'rate' variable, which is the number of packets-per-second rate we are\ntransmitting at, so the retransmit function is simply to use `i + rate`\nas the index. One of these days I'm going to do a study of the Internet,\nand differentiate \"back-to-back\", \"1 second\", \"10 second\", and \"1 minute\"\nretransmits this way in order to see if there is any difference in what\ngets dropped.\n\n\n\n## C10 Scalability\n\nThe asynchronous technique is known as a solution to the \"c10k problem\".\nMasscan is designed for the next level of scalability, the \"C10M problem\".\n\nThe C10M solution is to bypass the kernel. There are three primary kernel\nbypasses in Masscan:\n* custom network driver\n* user-mode TCP stack\n* user-mode synchronization\n\nMasscan can use the PF_RING DNA driver. This driver DMAs packets directly\nfrom user-mode memory to the network driver with zero kernel involvement.\nThat allows software, even with a slow CPU, to transmit packets at the maximum\nrate the hardware allows. If you put 8 10-gbps network cards in a computer,\nthis means it could transmit at 100-million packets/second.\n\nMasscan has its own built-in TCP stack for grabbing banners from TCP\nconnections. This means it can easily support 10 million concurrent TCP\nconnections, assuming of course that the computer has enough memory.\n\nMasscan has no \"mutex\". Modern mutexes (aka. futexes) are mostly user-mode,\nbut they have two problems. The first problem is that they cause cache-lines\nto bounce quickly back-and-forth between CPUs. The second is that when there\nis contention, they'll do a system call into the kernel, which kills\nperformance. Mutexes on the fast path of a program severely limits scalability.\nInstead, Masscan uses \"rings\" to synchronize things, such as when the\nuser-mode TCP stack in the receive thread needs to transmit a packet without\ninterfering with the transmit thread.\n\n\n## Portability\n\nThe code runs well on Linux, Windows, and Mac OS X. All the important bits are\nin standard C (C90). It therefore compiles on Visual Studio with Microsoft's\ncompiler, the Clang/LLVM compiler on Mac OS X, and GCC on Linux.\n\nWindows and Macs aren't tuned for packet transmit, and get only about 300,000\npackets-per-second, whereas Linux can do 1,500,000 packets/second. That's\nprobably faster than you want anyway.\n\n\n## Safe code\n\nA bounty is offered for vulnerabilities, see the VULNINFO.md file for more\ninformation.\n\nThis project uses safe functions like `strcpy_s()` instead of unsafe functions\nlike `strcpy()`.\n\nThis project has automated unit regression tests (`make regress`).\n\n\n## Compatibility\n\nA lot of effort has gone into making the input/output look like `nmap`, which\neveryone who does port scans is (or should be) familiar with.\n\n\n# Authors\n\nThis tool created by Robert Graham:\nemail: robert_david_graham@yahoo.com\ntwitter: @ErrataRob\n\n"
  },
  {
    "repo": "numpy/numpy",
    "content": "# \u003cimg alt=\"NumPy\" src=\"https://cdn.rawgit.com/numpy/numpy/master/branding/icons/numpylogo.svg\" height=\"60\"\u003e\n\n[![Travis](https://img.shields.io/travis/numpy/numpy/master.svg?label=Travis%20CI)](\n    https://travis-ci.org/numpy/numpy)\n[![AppVeyor](https://img.shields.io/appveyor/ci/charris/numpy/master.svg?label=AppVeyor)](\n    https://ci.appveyor.com/project/charris/numpy)\n[![Azure](https://dev.azure.com/numpy/numpy/_apis/build/status/azure-pipeline%20numpy.numpy)](\n    https://dev.azure.com/numpy/numpy/_build/latest?definitionId=5)\n[![codecov](https://codecov.io/gh/numpy/numpy/branch/master/graph/badge.svg)](\n    https://codecov.io/gh/numpy/numpy)\n\nNumPy is the fundamental package needed for scientific computing with Python.\n\n- **Website (including documentation):** https://www.numpy.org\n- **Mailing list:** https://mail.python.org/mailman/listinfo/numpy-discussion\n- **Source:** https://github.com/numpy/numpy\n- **Bug reports:** https://github.com/numpy/numpy/issues\n\nIt provides:\n\n- a powerful N-dimensional array object\n- sophisticated (broadcasting) functions\n- tools for integrating C/C++ and Fortran code\n- useful linear algebra, Fourier transform, and random number capabilities\n\nTesting:\n\n- NumPy versions \u0026ge; 1.15 require `pytest`\n- NumPy versions \u0026lt; 1.15 require `nose`\n\nTests can then be run after installation with:\n\n    python -c 'import numpy; numpy.test()'\n\n[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat\u0026colorA=E1523D\u0026colorB=007D8A)](https://numfocus.org)\n"
  },
  {
    "repo": "commaai/openpilot",
    "content": "[![](https://i.imgur.com/UetIFyH.jpg)](#)\n\nWelcome to openpilot\n======\n\n[openpilot](http://github.com/commaai/openpilot) is an open source driving agent. Currently, it performs the functions of Adaptive Cruise Control (ACC) and Lane Keeping Assist System (LKAS) for selected Honda, Toyota, Acura, Lexus, Chevrolet, Hyundai, Kia. It's about on par with Tesla Autopilot and GM Super Cruise, and better than [all other manufacturers](http://www.thedrive.com/tech/5707/the-war-for-autonomous-driving-part-iii-us-vs-germany-vs-japan).\n\nThe openpilot codebase has been written to be concise and to enable rapid prototyping. We look forward to your contributions - improving real vehicle automation has never been easier.\n\nTable of Contents\n=======================\n\n* [Community](#community)\n* [Hardware](#hardware)\n* [Supported Cars](#supported-cars)\n* [Community Maintained Cars](#community-maintained-cars)\n* [In Progress Cars](#in-progress-cars)\n* [How can I add support for my car?](#how-can-i-add-support-for-my-car-)\n* [Directory structure](#directory-structure)\n* [User Data / chffr Account / Crash Reporting](#user-data-chffr-account-crash-reporting)\n* [Testing on PC](#testing-on-pc)\n* [Contributing](#contributing)\n* [Licensing](#licensing)\n\n---\n\nCommunity\n------\n\nopenpilot is developed by [comma.ai](https://comma.ai/) and users like you.\n\nWe have a [Twitter you should follow](https://twitter.com/comma_ai).\n\nAlso, we have a several thousand people community on [slack](https://slack.comma.ai).\n\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e\u003ca href=\"https://www.youtube.com/watch?v=ICOIin4p70w\" title=\"YouTube\" rel=\"noopener\"\u003e\u003cimg src=\"https://i.imgur.com/gBTo7yB.png\"\u003e\u003c/a\u003e\u003c/td\u003e\n    \u003ctd\u003e\u003ca href=\"https://www.youtube.com/watch?v=1zCtj3ckGFo\" title=\"YouTube\" rel=\"noopener\"\u003e\u003cimg src=\"https://i.imgur.com/gNhhcep.png\"\u003e\u003c/a\u003e\u003c/td\u003e\n    \u003ctd\u003e\u003ca href=\"https://www.youtube.com/watch?v=Qd2mjkBIRx0\" title=\"YouTube\" rel=\"noopener\"\u003e\u003cimg src=\"https://i.imgur.com/tFnSexp.png\"\u003e\u003c/a\u003e\u003c/td\u003e\n    \u003ctd\u003e\u003ca href=\"https://www.youtube.com/watch?v=ju12vlBm59E\" title=\"YouTube\" rel=\"noopener\"\u003e\u003cimg src=\"https://i.imgur.com/3BKiJVy.png\"\u003e\u003c/a\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e\u003ca href=\"https://www.youtube.com/watch?v=Z5VY5FzgNt4\" title=\"YouTube\" rel=\"noopener\"\u003e\u003cimg src=\"https://i.imgur.com/3I9XOK2.png\"\u003e\u003c/a\u003e\u003c/td\u003e\n    \u003ctd\u003e\u003ca href=\"https://www.youtube.com/watch?v=blnhZC7OmMg\" title=\"YouTube\" rel=\"noopener\"\u003e\u003cimg src=\"https://i.imgur.com/f9IgX6s.png\"\u003e\u003c/a\u003e\u003c/td\u003e\n    \u003ctd\u003e\u003ca href=\"https://www.youtube.com/watch?v=iRkz7FuJsA8\" title=\"YouTube\" rel=\"noopener\"\u003e\u003cimg src=\"https://i.imgur.com/Vo5Zvmn.png\"\u003e\u003c/a\u003e\u003c/td\u003e\n    \u003ctd\u003e\u003ca href=\"https://www.youtube.com/watch?v=IHjEqAKDqjM\" title=\"YouTube\" rel=\"noopener\"\u003e\u003cimg src=\"https://i.imgur.com/V9Zd81n.png\"\u003e\u003c/a\u003e\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\nHardware\n------\n\nAt the moment openpilot supports the [EON Dashcam DevKit](https://comma.ai/shop/products/eon-dashcam-devkit). A [panda](https://shop.comma.ai/products/panda-obd-ii-dongle) and a [giraffe](https://comma.ai/shop/products/giraffe/) are recommended tools to interface the EON with the car. We'd like to support other platforms as well.\n\nInstall openpilot on a neo device by entering ``https://openpilot.comma.ai`` during NEOS setup.\n\nSupported Cars\n------\n\n| Make                 | Model                    | Supported Package    | Lateral | Longitudinal   | No Accel Below   | No Steer Below | Giraffe           |\n| ---------------------| -------------------------| ---------------------| --------| ---------------| -----------------| ---------------|-------------------|\n| Acura                | ILX 2016-17              | AcuraWatch Plus      | Yes     | Yes            | 25mph\u003csup\u003e1\u003c/sup\u003e| 25mph          | Nidec             |\n| Acura                | RDX 2018                 | AcuraWatch Plus      | Yes     | Yes            | 25mph\u003csup\u003e1\u003c/sup\u003e| 12mph          | Nidec             |\n| Chevrolet\u003csup\u003e3\u003c/sup\u003e| Malibu 2017              | Adaptive Cruise      | Yes     | Yes            | 0mph             | 7mph           | Custom\u003csup\u003e7\u003c/sup\u003e|\n| Chevrolet\u003csup\u003e3\u003c/sup\u003e| Volt 2017-18             | Adaptive Cruise      | Yes     | Yes            | 0mph             | 7mph           | Custom\u003csup\u003e7\u003c/sup\u003e|\n| Cadillac\u003csup\u003e3\u003c/sup\u003e | ATS 2018                 | Adaptive Cruise      | Yes     | Yes            | 0mph             | 7mph           | Custom\u003csup\u003e7\u003c/sup\u003e|\n| GMC\u003csup\u003e3\u003c/sup\u003e      | Acadia Denali 2018       | Adaptive Cruise      | Yes     | Yes            | 0mph             | 7mph           | Custom\u003csup\u003e7\u003c/sup\u003e|\n| Holden\u003csup\u003e3\u003c/sup\u003e   | Astra 2017               | Adaptive Cruise      | Yes     | Yes            | 0mph             | 7mph           | Custom\u003csup\u003e7\u003c/sup\u003e|\n| Honda                | Accord 2018              | All                  | Yes     | Stock          | 0mph             | 3mph           | Bosch             |\n| Honda                | Civic Sedan/Coupe 2016-18| Honda Sensing        | Yes     | Yes            | 0mph             | 12mph          | Nidec             |\n| Honda                | Civic Hatchback 2017-18  | Honda Sensing        | Yes     | Stock          | 0mph             | 12mph          | Bosch             |\n| Honda                | CR-V 2015-16             | Touring              | Yes     | Yes            | 25mph\u003csup\u003e1\u003c/sup\u003e| 12mph          | Nidec             |\n| Honda                | CR-V 2017-18             | Honda Sensing        | Yes     | Stock          | 0mph             | 12mph          | Bosch             |\n| Honda                | Odyssey 2017-19          | Honda Sensing        | Yes     | Yes            | 25mph\u003csup\u003e1\u003c/sup\u003e| 0mph           | Inverted Nidec    |\n| Honda                | Pilot 2016-18            | Honda Sensing        | Yes     | Yes            | 25mph\u003csup\u003e1\u003c/sup\u003e| 12mph          | Nidec             |\n| Honda                | Pilot 2019               | All                  | Yes     | Yes            | 25mph\u003csup\u003e1\u003c/sup\u003e| 12mph          | Inverted Nidec    |\n| Honda                | Ridgeline 2017-18        | Honda Sensing        | Yes     | Yes            | 25mph\u003csup\u003e1\u003c/sup\u003e| 12mph          | Nidec             |\n| Hyundai              | Santa Fe 2019            | All                  | Yes     | Stock          | 0mph             | 0mph           | Custom\u003csup\u003e6\u003c/sup\u003e|\n| Hyundai              | Elantra 2017             | SCC + LKAS           | Yes     | Stock          | 19mph            | 34mph          | Custom\u003csup\u003e6\u003c/sup\u003e|\n| Hyundai              | Genesis 2018             | All                  | Yes     | Stock          | 19mph            | 34mph          | Custom\u003csup\u003e6\u003c/sup\u003e|\n| Kia                  | Sorento 2018             | All                  | Yes     | Stock          | 0mph             | 0mph           | Custom\u003csup\u003e6\u003c/sup\u003e|\n| Kia                  | Stinger 2018             | SCC + LKAS           | Yes     | Stock          | 0mph             | 0mph           | Custom\u003csup\u003e6\u003c/sup\u003e|\n| Lexus                | RX Hybrid 2016-18        | All                  | Yes     | Yes\u003csup\u003e2\u003c/sup\u003e| 0mph             | 0mph           | Toyota            |\n| Toyota               | Camry 2018\u003csup\u003e4\u003c/sup\u003e   | All                  | Yes     | Stock          | 0mph\u003csup\u003e5\u003c/sup\u003e | 0mph           | Toyota            |\n| Toyota               | C-HR 2017-18\u003csup\u003e4\u003c/sup\u003e | All                  | Yes     | Stock          | 0mph             | 0mph           | Toyota            |\n| Toyota               | Corolla 2017-18          | All                  | Yes     | Yes\u003csup\u003e2\u003c/sup\u003e| 20mph            | 0mph           | Toyota            |\n| Toyota               | Highlander 2017-18       | All                  | Yes     | Yes\u003csup\u003e2\u003c/sup\u003e| 0mph             | 0mph           | Toyota            |\n| Toyota               | Highlander Hybrid 2018   | All                  | Yes     | Yes\u003csup\u003e2\u003c/sup\u003e| 0mph             | 0mph           | Toyota            |\n| Toyota               | Prius 2016               | TSS-P                | Yes     | Yes\u003csup\u003e2\u003c/sup\u003e| 0mph             | 0mph           | Toyota            |\n| Toyota               | Prius 2017-18            | All                  | Yes     | Yes\u003csup\u003e2\u003c/sup\u003e| 0mph             | 0mph           | Toyota            |\n| Toyota               | Prius Prime 2017-18      | All                  | Yes     | Yes\u003csup\u003e2\u003c/sup\u003e| 0mph             | 0mph           | Toyota            |\n| Toyota               | Rav4 2016                | TSS-P                | Yes     | Yes\u003csup\u003e2\u003c/sup\u003e| 20mph            | 0mph           | Toyota            |\n| Toyota               | Rav4 2017-18             | All                  | Yes     | Yes\u003csup\u003e2\u003c/sup\u003e| 20mph            | 0mph           | Toyota            |\n| Toyota               | Rav4 Hybrid 2017-18      | All                  | Yes     | Yes\u003csup\u003e2\u003c/sup\u003e| 0mph             | 0mph           | Toyota            |\n\n\u003csup\u003e1\u003c/sup\u003e[Comma Pedal](https://community.comma.ai/wiki/index.php/Comma_Pedal) is used to provide stop-and-go capability to some of the openpilot-supported cars that don't currently support stop-and-go. Here is how to [build a Comma Pedal](https://medium.com/@jfrux/comma-pedal-building-with-macrofab-6328bea791e8). ***NOTE: The Comma Pedal is not officially supported by [comma.ai](https://comma.ai)***  \n\u003csup\u003e2\u003c/sup\u003eWhen disconnecting the Driver Support Unit (DSU), otherwise longitudinal control is stock ACC. For DSU locations, see [Toyota Wiki page](https://community.comma.ai/wiki/index.php/Toyota)  \n\u003csup\u003e3\u003c/sup\u003e[GM installation guide](https://zoneos.com/volt/).  \n\u003csup\u003e4\u003c/sup\u003eIt needs an extra 120Ohm resistor ([pic1](https://i.imgur.com/CmdKtTP.jpg), [pic2](https://i.imgur.com/s2etUo6.jpg)) on bus 3 and giraffe switches set to 01X1 (11X1 for stock LKAS), where X depends on if you have the [comma power](https://comma.ai/shop/products/power/).  \n\u003csup\u003e5\u003c/sup\u003e28mph for Camry 4CYL L, 4CYL LE and 4CYL SE which don't have Full-Speed Range Dynamic Radar Cruise Control.  \n\u003csup\u003e6\u003c/sup\u003eOpen sourced [Hyundai Giraffe](https://github.com/commaai/neo/tree/master/giraffe/hyundai) is designed ofor the 2019 Sante Fe; pinout may differ for other Hyundais.  \n\u003csup\u003e7\u003c/sup\u003eCommunity built Giraffe, find more information here, [GM Giraffe](https://zoneos.com/shop/).  \n\nCommunity Maintained Cars\n------\n\n| Make          | Model                     | Supported Package    | Lateral | Longitudinal   | No Accel Below   | No Steer Below | Giraffe           |\n| -------       | ----------------------    | -------------------- | ------- | ------------   | --------------   | -------------- | ------------------|\n| Honda         | Fit 2018                  | Honda Sensing        | Yes     | Yes            | 25mph\u003csup\u003e1\u003c/sup\u003e| 12mph          | Inverted Nidec    |\n| Tesla         | Model S 2012              | All                  | Yes     | Not yet        | Not applicable   | 0mph           | Custom\u003csup\u003e8\u003c/sup\u003e|\n| Tesla         | Model S 2013              | All                  | Yes     | Not yet        | Not applicable   | 0mph           | Custom\u003csup\u003e8\u003c/sup\u003e|\n\n[[Honda Fit Pull Request]](https://github.com/commaai/openpilot/pull/266). \u003cbr /\u003e\n[[Tesla Model S Pull Request]](https://github.com/commaai/openpilot/pull/246) \u003cbr /\u003e\n\u003csup\u003e8\u003c/sup\u003eCommunity built Giraffe, find more information here [Community Tesla Giraffe](https://github.com/jeankalud/neo/tree/tesla_giraffe/giraffe/tesla) \u003cbr /\u003e\n\nCommunity Maintained Cars are not confirmed by comma.ai to meet our [safety model](https://github.com/commaai/openpilot/blob/devel/SAFETY.md). Be extra cautious using them.\n\nIn Progress Cars\n------\n- All TSS-P Toyota with Steering Assist.\n  - 'Full Speed Range Dynamic Radar Cruise Control' is required to enable stop-and-go. Only the Prius, Camry and C-HR have this option.\n  - Even though the Tundra, Sequoia and the Land Cruiser have TSS-P, they don't have Steering Assist and are not supported.\n  - Only remaining Toyota cars with no port yet are the Avalon and the Sienna.\n- All LSS-P Lexus with Steering Assist or Lane Keep Assist.\n  - 'All-Speed Range Dynamic Radar Cruise Control' is required to enable stop-and-go. Only the GS, GSH, F, RX, RXH, LX, NX, NXH, LC, LCH, LS, LSH have this option.\n  - Even though the LX have TSS-P, it does not have Steering Assist and is not supported.\n- All Hyundai with SmartSense.\n- All Kia with SCC and LKAS.\n\nHow can I add support for my car?\n------\n\nIf your car has adaptive cruise control and lane keep assist, you are in luck. Using a [panda](https://comma.ai/shop/products/panda-obd-ii-dongle/) and [cabana](https://community.comma.ai/cabana/), you can understand how to make your car drive by wire.\n\nWe've written guides for [Brand](https://medium.com/@comma_ai/how-to-write-a-car-port-for-openpilot-7ce0785eda84) and [Model](https://medium.com/@comma_ai/openpilot-port-guide-for-toyota-models-e5467f4b5fe6) ports. These guides might help you after you have the basics figured out.\n\n- BMW, Audi, Volvo, and Mercedes all use [FlexRay](https://en.wikipedia.org/wiki/FlexRay) and are unlikely to be supported any time soon.\n- We put time into a Ford port, but the steering has a 10 second cutout limitation that makes it unusable.\n- The 2016-2017 Honda Accord uses a custom signaling protocol for steering that's unlikely to ever be upstreamed.\n\nDirectory structure\n------\n    .\n    ├── apk                 # The apk files used for the UI\n    ├── cereal              # The messaging spec used for all logs on EON\n    ├── common              # Library like functionality we've developed here\n    ├── installer/updater   # Manages auto-updates of openpilot\n    ├── opendbc             # Files showing how to interpret data from cars\n    ├── panda               # Code used to communicate on CAN and LIN\n    ├── phonelibs           # Libraries used on EON\n    ├── pyextra             # Libraries used on EON\n    └── selfdrive           # Code needed to drive the car\n        ├── assets          # Fonts and images for UI\n        ├── boardd          # Daemon to talk to the board\n        ├── can             # Helpers for parsing CAN messages\n        ├── car             # Car specific code to read states and control actuators\n        ├── common          # Shared C/C++ code for the daemons\n        ├── controls        # Perception, planning and controls\n        ├── debug           # Tools to help you debug and do car ports\n        ├── locationd       # Soon to be home of precise location\n        ├── logcatd         # Android logcat as a service\n        ├── loggerd         # Logger and uploader of car data\n        ├── proclogd        # Logs information from proc\n        ├── sensord         # IMU / GPS interface code\n        ├── test            # Car simulator running code through virtual maneuvers\n        ├── ui              # The UI\n        └── visiond         # Embedded vision pipeline\n\nTo understand how the services interact, see `selfdrive/service_list.yaml`\n\nUser Data / chffr Account / Crash Reporting\n------\n\nBy default, openpilot creates an account and includes a client for chffr, our dashcam app. We use your data to train better models and improve openpilot for everyone.\n\nIt's open source software, so you are free to disable it if you wish.\n\nIt logs the road facing camera, CAN, GPS, IMU, magnetometer, thermal sensors, crashes, and operating system logs.\nThe user facing camera is only logged if you explicitly opt-in in settings.\nIt does not log the microphone.\n\nBy using it, you agree to [our privacy policy](https://community.comma.ai/privacy.html). You understand that use of this software or its related services will generate certain types of user data, which may be logged and stored at the sole discretion of comma.ai. By accepting this agreement, you grant an irrevocable, perpetual, worldwide right to comma.ai for the use of this data.\n\nTesting on PC\n------\n\nThere is rudimentary infrastructure to run a basic simulation and generate a report of openpilot's behavior in different scenarios.\n\n```bash\n# Requires working docker\n./run_docker_tests.sh\n```\n\nThe resulting plots are displayed in `selfdrive/test/tests/plant/out/longitudinal/index.html`\n\nMore extensive testing infrastructure and simulation environments are coming soon.\n\n\nContributing\n------\n\nWe welcome both pull requests and issues on\n[github](http://github.com/commaai/openpilot). Bug fixes and new car ports encouraged.\n\nWant to get paid to work on openpilot? [comma.ai is hiring](https://comma.ai/jobs/)\n\nLicensing\n------\n\nopenpilot is released under the MIT license. Some parts of the software are released under other licenses as specified.\n\nAny user of this software shall indemnify and hold harmless Comma.ai, Inc. and its directors, officers, employees, agents, stockholders, affiliates, subcontractors and customers from and against all allegations, claims, actions, suits, demands, damages, liabilities, obligations, losses, settlements, judgments, costs and expenses (including without limitation attorneys’ fees and costs) which arise out of, relate to or result from any use of this software by user.\n\n**THIS IS ALPHA QUALITY SOFTWARE FOR RESEARCH PURPOSES ONLY. THIS IS NOT A PRODUCT.\nYOU ARE RESPONSIBLE FOR COMPLYING WITH LOCAL LAWS AND REGULATIONS.\nNO WARRANTY EXPRESSED OR IMPLIED.**\n\n---\n\n\u003cimg src=\"https://d1qb2nb5cznatu.cloudfront.net/startups/i/1061157-bc7e9bf3b246ece7322e6ffe653f6af8-medium_jpg.jpg?buster=1458363130\" width=\"75\"\u003e\u003c/img\u003e \u003cimg src=\"https://cdn-images-1.medium.com/max/1600/1*C87EjxGeMPrkTuVRVWVg4w.png\" width=\"225\"\u003e\u003c/img\u003e\n"
  },
  {
    "repo": "facebook/zstd",
    "content": "\u003cp align=\"center\"\u003e\u003cimg src=\"https://raw.githubusercontent.com/facebook/zstd/dev/doc/images/zstd_logo86.png\" alt=\"Zstandard\"\u003e\u003c/p\u003e\n\n__Zstandard__, or `zstd` as short version, is a fast lossless compression algorithm,\ntargeting real-time compression scenarios at zlib-level and better compression ratios.\nIt's backed by a very fast entropy stage, provided by [Huff0 and FSE library](https://github.com/Cyan4973/FiniteStateEntropy).\n\nThe project is provided as an open-source dual [BSD](LICENSE) and [GPLv2](COPYING) licensed **C** library,\nand a command line utility producing and decoding `.zst`, `.gz`, `.xz` and `.lz4` files.\nShould your project require another programming language,\na list of known ports and bindings is provided on [Zstandard homepage](http://www.zstd.net/#other-languages).\n\nDevelopment branch status : [![Build Status][travisDevBadge]][travisLink]   [![Build status][AppveyorDevBadge]][AppveyorLink]   [![Build status][CircleDevBadge]][CircleLink]\n\n[travisDevBadge]: https://travis-ci.org/facebook/zstd.svg?branch=dev \"Continuous Integration test suite\"\n[travisLink]: https://travis-ci.org/facebook/zstd\n[AppveyorDevBadge]: https://ci.appveyor.com/api/projects/status/xt38wbdxjk5mrbem/branch/dev?svg=true \"Windows test suite\"\n[AppveyorLink]: https://ci.appveyor.com/project/YannCollet/zstd-p0yf0\n[CircleDevBadge]: https://circleci.com/gh/facebook/zstd/tree/dev.svg?style=shield \"Short test suite\"\n[CircleLink]: https://circleci.com/gh/facebook/zstd\n\n### Benchmarks\n\nFor reference, several fast compression algorithms were tested and compared\non a server running Linux Debian (`Linux version 4.14.0-3-amd64`),\nwith a Core i7-6700K CPU @ 4.0GHz,\nusing [lzbench], an open-source in-memory benchmark by @inikep\ncompiled with [gcc] 7.3.0,\non the [Silesia compression corpus].\n\n[lzbench]: https://github.com/inikep/lzbench\n[Silesia compression corpus]: http://sun.aei.polsl.pl/~sdeor/index.php?page=silesia\n[gcc]: https://gcc.gnu.org/\n\n| Compressor name         | Ratio | Compression| Decompress.|\n| ---------------         | ------| -----------| ---------- |\n| **zstd 1.3.4 -1**       | 2.877 |   470 MB/s |  1380 MB/s |\n| zlib 1.2.11 -1          | 2.743 |   110 MB/s |   400 MB/s |\n| brotli 1.0.2 -0         | 2.701 |   410 MB/s |   430 MB/s |\n| quicklz 1.5.0 -1        | 2.238 |   550 MB/s |   710 MB/s |\n| lzo1x 2.09 -1           | 2.108 |   650 MB/s |   830 MB/s |\n| lz4 1.8.1               | 2.101 |   750 MB/s |  3700 MB/s |\n| snappy 1.1.4            | 2.091 |   530 MB/s |  1800 MB/s |\n| lzf 3.6 -1              | 2.077 |   400 MB/s |   860 MB/s |\n\n[zlib]:http://www.zlib.net/\n[LZ4]: http://www.lz4.org/\n\nZstd can also offer stronger compression ratios at the cost of compression speed.\nSpeed vs Compression trade-off is configurable by small increments.\nDecompression speed is preserved and remains roughly the same at all settings,\na property shared by most LZ compression algorithms, such as [zlib] or lzma.\n\nThe following tests were run\non a server running Linux Debian (`Linux version 4.14.0-3-amd64`)\nwith a Core i7-6700K CPU @ 4.0GHz,\nusing [lzbench], an open-source in-memory benchmark by @inikep\ncompiled with [gcc] 7.3.0,\non the [Silesia compression corpus].\n\nCompression Speed vs Ratio | Decompression Speed\n---------------------------|--------------------\n![Compression Speed vs Ratio](doc/images/CSpeed2.png \"Compression Speed vs Ratio\") | ![Decompression Speed](doc/images/DSpeed3.png \"Decompression Speed\")\n\nA few other algorithms can produce higher compression ratios at slower speeds, falling outside of the graph.\nFor a larger picture including slow modes, [click on this link](doc/images/DCspeed5.png).\n\n\n### The case for Small Data compression\n\nPrevious charts provide results applicable to typical file and stream scenarios (several MB). Small data comes with different perspectives.\n\nThe smaller the amount of data to compress, the more difficult it is to compress. This problem is common to all compression algorithms, and reason is, compression algorithms learn from past data how to compress future data. But at the beginning of a new data set, there is no \"past\" to build upon.\n\nTo solve this situation, Zstd offers a __training mode__, which can be used to tune the algorithm for a selected type of data.\nTraining Zstandard is achieved by providing it with a few samples (one file per sample). The result of this training is stored in a file called \"dictionary\", which must be loaded before compression and decompression.\nUsing this dictionary, the compression ratio achievable on small data improves dramatically.\n\nThe following example uses the `github-users` [sample set](https://github.com/facebook/zstd/releases/tag/v1.1.3), created from [github public API](https://developer.github.com/v3/users/#get-all-users).\nIt consists of roughly 10K records weighing about 1KB each.\n\nCompression Ratio | Compression Speed | Decompression Speed\n------------------|-------------------|--------------------\n![Compression Ratio](doc/images/dict-cr.png \"Compression Ratio\") | ![Compression Speed](doc/images/dict-cs.png \"Compression Speed\") | ![Decompression Speed](doc/images/dict-ds.png \"Decompression Speed\")\n\n\nThese compression gains are achieved while simultaneously providing _faster_ compression and decompression speeds.\n\nTraining works if there is some correlation in a family of small data samples. The more data-specific a dictionary is, the more efficient it is (there is no _universal dictionary_).\nHence, deploying one dictionary per type of data will provide the greatest benefits.\nDictionary gains are mostly effective in the first few KB. Then, the compression algorithm will gradually use previously decoded content to better compress the rest of the file.\n\n#### Dictionary compression How To:\n\n1) Create the dictionary\n\n`zstd --train FullPathToTrainingSet/* -o dictionaryName`\n\n2) Compress with dictionary\n\n`zstd -D dictionaryName FILE`\n\n3) Decompress with dictionary\n\n`zstd -D dictionaryName --decompress FILE.zst`\n\n\n### Build instructions\n\n#### Makefile\n\nIf your system is compatible with standard `make` (or `gmake`),\ninvoking `make` in root directory will generate `zstd` cli in root directory.\n\nOther available options include:\n- `make install` : create and install zstd cli, library and man pages\n- `make check` : create and run `zstd`, tests its behavior on local platform\n\n#### cmake\n\nA `cmake` project generator is provided within `build/cmake`.\nIt can generate Makefiles or other build scripts\nto create `zstd` binary, and `libzstd` dynamic and static libraries.\n\nBy default, `CMAKE_BUILD_TYPE` is set to `Release`.\n\n#### Meson\n\nA Meson project is provided within `build/meson`.\n\n#### Visual Studio (Windows)\n\nGoing into `build` directory, you will find additional possibilities:\n- Projects for Visual Studio 2005, 2008 and 2010.\n  + VS2010 project is compatible with VS2012, VS2013, VS2015 and VS2017.\n- Automated build scripts for Visual compiler by [@KrzysFR](https://github.com/KrzysFR), in `build/VS_scripts`,\n  which will build `zstd` cli and `libzstd` library without any need to open Visual Studio solution.\n\n#### Buck\n\nYou can build the zstd binary via buck by executing: `buck build programs:zstd` from the root of the repo.\nThe output binary will be in `buck-out/gen/programs/`.\n\n### Status\n\nZstandard is currently deployed within Facebook. It is used continuously to compress large amounts of data in multiple formats and use cases.\nZstandard is considered safe for production environments.\n\n### License\n\nZstandard is dual-licensed under [BSD](LICENSE) and [GPLv2](COPYING).\n\n### Contributing\n\nThe \"dev\" branch is the one where all contributions are merged before reaching \"master\".\nIf you plan to propose a patch, please commit into the \"dev\" branch, or its own feature branch.\nDirect commit to \"master\" are not permitted.\nFor more information, please read [CONTRIBUTING](CONTRIBUTING.md).\n"
  },
  {
    "repo": "openssl/openssl",
    "content": "\n OpenSSL 3.0.0-dev\n\n Copyright (c) 1998-2018 The OpenSSL Project\n Copyright (c) 1995-1998 Eric A. Young, Tim J. Hudson\n All rights reserved.\n\n DESCRIPTION\n -----------\n\n The OpenSSL Project is a collaborative effort to develop a robust,\n commercial-grade, fully featured, and Open Source toolkit implementing the\n Transport Layer Security (TLS) protocols (including SSLv3) as well as a\n full-strength general purpose cryptographic library.\n\n OpenSSL is descended from the SSLeay library developed by Eric A. Young\n and Tim J. Hudson.\n\n The OpenSSL toolkit is licensed under the Apache License 2.0, which means\n that you are free to get and use it for commercial and non-commercial\n purposes as long as you fulfill its conditions.\n\n OVERVIEW\n --------\n\n The OpenSSL toolkit includes:\n\n libssl (with platform specific naming):\n     Provides the client and server-side implementations for SSLv3 and TLS.\n\n libcrypto (with platform specific naming):\n     Provides general cryptographic and X.509 support needed by SSL/TLS but\n     not logically part of it.\n\n openssl:\n     A command line tool that can be used for:\n        Creation of key parameters\n        Creation of X.509 certificates, CSRs and CRLs\n        Calculation of message digests\n        Encryption and decryption\n        SSL/TLS client and server tests\n        Handling of S/MIME signed or encrypted mail\n        And more...\n\n INSTALLATION\n ------------\n\n See the appropriate file:\n        INSTALL         Linux, Unix, Windows, OpenVMS, ...\n        NOTES.*         INSTALL addendums for different platforms\n\n SUPPORT\n -------\n\n See the OpenSSL website www.openssl.org for details on how to obtain\n commercial technical support. Free community support is available through the\n openssl-users email list (see\n https://www.openssl.org/community/mailinglists.html for further details).\n\n If you have any problems with OpenSSL then please take the following steps\n first:\n\n    - Download the latest version from the repository\n      to see if the problem has already been addressed\n    - Configure with no-asm\n    - Remove compiler optimization flags\n\n If you wish to report a bug then please include the following information\n and create an issue on GitHub:\n\n    - OpenSSL version: output of 'openssl version -a'\n    - Configuration data: output of 'perl configdata.pm --dump'\n    - OS Name, Version, Hardware platform\n    - Compiler Details (name, version)\n    - Application Details (name, version)\n    - Problem Description (steps that will reproduce the problem, if known)\n    - Stack Traceback (if the application dumps core)\n\n Just because something doesn't work the way you expect does not mean it\n is necessarily a bug in OpenSSL. Use the openssl-users email list for this type\n of query.\n\n HOW TO CONTRIBUTE TO OpenSSL\n ----------------------------\n\n See CONTRIBUTING\n\n LEGALITIES\n ----------\n\n A number of nations restrict the use or export of cryptography. If you\n are potentially subject to such restrictions you should seek competent\n professional legal advice before attempting to develop or distribute\n cryptographic code.\n"
  },
  {
    "repo": "radare/radare2",
    "content": "```\n    ____  ___  ___  ___ ____  ___    ____\n   |  _ \\/   \\|   \\/   \\  _ \\/ _ \\  (__  \\\n   |    (  -  | |  ) -  |   (   _/  /  __/\n   |__\\__|_|__|___/__|__|_\\__|___|  |____|\n\n                https://www.radare.org\n\n                                  --pancake\n```\n\n\n\n| | |\n|----------|---------------------------------------------------------------------|\n| **Jenkins**  \t| [![Build Status](http://ci.rada.re/buildStatus/icon?job=radare2)](http://ci.rada.re/job/radare2)|\n| **TravisCI** \t| [![Build Status](https://travis-ci.org/radare/radare2.svg?branch=master)](https://travis-ci.org/radare/radare2)|\n| **AppVeyor**  | [![Build Status](https://ci.appveyor.com/api/projects/status/v9bxvsb1p6c3cmf9/branch/master?svg=true)](https://ci.appveyor.com/project/radare/radare2-shvdd)|\n| **Coverity** \t| [![Build Status](https://scan.coverity.com/projects/416/badge.svg)](https://scan.coverity.com/projects/416) |\n| **Infrastructure** |  [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/741/badge)](https://bestpractices.coreinfrastructure.org/projects/741) |\n| **Codecov** | [![codecov](https://codecov.io/gh/radare/radare2/branch/master/graph/badge.svg)](https://codecov.io/gh/radare/radare2)\n\u003ca href=\"https://repology.org/metapackage/radare2\"\u003e\n\u003cimg src=\"https://repology.org/badge/vertical-allrepos/radare2.svg\" alt=\"Packaging status\" align=\"right\" width=\"150px\"\u003e\n\u003c/a\u003e\n\n# Introduction\n\nr2 is a rewrite from scratch of radare in order to provide\na set of libraries and tools to work with binary files.\n\nRadare project started as a forensics tool, a scriptable\ncommand-line hexadecimal editor able to open disk files,\nbut later added support for analyzing binaries, disassembling\ncode, debugging programs, attaching to remote gdb servers...\n\nradare2 is portable.\n\n## Architectures\n\ni386, x86-64, ARM, MIPS, PowerPC, SPARC, RISC-V, SH, m68k, AVR, XAP,\nSystem Z, XCore, CR16, HPPA, ARC, Blackfin, Z80, H8/300, V810, V850,\nCRIS, XAP, PIC, LM32, 8051, 6502, i4004, i8080, Propeller, Tricore, Chip8\nLH5801, T8200, GameBoy, SNES, MSP430, Xtensa, NIOS II,\nDalvik, WebAssembly, MSIL, EBC, TMS320 (c54x, c55x, c55+, c66),\nHexagon, Brainfuck, Malbolge, DCPU16.\n\n## File Formats\n\nELF, Mach-O, Fatmach-O, PE, PE+, MZ, COFF, OMF, TE, XBE, BIOS/UEFI,\nDyldcache, DEX, ART, CGC, Java class, Android boot image, Plan9 executable,\nZIMG, MBN/SBL bootloader, ELF coredump, MDMP (Windows minidump),\nWASM (WebAssembly binary), Commodore VICE emulator, \nGame Boy (Advance), Nintendo DS ROMs and Nintendo 3DS FIRMs, various filesystems.\n\n## Operating Systems\n\nWindows (since XP), GNU/Linux, OS X, [Net|Free|Open]BSD,\nAndroid, iOS, OSX, QNX, Solaris, Haiku, FirefoxOS.\n\n## Bindings\n\nVala/Genie, Python (2, 3), NodeJS, Lua, Go, Perl,\nGuile, PHP, Newlisp, Ruby, Java, OCaml...\n\n# Dependencies\n\nradare2 can be built without any special dependency, just\nget a working toolchain (gcc, clang, tcc...) and use make.\n\nOptionally you can use libewf for loading EnCase disk images.\n\nTo build the bindings you need latest valabind, g++ and swig2.\n\n# Install\n\nThe easiest way to install radare2 from git is by running\nthe following command:\n\n\t$ sys/install.sh\n\nIf you want to install radare2 in the home directory without\nusing root privileges and sudo, simply run:\n\n\t$ sys/user.sh\n\n# Building with meson + ninja\n\nIf you don't already have meson and ninja, you can install them\nwith your distribution package manager or with r2pm:\n\n\t$ r2pm -i meson\n\nIf you already have them installed, you can run this line to\ncompile radare2:\n\n\t$ python ./sys/meson.py --prefix=/usr --shared --install\n\nThis method is mostly useful on Windows because the initial building\nwith Makefile is not suitable. If you are lost in any way, just type:\n\n\t$ python ./sys/meson.py --help\n\n# Update\n\nTo update Radare2 system-wide, you don't need to uninstall or pull.\nJust re-run:\n\n\t$ sys/install.sh\n\nIf you installed Radare2 in the home directory,\njust re-run:\n\n\t$ sys/user.sh\n\n# Uninstall\n\nIn case of a polluted filesystem, you can uninstall the current\nversion or remove all previous installations:\n\n\t$ make uninstall\n\t$ make purge\n\t\nTo remove all stuff including libraries, use\n\n\t$ make system-purge\n\n# Package manager\n\nRadare2 has its own package manager - r2pm. Its packages\nrepository is on [GitHub too](https://github.com/radare/radare2-pm).\nTo start to using it for the first time, you need to initialize packages:\n\n\t$ r2pm init\n\nRefresh the packages database before installing any package:\n\n\t$ r2pm update\n\nTo install a package, use the following command:\n\n\t$ r2pm install [package name]\n\n# Bindings\n\nAll language bindings are under the r2-bindings directory.\nYou will need to install swig and valabind in order to\nbuild the bindings for Python, Lua, etc..\n\nAPIs are defined in vapi files which are then translated\nto swig interfaces, nodejs-ffi or other and then compiled.\n\nThe easiest way to install the python bindings is to run:\n\n\t$ r2pm install lang-python2 #lang-python3 for python3 bindings\n\t$ r2pm install r2api-python\n\t$ r2pm install r2pipe-py\n\nIn addition there are `r2pipe` bindings, which is an API\ninterface to interact with the prompt, passing commands\nand receivent the output as a string, many commands support\nJSON output, so its integrated easily with many languages\nin order to deserialize it into native objects.\n\n\t$ npm install r2pipe   # NodeJS\n\t$ gem install r2pipe   # Ruby\n\t$ pip install r2pipe   # Python\n\t$ opam install radare2 # OCaml\n\nAnd also for Go, Rust, Swift, D, .NET, Java, NewLisp, Perl, Haskell,\nVala, OCaml, and many more to come!\n\n# Regression Testsuite\n\nRunning `make tests` will fetch the radare2-regressions\nrepository and run all the tests in order to verify that no\nchanges break any functionality.\n\nWe run those tests on every commit, and they are also\nexecuted with ASAN and valgrind on different platforms\nto catch other unwanted 'features'.\n\n# Documentation\n\nThere is no formal documentation of r2 yet. Not all commands\nare compatible with radare1, so the best way to learn how to\ndo stuff in r2 is by reading the examples from the web and\nappending '?' to every command you are interested in.\n\nCommands are small mnemonics of few characters and there is\nsome extra syntax sugar that makes the shell much more pleasant\nfor scripting and interacting with the APIs.\n\nYou could also checkout the [radare2 book](https://radare.gitbooks.io/radare2book/content/).\n\n# Coding Style\n\nLook at [CONTRIBUTING.md](https://github.com/radare/radare2/blob/master/CONTRIBUTING.md).\n\n# Webserver\n\nradare2 comes with an embedded webserver which serves a pure\nhtml/js interface that sends ajax queries to the core and\naims to implement an usable UI for phones, tablets and desktops.\n\n\t$ r2 -c=H /bin/ls\n\nTo use the webserver on Windows, you require a cmd instance\nwith administrator rights. To start the webserver, use the following command\nin the project root.\n\n\t\u003e radare2.exe -c=H rax2.exe\n\n# Pointers\n\nWebsite: https://www.radare.org/\n\nIRC: irc.freenode.net #radare\n\nTelegram: https://t.me/radare\n\nMatrix: @radare2:matrix.org\n\nTwitter: [@radareorg](https://twitter.com/radareorg)\n"
  },
  {
    "repo": "mpv-player/mpv",
    "content": "![mpv logo](https://raw.githubusercontent.com/mpv-player/mpv.io/master/source/images/mpv-logo-128.png)\n\n# mpv\n\n\n* [External links](#external-links)\n* [Overview](#overview)\n* [System requirements](#system-requirements)\n* [Downloads](#downloads)\n* [Changelog](#changelog)\n* [Compilation](#compilation)\n* [FFmpeg vs. Libav](#ffmpeg-vs-libav)\n* [FFmpeg ABI compatibility](#ffmpeg-abi-compatibility)\n* [Release cycle](#release-cycle)\n* [Bug reports](#bug-reports)\n* [Contributing](#contributing)\n* [Relation to MPlayer and mplayer2](#relation-to-mplayer-and-mplayer2)\n* [License](#license)\n* [Contact](#contact)\n\n\n## External links\n\n\n* [Wiki](https://github.com/mpv-player/mpv/wiki)\n* [FAQ](https://github.com/mpv-player/mpv/wiki/FAQ)\n* [Manual](http://mpv.io/manual/master/)\n\n\n## Overview\n\n\n**mpv** is a media player based on MPlayer and mplayer2. It supports a wide\nvariety of video file formats, audio and video codecs, and subtitle types.\n\nReleases can be found on the [release list][releases].\n\n## System requirements\n\n- A not too ancient Linux, Windows 7 or later, or OSX 10.8 or later.\n- A somewhat capable CPU. Hardware decoding might help if the CPU is too slow to\n  decode video in realtime, but must be explicitly enabled with the `--hwdec`\n  option.\n- A not too crappy GPU. mpv is not intended to be used with bad GPUs. There are\n  many caveats with drivers or system compositors causing tearing, stutter,\n  etc. On Windows, you might want to make sure the graphics drivers are\n  current. In some cases, ancient fallback video output methods can help\n  (such as `--vo=xv` on Linux), but this use is not recommended or supported.\n\n\n## Downloads\n\n\nFor semi-official builds and third-party packages please see\n[mpv.io/installation](http://mpv.io/installation/).\n\n## Changelog\n\n\nThere is no complete changelog; however, changes to the player core interface\nare listed in the [interface changelog][interface-changes].\n\nChanges to the C API are documented in the [client API changelog][api-changes].\n\nThe [release list][releases] has a summary of most of the important changes\non every release.\n\nChanges to the default key bindings are indicated in\n[restore-old-bindings.conf][restore-old-bindings].\n\n## Compilation\n\n\nCompiling with full features requires development files for several\nexternal libraries. Below is a list of some important requirements.\n\nThe mpv build system uses [waf](https://waf.io/), but we don't store it in the\nrepository. The `./bootstrap.py` script will download the latest version\nof waf that was tested with the build system.\n\nFor a list of the available build options use `./waf configure --help`. If\nyou think you have support for some feature installed but configure fails to\ndetect it, the file `build/config.log` may contain information about the\nreasons for the failure.\n\nNOTE: To avoid cluttering the output with unreadable spam, `--help` only shows\none of the two switches for each option. If the option is autodetected by\ndefault, the `--disable-***` switch is printed; if the option is disabled by\ndefault, the `--enable-***` switch is printed. Either way, you can use\n`--enable-***` or `--disable-**` regardless of what is printed by `--help`.\n\nTo build the software you can use `./waf build`: the result of the compilation\nwill be located in `build/mpv`. You can use `./waf install` to install mpv\nto the *prefix* after it is compiled.\n\nExample:\n\n    ./bootstrap.py\n    ./waf configure\n    ./waf\n    ./waf install\n\nEssential dependencies (incomplete list):\n\n- gcc or clang\n- X development headers (xlib, xrandr, xext, xscrnsaver, xinerama, libvdpau,\n  libGL, GLX, EGL, xv, ...)\n- Audio output development headers (libasound/ALSA, pulseaudio)\n- FFmpeg libraries (libavutil libavcodec libavformat libswscale libavfilter\n  and either libswresample or libavresample)\n- zlib\n- iconv (normally provided by the system libc)\n- libass (OSD, OSC, text subtitles)\n- Lua (optional, required for the OSC pseudo-GUI and youtube-dl integration)\n- libjpeg (optional, used for screenshots only)\n- uchardet (optional, for subtitle charset detection)\n- vdpau and vaapi libraries for hardware decoding on Linux (optional)\n\nLibass dependencies:\n\n- gcc or clang, yasm on x86 and x86_64\n- fribidi, freetype, fontconfig development headers (for libass)\n- harfbuzz (optional, required for correct rendering of combining characters,\n  particularly for correct rendering of non-English text on OSX, and\n  Arabic/Indic scripts on any platform)\n\nFFmpeg dependencies:\n\n- gcc or clang, yasm on x86 and x86_64\n- OpenSSL or GnuTLS (have to be explicitly enabled when compiling FFmpeg)\n- libx264/libmp3lame/libfdk-aac if you want to use encoding (have to be\n  explicitly enabled when compiling FFmpeg)\n- For native DASH playback, FFmpeg needs to be built with --enable-libxml2\n  (although there are security implications).\n- For good nvidia support on Linux, make sure nv-codec-headers is installed\n  and can be found by configure.\n- Libav support is broken. (See section below.)\n\nMost of the above libraries are available in suitable versions on normal\nLinux distributions. For ease of compiling the latest git master of everything,\nyou may wish to use the separately available build wrapper ([mpv-build][mpv-build])\nwhich first compiles FFmpeg libraries and libass, and then compiles the player\nstatically linked against those.\n\nIf you want to build a Windows binary, you either have to use MSYS2 and MinGW,\nor cross-compile from Linux with MinGW. See\n[Windows compilation][windows_compilation].\n\n\n## FFmpeg vs. Libav\n\n\nGenerally, mpv should work with the latest release as well as the git version\nof FFmpeg. Libav support is currently broken, because they did not add certain\nFFmpeg API changes which mpv relies on.\n\n\n## FFmpeg ABI compatibility\n\nmpv does not support linking against FFmpeg versions it was not built with, even\nif the linked version is supposedly ABI-compatible with the version it was\ncompiled against. Expect malfunctions, crashes, and security issues if you\ndo it anyway.\n\nThe reason for not supporting this is because it creates far too much complexity\nwith little to no benefit, coupled with absurd and unusable FFmpeg API\nartifacts.\n\nNewer mpv versions will refuse to start if runtime and compile time FFmpeg\nlibrary versions mismatch.\n\n## Release cycle\n\nEvery other month, an arbitrary git snapshot is made, and is assigned\na 0.X.0 version number. No further maintenance is done.\n\nThe goal of releases is to make Linux distributions happy. Linux distributions\nare also expected to apply their own patches in case of bugs and security\nissues.\n\nReleases other than the latest release are unsupported and unmaintained.\n\nSee the [release policy document][release-policy] for more information.\n\n## Bug reports\n\n\nPlease use the [issue tracker][issue-tracker] provided by GitHub to send us bug\nreports or feature requests. Follow the template's instructions or the issue\nwill likely be ignored or closed as invalid.\n\nUsing the bug tracker as place for simple questions is fine but IRC is\nrecommended (see [Contact](#Contact) below).\n\n## Contributing\n\n\nPlease read [contribute.md][contribute.md].\n\nFor small changes you can just send us pull requests through GitHub. For bigger\nchanges come and talk to us on IRC before you start working on them. It will\nmake code review easier for both parties later on.\n\nYou can check [the wiki](https://github.com/mpv-player/mpv/wiki/Stuff-to-do)\nor the [issue tracker](https://github.com/mpv-player/mpv/issues?q=is%3Aopen+is%3Aissue+label%3A%22feature+request%22)\nfor ideas on what you could contribute with.\n\n## Relation to MPlayer and mplayer2\n\nmpv is a fork of MPlayer. Much has changed, and in general, mpv should be\nconsidered a completely new program, rather than a MPlayer drop-in replacement.\n\nFor details see [FAQ entry](https://github.com/mpv-player/mpv/wiki/FAQ#How_is_mpv_related_to_MPlayer).\n\nIf you are wondering what's different from mplayer2 and MPlayer, an incomplete\nand largely unmaintained list of changes is located [here][mplayer-changes].\n\n## License\n\nGPLv2 \"or later\" by default, LGPLv2.1 \"or later\" with `--enable-lgpl`.\nSee [details.](https://github.com/mpv-player/mpv/blob/master/Copyright)\n\n\n## Contact\n\n\nMost activity happens on the IRC channel and the github issue tracker.\n\n- **GitHub issue tracker**: [issue tracker][issue-tracker] (report bugs here)\n- **User IRC Channel**: `#mpv` on `irc.freenode.net`\n- **Developer IRC Channel**: `#mpv-devel` on `irc.freenode.net`\n\nTo contact the `mpv` team in private write to `mpv-team@googlegroups.com`. Use\nonly if discretion is required.\n\n[releases]: https://github.com/mpv-player/mpv/releases\n[mpv-build]: https://github.com/mpv-player/mpv-build\n[homebrew-mpv]: https://github.com/mpv-player/homebrew-mpv\n[issue-tracker]:  https://github.com/mpv-player/mpv/issues\n[ffmpeg_vs_libav]: https://github.com/mpv-player/mpv/wiki/FFmpeg-versus-Libav\n[release-policy]: https://github.com/mpv-player/mpv/blob/master/DOCS/release-policy.md\n[windows_compilation]: https://github.com/mpv-player/mpv/blob/master/DOCS/compile-windows.md\n[mplayer-changes]: https://github.com/mpv-player/mpv/blob/master/DOCS/mplayer-changes.rst\n[interface-changes]: https://github.com/mpv-player/mpv/blob/master/DOCS/interface-changes.rst\n[api-changes]: https://github.com/mpv-player/mpv/blob/master/DOCS/client-api-changes.rst\n[restore-old-bindings]: https://github.com/mpv-player/mpv/blob/master/etc/restore-old-bindings.conf\n[contribute.md]: https://github.com/mpv-player/mpv/blob/master/DOCS/contribute.md\n"
  }
]