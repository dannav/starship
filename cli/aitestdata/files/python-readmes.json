[
  {
    "repo": "vinta/awesome-python",
    "content": "# Awesome Python [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n\nA curated list of awesome Python frameworks, libraries, software and resources.\n\nInspired by [awesome-php](https://github.com/ziadoz/awesome-php).\n\n- [Awesome Python](#awesome-python)\n    - [Admin Panels](#admin-panels)\n    - [Algorithms and Design Patterns](#algorithms-and-design-patterns)\n    - [Anti-spam](#anti-spam)\n    - [Asset Management](#asset-management)\n    - [Audio](#audio)\n    - [Authentication](#authentication)\n    - [Build Tools](#build-tools)\n    - [Built-in Classes Enhancement](#built-in-classes-enhancement)\n    - [Caching](#caching)\n    - [ChatOps Tools](#chatops-tools)\n    - [Cluster Computing](#cluster-computing)\n    - [CMS](#cms)\n    - [Code Analysis](#code-analysis)\n    - [Command-line Tools](#command-line-tools)\n    - [Compatibility](#compatibility)\n    - [Computer Vision](#computer-vision)\n    - [Concurrency and Parallelism](#concurrency-and-parallelism)\n    - [Configuration](#configuration)\n    - [Cryptography](#cryptography)\n    - [Data Analysis](#data-analysis)\n    - [Data Validation](#data-validation)\n    - [Data Visualization](#data-visualization)\n    - [Database Drivers](#database-drivers)\n    - [Database](#database)\n    - [Date and Time](#date-and-time)\n    - [Debugging Tools](#debugging-tools)\n    - [Deep Learning](#deep-learning)\n    - [DevOps Tools](#devops-tools)\n    - [Distribution](#distribution)\n    - [Documentation](#documentation)\n    - [Downloader](#downloader)\n    - [E-commerce](#e-commerce)\n    - [Editor Plugins and IDEs](#editor-plugins-and-ides)\n    - [Email](#email)\n    - [Environment Management](#environment-management)\n    - [Files](#files)\n    - [Foreign Function Interface](#foreign-function-interface)\n    - [Forms](#forms)\n    - [Functional Programming](#functional-programming)\n    - [Game Development](#game-development)\n    - [Geolocation](#geolocation)\n    - [GUI](#gui)\n    - [Hardware](#hardware)\n    - [HTML Manipulation](#html-manipulation)\n    - [HTTP](#http)\n    - [Image Processing](#image-processing)\n    - [Implementations](#implementations)\n    - [Interactive Interpreter](#interactive-interpreter)\n    - [Internationalization](#internationalization)\n    - [Job Scheduler](#job-scheduler)\n    - [Logging](#logging)\n    - [Machine Learning](#machine-learning)\n    - [Miscellaneous](#miscellaneous)\n    - [Natural Language Processing](#natural-language-processing)\n    - [Network Virtualization](#network-virtualization)\n    - [Networking](#networking)\n    - [News Feed](#news-feed)\n    - [ORM](#orm)\n    - [Package Management](#package-management)\n    - [Package Repositories](#package-repositories)\n    - [Permissions](#permissions)\n    - [Processes](#processes)\n    - [Queue](#queue)\n    - [Recommender Systems](#recommender-systems)\n    - [RESTful API](#restful-api)\n    - [Robotics](#robotics)\n    - [RPC Servers](#rpc-servers)\n    - [Science](#science)\n    - [Search](#search)\n    - [Serialization](#serialization)\n    - [Serverless Frameworks](#serverless-frameworks)\n    - [Specific Formats Processing](#specific-formats-processing)\n    - [Static Site Generator](#static-site-generator)\n    - [Tagging](#tagging)\n    - [Template Engine](#template-engine)\n    - [Testing](#testing)\n    - [Text Processing](#text-processing)\n    - [Third-party APIs](#third-party-apis)\n    - [URL Manipulation](#url-manipulation)\n    - [Video](#video)\n    - [Web Content Extracting](#web-content-extracting)\n    - [Web Crawling \u0026 Web Scraping](#web-crawling--web-scraping)\n    - [Web Frameworks](#web-frameworks)\n    - [WebSocket](#websocket)\n    - [WSGI Servers](#wsgi-servers)\n- [Services](#services)\n    - [Code Quality](#code-quality)\n    - [Continuous Integration](#continuous-integration)\n- [Resources](#resources)\n    - [Podcasts](#podcasts)\n    - [Twitter](#twitter)\n    - [Websites](#websites)\n    - [Weekly](#weekly)\n- [Other Awesome Lists](#other-awesome-lists)\n- [Contributing](#contributing)\n\n- - -\n\n## Admin Panels\n\n*Libraries for administrative interfaces.*\n\n* [Ajenti](https://github.com/ajenti/ajenti) - The admin panel your servers deserve.\n* [django-suit](http://djangosuit.com/) - Alternative Django Admin-Interface (free only for Non-commercial use).\n* [django-xadmin](https://github.com/sshwsfc/xadmin) - Drop-in replacement of Django admin comes with lots of goodies.\n* [flask-admin](https://github.com/flask-admin/flask-admin) - Simple and extensible administrative interface framework for Flask.\n* [flower](https://github.com/mher/flower) - Real-time monitor and web admin for Celery.\n* [Grappelli](http://grappelliproject.com) - A jazzy skin for the Django Admin-Interface.\n* [Wooey](https://github.com/wooey/wooey) - A Django app which creates automatic web UIs for Python scripts.\n\n## Algorithms and Design Patterns\n\n*Python implementation of algorithms and design patterns.*\n\n* [algorithms](https://github.com/keon/algorithms) - Minimal examples of data structures and algorithms in Python.\n* [PyPattyrn](https://github.com/tylerlaberge/PyPattyrn) - A simple yet effective library for implementing common design patterns.\n* [python-patterns](https://github.com/faif/python-patterns) - A collection of design patterns in Python.\n* [sortedcontainers](http://www.grantjenks.com/docs/sortedcontainers/) - Fast, pure-Python implementation of SortedList, SortedDict, and SortedSet types.\n\n## Anti-spam\n\n*Libraries for fighting spam.*\n\n* [django-simple-captcha](https://github.com/mbi/django-simple-captcha) - A simple and highly customizable Django app to add captcha images to any Django form.\n* [django-simple-spam-blocker](https://github.com/moqada/django-simple-spam-blocker) - Simple spam blocker for Django.\n\n## Asset Management\n\n*Tools for managing, compressing and minifying website assets.*\n\n* [django-compressor](https://github.com/django-compressor/django-compressor) - Compresses linked and inline JavaScript or CSS into a single cached file.\n* [django-pipeline](https://github.com/jazzband/django-pipeline) - An asset packaging library for Django.\n* [django-storages](https://github.com/jschneier/django-storages) - A collection of custom storage back ends for Django.\n* [fanstatic](http://www.fanstatic.org/en/latest/) - Packages, optimizes, and serves static file dependencies as Python packages.\n* [fileconveyor](http://wimleers.com/fileconveyor) - A daemon to detect and sync files to CDNs, S3 and FTP.\n* [flask-assets](https://github.com/miracle2k/flask-assets) - Helps you integrate webassets into your Flask app.\n* [jinja-assets-compressor](https://github.com/jaysonsantos/jinja-assets-compressor) - A Jinja extension to compile and compress your assets.\n* [webassets](https://github.com/miracle2k/webassets) - Bundles, optimizes, and manages unique cache-busting URLs for static resources.\n\n## Audio\n\n*Libraries for manipulating audio.*\n\n* [audiolazy](https://github.com/danilobellini/audiolazy) - Expressive Digital Signal Processing (DSP) package for Python.\n* [audioread](https://github.com/beetbox/audioread) - Cross-library (GStreamer + Core Audio + MAD + FFmpeg) audio decoding.\n* [beets](http://beets.io/) - A music library manager and [MusicBrainz](https://musicbrainz.org/) tagger.\n* [dejavu](https://github.com/worldveil/dejavu) - Audio fingerprinting and recognition.\n* [django-elastic-transcoder](https://github.com/StreetVoice/django-elastic-transcoder) - Django + [Amazon Elastic Transcoder](https://aws.amazon.com/elastictranscoder/).\n* [eyeD3](http://eyed3.nicfit.net/) - A tool for working with audio files, specifically MP3 files containing ID3 metadata.\n* [id3reader](https://nedbatchelder.com/code/modules/id3reader.py) - A Python module for reading MP3 meta data.\n* [m3u8](https://github.com/globocom/m3u8) - A module for parsing m3u8 file.\n* [mingus](http://bspaans.github.io/python-mingus/) - An advanced music theory and notation package with MIDI file and playback support.\n* [mutagen](https://github.com/quodlibet/mutagen) - A Python module to handle audio metadata.\n* [pyAudioAnalysis](https://github.com/tyiannak/pyAudioAnalysis) - Python Audio Analysis Library: Feature Extraction, Classification, Segmentation and Applications\n* [pydub](https://github.com/jiaaro/pydub) - Manipulate audio with a simple and easy high level interface.\n* [pyechonest](https://github.com/echonest/pyechonest) - Python client for the [Echo Nest](http://static.echonest.com/enspex/) API.\n* [talkbox](http://scikits.appspot.com/talkbox) - A Python library for speech/signal processing.\n* [TimeSide](https://github.com/Parisson/TimeSide) - Open web audio processing framework.\n* [tinytag](https://github.com/devsnd/tinytag) - A library for reading music meta data of MP3, OGG, FLAC and Wave files.\n\n## Authentication\n\n*Libraries for implementing authentications schemes.*\n\n* OAuth\n    * [Authomatic](https://github.com/authomatic/authomatic) - Simple but powerful framework agnostic authentication/authorization client.\n    * [django-allauth](https://github.com/pennersr/django-allauth) - Authentication app for Django that \"just works.\"\n    * [django-oauth-toolkit](https://github.com/evonove/django-oauth-toolkit) - OAuth 2 goodies for Django.\n    * [Flask-OAuthlib](https://github.com/lepture/flask-oauthlib) - OAuth 1.0/a, 2.0 implementation of client and provider for Flask.\n    * [OAuthLib](https://github.com/idan/oauthlib) - A generic and thorough implementation of the OAuth request-signing logic.\n    * [python-oauth2](https://github.com/joestump/python-oauth2) - A fully tested, abstract interface to creating OAuth clients and servers.\n    * [python-social-auth](https://github.com/omab/python-social-auth) - An easy-to-setup social authentication mechanism.\n    * [rauth](https://github.com/litl/rauth) - A Python library for OAuth 1.0/a, 2.0, and Ofly.\n    * [sanction](https://github.com/demianbrecht/sanction) - A dead simple OAuth2 client implementation.\n* Others\n    * [jose](https://github.com/demonware/jose) - JavaScript Object Signing and Encryption draft implementation.\n    * [PyJWT](https://github.com/jpadilla/pyjwt) - Implementation of the JSON Web Token draft 01.\n    * [python-jws](https://github.com/brianloveswords/python-jws) - Implementation of JSON Web Signatures draft 02.\n    * [python-jwt](https://github.com/davedoesdev/python-jwt) - Module for generating and verifying JSON Web Tokens.\n\n## Build Tools\n\n*Compile software from source code.*\n\n* [BitBake](http://www.yoctoproject.org/docs/1.6/bitbake-user-manual/bitbake-user-manual.html) - A make-like build tool for embedded Linux.\n* [buildout](http://www.buildout.org/en/latest/) - A build system for creating, assembling and deploying applications from multiple parts.\n* [PlatformIO](https://github.com/platformio/platformio-core) - A console tool to build code with different development platforms.\n* [PyBuilder](https://github.com/pybuilder/pybuilder) - A continuous build tool written in pure Python.\n* [SCons](http://www.scons.org/) - A software construction tool.\n\n## Built-in Classes Enhancement\n\n*Libraries for enhancing Python built-in classes.*\n\n* [attrs](https://github.com/python-attrs/attrs) - Replacement for `__init__`, `__eq__`, `__repr__`, etc. boilerplate in class definitions.\n* [bidict](https://github.com/jab/bidict) - Efficient, Pythonic bidirectional map data structures and related functionality..\n* [Box](https://github.com/cdgriffith/Box) - Python dictionaries with advanced dot notation access.\n* [dotted](https://github.com/carlosescri/DottedDict) - A library that provides a method of accessing lists and dicts with a dotted path notation.\n\n## CMS\n\n*Content Management Systems.*\n\n* [django-cms](https://www.django-cms.org/en/) - An Open source enterprise CMS based on the Django.\n* [djedi-cms](http://djedi-cms.org/) - A lightweight but yet powerful Django CMS with plugins, inline editing and performance in mind.\n* [FeinCMS](http://www.feincms.org/) - One of the most advanced Content Management Systems built on Django.\n* [Kotti](http://kotti.pylonsproject.org/) - A high-level, Pythonic web application framework built on Pyramid.\n* [Mezzanine](http://mezzanine.jupo.org/) - A powerful, consistent, and flexible content management platform.\n* [Opps](http://opps.github.io/opps/) - A Django-based CMS for magazines, newspapers websites and portals with high-traffic.\n* [Plone](https://plone.org/) - A CMS built on top of the open source application server Zope.\n* [Quokka](http://quokkaproject.org/) - Flexible, extensible, small CMS powered by Flask and MongoDB.\n* [Wagtail](https://wagtail.io/) - A Django content management system.\n* [Widgy](https://wid.gy/) - Last CMS framework, based on Django.\n\n## Caching\n\n*Libraries for caching data.*\n\n* [Beaker](https://github.com/bbangert/beaker) - A library for caching and sessions for use with web applications and stand-alone Python scripts and applications.\n* [DiskCache](http://www.grantjenks.com/docs/diskcache/) - SQLite and file backed cache backend with faster lookups than memcached and redis.\n* [django-cache-machine](https://github.com/django-cache-machine/django-cache-machine) - Automatic caching and invalidation for Django models.\n* [django-cacheops](https://github.com/Suor/django-cacheops) - A slick ORM cache with automatic granular event-driven invalidation.\n* [django-viewlet](https://github.com/5monkeys/django-viewlet) - Render template parts with extended cache control.\n* [dogpile.cache](http://dogpilecache.readthedocs.io/en/latest/) - dogpile.cache is next generation replacement for Beaker made by same authors.\n* [HermesCache](https://pypi.python.org/pypi/HermesCache) - Python caching library with tag-based invalidation and dogpile effect prevention.\n* [johnny-cache](https://github.com/jmoiron/johnny-cache) - A caching framework for Django applications.\n* [pylibmc](https://github.com/lericson/pylibmc) - A Python wrapper around the [libmemcached](http://libmemcached.org/libMemcached.html) interface.\n\n## ChatOps Tools\n\n*Libraries for chatbot development.*\n\n* [Errbot](http://errbot.io/en/latest/) - The easiest and most popular chatbot to implement ChatOps.\n\n## Cluster Computing\n\n*Frameworks and libraries for Cluster Computing.*\n\n* [PySpark](https://pypi.python.org/pypi/pyspark/) - [Apache Spark](https://spark.apache.org/) Python API.\n* [dask](https://dask.pydata.org/en/latest/) - A flexible parallel computing library for analytic computing.\n* [faust](https://github.com/robinhood/faust) - A stream processing library, porting the ideas from [Kafka Streams](https://kafka.apache.org/documentation/streams/) to Python.\n* [luigi](https://github.com/spotify/luigi) - A module that helps you build complex pipelines of batch jobs.\n* [mrjob](https://github.com/Yelp/mrjob) - Run MapReduce jobs on Hadoop or Amazon Web Services.\n* [streamparse](https://github.com/Parsely/streamparse) - Run Python code against real-time streams of data via [Apache Storm](http://storm.apache.org/).\n\n## Code Analysis\n\n*Tools of static analysis, linters and code quality checkers. See: [awesome-static-analysis](https://github.com/mre/awesome-static-analysis).*\n\n* Code Analysis\n    * [flake8](https://pypi.python.org/pypi/flake8) - A wrapper around pycodestyle, pyflakes and McCabe.\n    * [coala](http://coala.io/) - Language independent and easily extendable code analysis application.\n    * [code2flow](https://github.com/scottrogowski/code2flow) - Turn your Python and JavaScript code into DOT flowcharts.\n    * [prospector](https://github.com/landscapeio/prospector) - A tool to analyse Python code.\n    * [pycallgraph](https://github.com/gak/pycallgraph) - A library that visualises the flow (call graph) of your Python application.\n    * [pylama](https://github.com/klen/pylama) - A code audit tool for Python and JavaScript.\n    * [pylint](https://www.pylint.org/) - A fully customizable source code analyzer.\n* Static Type Checkers\n    * [mypy](http://mypy-lang.org/) - Check variable types during compile time.\n    * [Pyre](https://github.com/facebook/pyre-check) - Performant type checking.\n* Static Type Annotations Generators\n    * [MonkeyType](https://github.com/Instagram/MonkeyType) - A system for Python that generates static type annotations by collecting runtime types\n\n## Command-line Tools\n\n*Libraries for building command-line application.*\n\n* Command-line Application Development\n    * [cement](http://builtoncement.com/) - CLI Application Framework for Python.\n    * [click](http://click.pocoo.org/dev/) - A package for creating beautiful command line interfaces in a composable way.\n    * [cliff](https://docs.openstack.org/developer/cliff/) - A framework for creating command-line programs with multi-level commands.\n    * [clint](https://github.com/kennethreitz/clint) - Python Command-line Application Tools.\n    * [docopt](http://docopt.org/) - Pythonic command line arguments parser.\n    * [Gooey](https://github.com/chriskiehl/Gooey) - Turn command line programs into a full GUI application with one line.\n    * [Python Fire](https://github.com/google/python-fire) - A library for creating command line interfaces from absolutely any Python object.\n    * [python-prompt-toolkit](https://github.com/jonathanslenders/python-prompt-toolkit) - A library for building powerful interactive command lines.\n* Terminal Rendering\n    * [asciimatics](https://github.com/peterbrittain/asciimatics) - A package to create full-screen text UIs (from interactive forms to ASCII animations).\n    * [bashplotlib](https://github.com/glamp/bashplotlib) - Making basic plots in the terminal.\n    * [colorama](https://pypi.python.org/pypi/colorama) - Cross-platform colored terminal text.\n* Productivity Tools\n    * [aws-cli](https://github.com/aws/aws-cli) - A universal command-line interface for Amazon Web Services.\n    * [cookiecutter](https://github.com/audreyr/cookiecutter) - A command-line utility that creates projects from cookiecutters (project templates).\n    * [doitlive](https://github.com/sloria/doitlive) - A tool for live presentations in the terminal.\n    * [howdoi](https://github.com/gleitz/howdoi) - Instant coding answers via the command line.\n    * [httpie](https://github.com/jakubroztocil/httpie) - A command line HTTP client, a user-friendly cURL replacement.\n    * [kube-shell](https://github.com/cloudnativelabs/kube-shell) - An integrated shell for working with the Kubernetes CLI.\n    * [mycli](https://github.com/dbcli/mycli) - A Terminal Client for MySQL with AutoCompletion and Syntax Highlighting.\n    * [PathPicker](https://github.com/facebook/PathPicker) - Select files out of bash output.\n    * [percol](https://github.com/mooz/percol) - Adds flavor of interactive selection to the traditional pipe concept on UNIX.\n    * [pgcli](https://github.com/dbcli/pgcli) - Postgres CLI with autocompletion and syntax highlighting.\n    * [SAWS](https://github.com/donnemartin/saws) - A Supercharged AWS CLI.\n    * [thefuck](https://github.com/nvbn/thefuck) - Correcting your previous console command.\n    * [tmuxp](https://github.com/tony/tmuxp) - A [tmux](https://github.com/tmux/tmux) session manager.\n    * [try](https://github.com/timofurrer/try) - A dead simple CLI to try out python packages - it's never been easier.\n\n## Compatibility\n\n*Libraries for migrating from Python 2 to 3.*\n\n* [Python-Future](http://python-future.org/index.html) - The missing compatibility layer between Python 2 and Python 3.\n* [Python-Modernize](https://github.com/mitsuhiko/python-modernize) - Modernizes Python code for eventual Python 3 migration.\n* [Six](https://pypi.python.org/pypi/six) - Python 2 and 3 compatibility utilities.\n\n## Computer Vision\n\n*Libraries for computer vision.*\n\n* [OpenCV](http://opencv.org/) - Open Source Computer Vision Library.\n* [pyocr](https://github.com/openpaperwork/pyocr) - A wrapper for Tesseract and Cuneiform.\n* [pytesseract](https://github.com/madmaze/pytesseract) - Another wrapper for [Google Tesseract OCR](https://github.com/tesseract-ocr).\n* [SimpleCV](http://simplecv.org/) - An open source framework for building computer vision applications.\n\n## Concurrency and Parallelism\n\n*Libraries for concurrent and parallel execution.*\n\n* [concurrent.futures](https://docs.python.org/3/library/multiprocessing.html) - (Python standard library) Process-based \"[threading](https://docs.python.org/3/library/threading.html)\" interface.\n* [multiprocessing](https://docs.python.org/3/library/multiprocessing.html) - (Python standard library) A high-level interface for asynchronously executing callables.\n* [eventlet](http://eventlet.net/) - Asynchronous framework with WSGI support.\n* [gevent](http://www.gevent.org/) - A coroutine-based Python networking library that uses [greenlet](https://github.com/python-greenlet/greenlet).\n* [SCOOP](https://github.com/soravux/scoop) - Scalable Concurrent Operations in Python.\n* [Tomorrow](https://github.com/madisonmay/Tomorrow) - Magic decorator syntax for asynchronous code.\n* [uvloop](https://github.com/MagicStack/uvloop) - Ultra fast implementation of asyncio event loop on top of libuv.\n\n## Configuration\n\n*Libraries for storing and parsing configuration options.*\n\n* [config](https://www.red-dove.com/config-doc/) - Hierarchical config from the author of [logging](https://docs.python.org/3/library/logging.html).\n* [ConfigObj](https://github.com/DiffSK/configobj) - INI file parser with validation.\n* [ConfigParser](https://docs.python.org/3/library/configparser.html) - (Python standard library) INI file parser.\n* [profig](http://profig.readthedocs.org/en/default/) - Config from multiple formats with value conversion.\n* [python-decouple](https://github.com/henriquebastos/python-decouple) - Strict separation of settings from code.\n\n## Cryptography\n\n* [cryptography](https://cryptography.io/en/latest/) - A package designed to expose cryptographic primitives and recipes to Python developers.\n* [Paramiko](http://www.paramiko.org/) - A Python (2.6+, 3.3+) implementation of the SSHv2 protocol, providing both client and server functionality.\n* [Passlib](https://passlib.readthedocs.io/en/stable/) - Secure password storage/hashing library, very high level.\n* [PyNacl](https://github.com/pyca/pynacl) - Python binding to the Networking and Cryptography (NaCl) library.\n\n## Data Analysis\n\n*Libraries for data analyzing.*\n\n* [Blaze](https://github.com/blaze/blaze) - NumPy and Pandas interface to Big Data.\n* [Open Mining](https://github.com/mining/mining) - Business Intelligence (BI) in Pandas interface.\n* [Orange](https://orange.biolab.si/) - Data mining, data visualization, analysis and machine learning through visual programming or scripts.\n* [Pandas](http://pandas.pydata.org/) - A library providing high-performance, easy-to-use data structures and data analysis tools.\n* [Optimus](https://github.com/ironmussa/Optimus) - Cleansing, pre-processing, feature engineering, exploratory data analysis and easy Machine Learning with a PySpark backend.\n\n## Data Validation\n\n*Libraries for validating data. Used for forms in many cases.*\n\n* [Cerberus](https://github.com/pyeve/cerberus) - A lightweight and extensible data validation library.\n* [colander](https://docs.pylonsproject.org/projects/colander/en/latest/) - Validating and deserializing data obtained via XML, JSON, an HTML form post.\n* [Dash](https://plot.ly/products/dash/) - Built on top of Flask, React and Plotly aimed at analytical web applications.\n    * [awesome-dash](https://github.com/Acrotrend/awesome-dash)\n* [jsonschema](https://github.com/Julian/jsonschema) - An implementation of [JSON Schema](http://json-schema.org/) for Python.\n* [schema](https://github.com/keleshev/schema) - A library for validating Python data structures.\n* [Schematics](https://github.com/schematics/schematics) - Data Structure Validation.\n* [valideer](https://github.com/podio/valideer) - Lightweight extensible data validation and adaptation library.\n* [voluptuous](https://github.com/alecthomas/voluptuous) - A Python data validation library.\n\n## Data Visualization\n\n*Libraries for visualizing data. See: [awesome-javascript](https://github.com/sorrycc/awesome-javascript#data-visualization).*\n\n* [Altair](https://github.com/altair-viz/altair) - Declarative statistical visualization library for Python.\n* [Bokeh](https://github.com/bokeh/bokeh) - Interactive Web Plotting for Python.\n* [bqplot](https://github.com/bloomberg/bqplot) - Interactive Plotting Library for the Jupyter Notebook\n* [ggplot](https://github.com/yhat/ggpy) - Same API as ggplot2 for R.\n* [Matplotlib](http://matplotlib.org/) - A Python 2D plotting library.\n* [Pygal](http://www.pygal.org/en/latest/) - A Python SVG Charts Creator.\n* [PyGraphviz](https://pypi.python.org/pypi/pygraphviz) - Python interface to [Graphviz](http://www.graphviz.org/).\n* [PyQtGraph](http://www.pyqtgraph.org/) - Interactive and realtime 2D/3D/Image plotting and science/engineering widgets.\n* [Seaborn](https://github.com/mwaskom/seaborn) - Statistical data visualization using Matplotlib.\n* [VisPy](https://github.com/vispy/vispy) - High-performance scientific visualization based on OpenGL.\n\n## Database\n\n*Databases implemented in Python.*\n\n* [pickleDB](https://pythonhosted.org/pickleDB/) - A simple and lightweight key-value store for Python.\n* [TinyDB](https://github.com/msiemens/tinydb) - A tiny, document-oriented database.\n* [ZODB](http://www.zodb.org/en/latest/) - A native object database for Python. A key-value and object graph database.\n\n## Database Drivers\n\n*Libraries for connecting and operating databases.*\n\n* MySQL - [awesome-mysql](http://shlomi-noach.github.io/awesome-mysql/)\n    * [mysqlclient](https://github.com/PyMySQL/mysqlclient-python) - MySQL connector with Python 3 support ([mysql-python](https://sourceforge.net/projects/mysql-python/) fork).\n    * [oursql](https://pythonhosted.org/oursql/) - A better MySQL connector with support for native prepared statements and BLOBs.\n    * [PyMySQL](https://github.com/PyMySQL/PyMySQL) - A pure Python MySQL driver compatible to mysql-python.\n* PostgreSQL - [awesome-postgres](https://github.com/dhamaniasad/awesome-postgres)\n    * [psycopg2](http://initd.org/psycopg/) - The most popular PostgreSQL adapter for Python.\n    * [queries](https://github.com/gmr/queries) - A wrapper of the psycopg2 library for interacting with PostgreSQL.\n    * [txpostgres](https://github.com/wulczer/txpostgres) - Twisted based asynchronous driver for PostgreSQL.\n* Other Relational Databases\n    * [apsw](http://rogerbinns.github.io/apsw/) - Another Python SQLite wrapper.\n    * [dataset](https://github.com/pudo/dataset) - Store Python dicts in a database - works with SQLite, MySQL, and PostgreSQL.\n    * [pymssql](http://www.pymssql.org/en/latest/) - A simple database interface to Microsoft SQL Server.\n* NoSQL Databases\n    * [cassandra-driver](https://github.com/datastax/python-driver) - The Python Driver for Apache Cassandra.\n    * [HappyBase](https://github.com/wbolster/happybase) - A developer-friendly library for Apache HBase.\n    * [kafka-python](https://github.com/dpkp/kafka-python) - The Python client for Apache Kafka.\n    * [py2neo](http://py2neo.org/2.0/) - Python wrapper client for Neo4j's restful interface.\n    * [PyMongo](https://docs.mongodb.com/ecosystem/drivers/python/) - The official Python client for MongoDB.\n    * [redis-py](https://github.com/andymccurdy/redis-py) - The Python client for Redis.\n* Asynchronous Clients\n    * [Motor](https://github.com/mongodb/motor) - The async Python driver for MongoDB.\n    * [telephus](https://github.com/driftx/Telephus) - Twisted based client for Cassandra.\n    * [txRedis](https://github.com/deldotdr/txRedis) - Twisted based client for Redis.\n\n## Date and Time\n\n*Libraries for working with dates and times.*\n\n* [Chronyk](https://github.com/KoffeinFlummi/Chronyk) - A Python 3 library for parsing human-written times and dates.\n* [dateutil](https://github.com/dateutil/dateutil) - Extensions to the standard Python [datetime](https://docs.python.org/3/library/datetime.html) module.\n* [delorean](https://github.com/myusuf3/delorean/) - A library for clearing up the inconvenient truths that arise dealing with datetimes.\n* [moment](https://github.com/zachwill/moment) - A Python library for dealing with dates/times. Inspired by [Moment.js](http://momentjs.com/).\n* [Pendulum](https://github.com/sdispater/pendulum) - Python datetimes made easy.\n* [PyTime](https://github.com/shinux/PyTime) - A easy-use Python module which aims to operate date/time/datetime by string.\n* [pytz](https://launchpad.net/pytz) - World timezone definitions, modern and historical. Brings the [tz database](https://en.wikipedia.org/wiki/Tz_database) into Python.\n* [when.py](https://github.com/dirn/When.py) - Providing user-friendly functions to help perform common date and time actions.\n* [maya](https://github.com/kennethreitz/maya) - Datetimes for Humans, Maya is mostly built around the headaches and use-cases around parsing datetime data from websites.\n\n## Debugging Tools\n\n*Libraries for debugging code.*\n\n* pdb-like Debugger\n    * [ipdb](https://pypi.python.org/pypi/ipdb) - IPython-enabled [pdb](https://docs.python.org/3/library/pdb.html).\n    * [pdb++](https://pypi.python.org/pypi/pdbpp/) - Another drop-in replacement for pdb.\n    * [pudb](https://pypi.python.org/pypi/pudb) - A full-screen, console-based Python debugger.\n    * [remote-pdb](https://github.com/ionelmc/python-remote-pdb) - Remote vanilla PDB (over TCP sockets).\n    * [wdb](https://github.com/Kozea/wdb) - An improbable web debugger through WebSockets.\n* Profiler\n    * [line_profiler](https://github.com/rkern/line_profiler) - Line-by-line profiling.\n    * [memory_profiler](https://github.com/fabianp/memory_profiler) - Monitor Memory usage of Python code.\n    * [profiling](https://github.com/what-studio/profiling) - An interactive Python profiler.\n    * [py-spy](https://github.com/benfred/py-spy) - A sampling profiler for Python programs. Written in Rust.\n    * [Pyflame](https://github.com/uber/pyflame) - A ptracing profiler For Python.\n    * [vprof](https://github.com/nvdv/vprof) - Visual Python profiler.\n* Others\n    * [IceCream](https://github.com/gruns/icecream) - Inspect variables, expressions, and program execution with a single, simple function call.\n    * [django-debug-toolbar](https://github.com/jazzband/django-debug-toolbar) - Display various debug information for Django.\n    * [django-devserver](https://github.com/dcramer/django-devserver) - A drop-in replacement for Django's runserver.\n    * [flask-debugtoolbar](https://github.com/mgood/flask-debugtoolbar) - A port of the django-debug-toolbar to flask.\n    * [hunter](https://github.com/ionelmc/python-hunter) - Hunter is a flexible code tracing toolkit.\n    * [lptrace](https://github.com/khamidou/lptrace) - [strace](http://man7.org/linux/man-pages/man1/strace.1.html) for Python programs.\n    * [manhole](https://github.com/ionelmc/python-manhole) - Debug service that will accept unix domain socket connections and present the stacktraces for all threads and an interactive prompt.\n    * [pyelftools](https://github.com/eliben/pyelftools) - Parsing and analyzing ELF files and DWARF debugging information.\n    * [pyringe](https://github.com/google/pyringe) - Debugger capable of attaching to and injecting code into Python processes.\n\n## Deep Learning\n\n*Frameworks for Neural Networks and Deep Learning. See: [awesome-deep-learning](https://github.com/ChristosChristofidis/awesome-deep-learning).*\n\n* [Caffe](https://github.com/BVLC/caffe) - A fast open framework for deep learning..\n* [Keras](https://github.com/fchollet/keras) - A high-level neural networks library and capable of running on top of either TensorFlow or Theano.\n* [MXNet](https://github.com/dmlc/mxnet) - A deep learning framework designed for both efficiency and flexibility.\n* [Neupy](http://neupy.com/pages/home.html) - Running and testing different Artificial Neural Networks algorithms.\n* [Pytorch](http://pytorch.org/) - Tensors and Dynamic neural networks in Python with strong GPU acceleration.\n* [Serpent.AI](https://github.com/SerpentAI/SerpentAI) - Game agent framework. Use any video game as a deep learning sandbox.\n* [TensorFlow](https://github.com/tensorflow/tensorflow) - The most popular Deep Learning framework created by Google.\n* [Theano](https://github.com/Theano/Theano) - A library for fast numerical computation.\n\n## DevOps Tools\n\n*Software and libraries for DevOps.*\n\n* [Ansible](https://github.com/ansible/ansible) - A radically simple IT automation platform.\n* [Cloud-Init](http://cloudinit.readthedocs.io/en/latest/) - A multi-distribution package that handles early initialization of a cloud instance.\n* [cuisine](https://github.com/sebastien/cuisine) - Chef-like functionality for Fabric.\n* [Docker Compose](https://docs.docker.com/compose/) - Fast, isolated development environments using [Docker](https://www.docker.com/).\n* [Fabric](http://www.fabfile.org/) - A simple, Pythonic tool for remote execution and deployment.\n* [Fabtools](https://github.com/fabtools/fabtools) - Tools for writing awesome Fabric files.\n* [honcho](https://github.com/nickstenning/honcho) - A Python clone of [Foreman](https://github.com/ddollar/foreman), for managing Procfile-based applications.\n* [OpenStack](https://www.openstack.org/) - Open source software for building private and public clouds.\n* [pexpect](https://github.com/pexpect/pexpect) - Controlling interactive programs in a pseudo-terminal like GNU expect.\n* [psutil](https://github.com/giampaolo/psutil) - A cross-platform process and system utilities module.\n* [SaltStack](https://github.com/saltstack/salt) - Infrastructure automation and management system.\n* [supervisor](https://github.com/Supervisor/supervisor) - Supervisor process control system for UNIX.\n\n## Distribution\n\n*Libraries to create packaged executables for release distribution.*\n\n* [dh-virtualenv](https://github.com/spotify/dh-virtualenv) - Build and distribute a virtualenv as a Debian package.\n* [Nuitka](http://nuitka.net/) - Compile scripts, modules, packages to an executable or extension module.\n* [py2app](http://pythonhosted.org/py2app/) - Freezes Python scripts (Mac OS X).\n* [py2exe](http://www.py2exe.org/) - Freezes Python scripts (Windows).\n* [PyInstaller](https://github.com/pyinstaller/pyinstaller) - Converts Python programs into stand-alone executables (cross-platform).\n* [pynsist](http://pynsist.readthedocs.io/en/latest/) - A tool to build Windows installers, installers bundle Python itself.\n\n## Documentation\n\n*Libraries for generating project documentation.*\n\n* [Sphinx](http://www.sphinx-doc.org/en/latest/) - Python Documentation generator.\n    * [awesome-sphinxdoc](https://github.com/yoloseem/awesome-sphinxdoc)\n* [MkDocs](http://www.mkdocs.org/) - Markdown friendly documentation generator.\n* [pdoc](https://github.com/BurntSushi/pdoc) - Epydoc replacement to auto generate API documentation for Python libraries.\n* [Pycco](https://github.com/pycco-docs/pycco) - The literate-programming-style documentation generator.\n\n## Downloader\n\n*Libraries for downloading.*\n\n* [s3cmd](https://github.com/s3tools/s3cmd) - A command line tool for managing Amazon S3 and CloudFront.\n* [s4cmd](https://github.com/bloomreach/s4cmd) - Super S3 command line tool, good for higher performance.\n* [you-get](http://you-get.org/) - A YouTube/Youku/Niconico video downloader written in Python 3.\n* [youtube-dl](http://rg3.github.io/youtube-dl/) - A small command-line program to download videos from YouTube.\n\n## E-commerce\n\n*Frameworks and libraries for e-commerce and payments.*\n\n* [alipay](https://github.com/lxneng/alipay) - Unofficial Alipay API for Python.\n* [Cartridge](https://github.com/stephenmcd/cartridge) - A shopping cart app built using the Mezzanine.\n* [django-oscar](http://oscarcommerce.com/) - An open-source e-commerce framework for Django.\n* [django-shop](https://github.com/awesto/django-shop) - A Django based shop system.\n* [merchant](https://github.com/agiliq/merchant) - A Django app to accept payments from various payment processors.\n* [money](https://github.com/carlospalol/money) - Money class with optional CLDR-backed locale-aware formatting and an extensible currency exchange solution.\n* [python-currencies](https://github.com/Alir3z4/python-currencies) - Display money format and its filthy currencies.\n* [forex-python](https://github.com/MicroPyramid/forex-python) - Foreign exchange rates, Bitcoin price index and currency conversion.\n* [saleor](http://getsaleor.com/) - An e-commerce storefront for Django.\n* [shoop](https://www.shuup.com/en/) - An open source E-Commerce platform based on Django.\n\n## Editor Plugins and IDEs\n\n* Emacs\n    * [Elpy](https://github.com/jorgenschaefer/elpy) - Emacs Python Development Environment.\n* Sublime Text\n    * [Anaconda](https://github.com/DamnWidget/anaconda) - Anaconda turns your Sublime Text 3 in a full featured Python development IDE.\n    * [SublimeJEDI](https://github.com/srusskih/SublimeJEDI) - A Sublime Text plugin to the awesome auto-complete library Jedi.\n* Vim\n    * [Jedi-vim](https://github.com/davidhalter/jedi-vim) - Vim bindings for the Jedi auto-completion library for Python.\n    * [Python-mode](https://github.com/python-mode/python-mode) - An all in one plugin for turning Vim into a Python IDE.\n    * [YouCompleteMe](https://github.com/Valloric/YouCompleteMe) - Includes [Jedi](https://github.com/davidhalter/jedi)-based completion engine for Python.\n* Visual Studio\n    * [PTVS](https://github.com/Microsoft/PTVS) - Python Tools for Visual Studio.\n* Visual Studio Code\n    * [Python](https://github.com/DonJayamanne/pythonVSCode) - An extension with rich support for the Python language, with features including linting, IntelliSense, formatting, refactoring, debugging, unit tests, and jupyter support.\n    * [Magic Python](https://github.com/MagicStack/MagicPython) - Cutting edge Python syntax highlighter for Sublime Text, Atom, and Visual Studio Code. Used by GitHub to highlight your Python code!\n* IDE\n    * [LiClipse](http://www.liclipse.com/) - Free polyglot IDE based on Eclipse. Uses PyDev for Python support.\n    * [PyCharm](https://www.jetbrains.com/pycharm/) - Commercial Python IDE by JetBrains. Has free community edition available.\n    * [Spyder](https://github.com/spyder-ide/spyder) - Open Source Python IDE.\n\n## Email\n\n*Libraries for sending and parsing email.*\n\n* [envelopes](http://tomekwojcik.github.io/envelopes/) - Mailing for human beings.\n* [flanker](https://github.com/mailgun/flanker) - A email address and Mime parsing library.\n* [imbox](https://github.com/martinrusev/imbox) - Python IMAP for Humans.\n* [inbox.py](https://github.com/kennethreitz/inbox.py) - Python SMTP Server for Humans.\n* [lamson](https://github.com/zedshaw/lamson) - Pythonic SMTP Application Server.\n* [Marrow Mailer](https://github.com/marrow/mailer) - High-performance extensible mail delivery framework.\n* [modoboa](https://github.com/modoboa/modoboa) - A mail hosting and management platform including a modern and simplified Web UI.\n* [Nylas Sync Engine](https://github.com/nylas/sync-engine) - Providing a RESTful API on top of a powerful email sync platform.\n* [yagmail](https://github.com/kootenpv/yagmail) - Yet another Gmail/SMTP client.\n\n## Environment Management\n\n*Libraries for Python version and environment management.*\n\n* [Pipenv](https://github.com/kennethreitz/pipenv) - Sacred Marriage of Pipfile, Pip, \u0026 Virtualenv.\n* [p](https://github.com/qw3rtman/p) - Dead simple interactive Python version management.\n* [pyenv](https://github.com/pyenv/pyenv) - Simple Python version management.\n* [venv](https://docs.python.org/3/library/venv.html) - (Python standard library in Python 3.3+) Creating lightweight virtual environments.\n* [virtualenv](https://pypi.python.org/pypi/virtualenv) - A tool to create isolated Python environments.\n* [virtualenvwrapper](https://pypi.python.org/pypi/virtualenvwrapper) - A set of extensions to virtualenv.\n\n## Files\n\n*Libraries for file manipulation and MIME type detection.*\n\n* [mimetypes](https://docs.python.org/3/library/mimetypes.html) - (Python standard library) Map filenames to MIME types.\n* [path.py](https://github.com/jaraco/path.py) - A module wrapper for [os.path](https://docs.python.org/3/library/os.path.html).\n* [pathlib](https://docs.python.org/3/library/pathlib.html) - (Python standard library) An cross-platform, object-oriented path library.\n* [PyFilesystem2](https://github.com/pyfilesystem/pyfilesystem2) - Python's filesystem abstraction layer.\n* [python-magic](https://github.com/ahupp/python-magic) - A Python interface to the libmagic file type identification library.\n* [Unipath](https://github.com/mikeorr/Unipath) - An object-oriented approach to file/directory operations.\n* [watchdog](https://github.com/gorakhargosh/watchdog) - API and shell utilities to monitor file system events.\n\n## Foreign Function Interface\n\n*Libraries for providing foreign function interface.*\n\n* [cffi](https://pypi.python.org/pypi/cffi) - Foreign Function Interface for Python calling C code.\n* [ctypes](https://docs.python.org/3/library/ctypes.html) - (Python standard library) Foreign Function Interface for Python calling C code.\n* [PyCUDA](https://mathema.tician.de/software/pycuda/) - A Python wrapper for Nvidia's CUDA API.\n* [SWIG](http://www.swig.org/Doc1.3/Python.html) - Simplified Wrapper and Interface Generator.\n\n## Forms\n\n*Libraries for working with forms.*\n\n* [Deform](https://github.com/Pylons/deform) - Python HTML form generation library influenced by the formish form generation library.\n* [django-bootstrap3](https://github.com/dyve/django-bootstrap3) - Bootstrap 3 integration with Django.\n* [django-crispy-forms](https://github.com/django-crispy-forms/django-crispy-forms) - A Django app which lets you create beautiful forms in a very elegant and DRY way.\n* [django-remote-forms](https://github.com/WiserTogether/django-remote-forms) - A platform independent Django form serializer.\n* [WTForms](https://github.com/wtforms/wtforms) - A flexible forms validation and rendering library.\n\n## Functional Programming\n\n*Functional Programming with Python.*\n\n* [Coconut](http://coconut-lang.org/) - Coconut is a variant of Python built for simple, elegant, Pythonic functional programming.\n* [CyToolz](https://github.com/pytoolz/cytoolz/) - Cython implementation of Toolz: High performance functional utilities.\n* [fn.py](https://github.com/kachayev/fn.py) - Functional programming in Python: implementation of missing features to enjoy FP.\n* [funcy](https://github.com/Suor/funcy) - A fancy and practical functional tools.\n* [Toolz](https://github.com/pytoolz/toolz) - A collection of functional utilities for iterators, functions, and dictionaries.\n\n## GUI\n\n*Libraries for working with graphical user interface applications.*\n\n* [curses](https://docs.python.org/3/library/curses.html) - Built-in wrapper for [ncurses](http://www.gnu.org/software/ncurses/) used to create terminal GUI applications.\n* [Eel](https://github.com/ChrisKnott/Eel) - Little library for making simple Electron-like offline HTML/JS GUI apps, with full access to Python capabilities and libraries.\n* [enaml](https://github.com/nucleic/enaml) - Creating beautiful user-interfaces with Declaratic Syntax like QML.\n* [Flexx](https://github.com/zoofIO/flexx) - Flexx is a pure Python toolkit for creating GUI's, that uses web technology for its rendering.\n* [kivy](https://kivy.org/) - A library for creating NUI applications, running on Windows, Linux, Mac OS X, Android and iOS.\n* [pyglet](https://bitbucket.org/pyglet/pyglet/wiki/Home) - A cross-platform windowing and multimedia library for Python.\n* [PyGObject](https://wiki.gnome.org/Projects/PyGObject) - Python Bindings for GLib/GObject/GIO/GTK+ (GTK+3).\n* [PyQt](https://riverbankcomputing.com/software/pyqt/intro) - Python bindings for the [Qt](https://www.qt.io/) cross-platform application and UI framework, with support for both Qt v4 and Qt v5 frameworks.\n* [PySide](https://wiki.qt.io/PySide) - Python bindings for the [Qt](http://www.qt.io/) cross-platform application and UI framework, supporting the Qt v4 framework.\n* [PySimpleGUI](https://github.com/MikeTheWatchGuy/PySimpleGUI) - Wrapper for tkinter that creates a more Python-like interface for beginner and intermediate level custom GUIs.\n* [pywebview](https://github.com/r0x0r/pywebview/) - A lightweight cross-platform native wrapper around a webview component that allows to display HTML content in its own native dedicated window.\n* [Tkinter](https://wiki.python.org/moin/TkInter) - Tkinter is Python's de-facto standard GUI package.\n* [Toga](https://github.com/pybee/toga) - A Python native, OS native GUI toolkit.\n* [urwid](http://urwid.org/) - A library for creating terminal GUI applications with strong support for widgets, events, rich colors, etc.\n* [wxPython](https://wxpython.org/) - A blending of the wxWidgets C++ class library with the Python.\n\n## Game Development\n\n*Awesome game development libraries.*\n\n* [Cocos2d](http://cocos2d.org/) - cocos2d is a framework for building 2D games, demos, and other graphical/interactive applications. It is based on pyglet.\n* [Panda3D](https://www.panda3d.org/) - 3D game engine developed by Disney and maintained by Carnegie Mellon's Entertainment Technology Center. Written in C++, completely wrapped in Python.\n* [Pygame](http://www.pygame.org/news.html) - Pygame is a set of Python modules designed for writing games.\n* [PyOgre](http://www.ogre3d.org/tikiwiki/PyOgre) - Python bindings for the Ogre 3D render engine, can be used for games, simulations, anything 3D.\n* [PyOpenGL](http://pyopengl.sourceforge.net/) - Python ctypes bindings for OpenGL and it's related APIs.\n* [PySDL2](http://pysdl2.readthedocs.io/en/rel_0_9_5/) - A ctypes based wrapper for the SDL2 library.\n* [RenPy](https://www.renpy.org/) - A Visual Novel engine.\n\n## Geolocation\n\n*Libraries for geocoding addresses and working with latitudes and longitudes.*\n\n* [django-countries](https://github.com/SmileyChris/django-countries) - A Django app that provides country choices for use with forms, flag icons static files, and a country field for models.\n* [GeoDjango](https://docs.djangoproject.com/en/dev/ref/contrib/gis/) - A world-class geographic web framework.\n* [GeoIP](https://github.com/maxmind/geoip-api-python) - Python API for MaxMind GeoIP Legacy Database.\n* [geojson](https://github.com/frewsxcv/python-geojson) - Python bindings and utilities for GeoJSON.\n* [geopy](https://github.com/geopy/geopy) - Python Geocoding Toolbox.\n* [pygeoip](https://github.com/appliedsec/pygeoip) - Pure Python GeoIP API.\n\n## HTML Manipulation\n\n*Libraries for working with HTML and XML.*\n\n* [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) - Providing Pythonic idioms for iterating, searching, and modifying HTML or XML.\n* [bleach](https://github.com/mozilla/bleach) - A whitelist-based HTML sanitization and text linkification library.\n* [cssutils](https://pypi.python.org/pypi/cssutils/) - A CSS library for Python.\n* [html5lib](https://github.com/html5lib/html5lib-python) - A standards-compliant library for parsing and serializing HTML documents and fragments.\n* [lxml](http://lxml.de/) - A very fast, easy-to-use and versatile library for handling HTML and XML.\n* [MarkupSafe](https://github.com/pallets/markupsafe) - Implements a XML/HTML/XHTML Markup safe string for Python.\n* [pyquery](https://github.com/gawel/pyquery) - A jQuery-like library for parsing HTML.\n* [untangle](https://github.com/stchris/untangle) - Converts XML documents to Python objects for easy access.\n* [WeasyPrint](http://weasyprint.org) - A visual rendering engine for HTML and CSS that can export to PDF.\n* [xmldataset](https://xmldataset.readthedocs.io/en/latest/) - Simple XML Parsing.\n* [xmltodict](https://github.com/martinblech/xmltodict) - Working with XML feel like you are working with JSON.\n\n## HTTP\n\n*Libraries for working with HTTP.*\n\n* [grequests](https://github.com/kennethreitz/grequests) - requests + gevent for asynchronous HTTP requests.\n* [httplib2](https://github.com/httplib2/httplib2) - Comprehensive HTTP client library.\n* [requests](http://docs.python-requests.org/en/latest/) - HTTP Requests for Humans™.\n* [treq](https://github.com/twisted/treq) - Python requests like API built on top of Twisted's HTTP client.\n* [urllib3](https://github.com/shazow/urllib3) - A HTTP library with thread-safe connection pooling, file post support, sanity friendly.\n\n## Hardware\n\n*Libraries for programming with hardware.*\n\n* [ino](http://inotool.org/) - Command line toolkit for working with [Arduino](https://www.arduino.cc/).\n* [keyboard](https://github.com/boppreh/keyboard) - Hook and simulate global keyboard events on Windows and Linux.\n* [mouse](https://github.com/boppreh/mouse) - Hook and simulate global mouse events on Windows and Linux.\n* [Pingo](http://www.pingo.io/) - Pingo provides a uniform API to program devices like the Raspberry Pi, pcDuino, Intel Galileo, etc.\n* [PyUserInput](https://github.com/SavinaRoja/PyUserInput) - A module for cross-platform control of the mouse and keyboard.\n* [scapy](https://github.com/secdev/scapy) - A brilliant packet manipulation library.\n* [wifi](https://github.com/rockymeza/wifi) - A Python library and command line tool for working with WiFi on Linux.\n\n## Image Processing\n\n*Libraries for manipulating images.*\n\n* [hmap](https://github.com/rossgoodwin/hmap) - Image histogram remapping.\n* [imgSeek](https://sourceforge.net/projects/imgseek/) - A project for searching a collection of images using visual similarity.\n* [nude.py](https://github.com/hhatto/nude.py) - Nudity detection.\n* [pagan](https://github.com/daboth/pagan) - Retro identicon (Avatar) generation based on input string and hash.\n* [pillow](https://github.com/python-pillow/Pillow) - Pillow is the friendly [PIL](http://www.pythonware.com/products/pil/) fork.\n* [pyBarcode](https://pythonhosted.org/pyBarcode/) - Create barcodes in Python without needing PIL.\n* [pygram](https://github.com/ajkumar25/pygram) - Instagram-like image filters.\n* [python-qrcode](https://github.com/lincolnloop/python-qrcode) - A pure Python QR Code generator.\n* [Quads](https://github.com/fogleman/Quads) - Computer art based on quadtrees.\n* [scikit-image](http://scikit-image.org/) - A Python library for (scientific) image processing.\n* [thumbor](https://github.com/thumbor/thumbor) - A smart imaging service. It enables on-demand crop, re-sizing and flipping of images.\n* [wand](https://github.com/dahlia/wand) - Python bindings for [MagickWand](http://www.imagemagick.org/script/magick-wand.php), C API for ImageMagick.\n\n## Implementations\n\n*Implementations of Python.*\n\n* [CLPython](https://github.com/metawilm/cl-python) - Implementation of the Python programming language written in Common Lisp.\n* [CPython](https://github.com/python/cpython) - **Default, most widely used implementation of the Python programming language written in C.**\n* [Cython](http://cython.org/) - Optimizing Static Compiler for Python. Uses type mixins to compile Python into C or C++ modules resulting in large performance gains\n* [Grumpy](https://github.com/google/grumpy) - More compiler than interpreter as more powerful CPython2.7 replacement (alpha).\n* [IronPython](https://github.com/IronLanguages/ironpython3) - Implementation of the Python programming language written in C# targeting the .NET Framework and Mono.\n* [Jython](https://hg.python.org/jython) - Implementation of Python programming language written in Java for the Java virtual machine (JVM).\n* [MicroPython](https://github.com/micropython/micropython) - MicroPython - a lean and efficient Python programming language implementation for microcontrollers and constrained systems\n* [Numba](http://numba.pydata.org/) - Python JIT compiler to LLVM aimed at scientific Python.\n* [PeachPy](https://github.com/Maratyszcza/PeachPy) - x86-64 assembler embedded in Python. Can be used as inline assembler for Python or as a stand-alone assembler for Windows, Linux, OS X, Native Client and Go.\n* [Pyjion](https://github.com/Microsoft/Pyjion) - A JIT for Python based upon CoreCLR.\n* [PyPy](https://bitbucket.org/pypy/pypy) - Implementation of the Python programming language written in RPython and translated into C. PyPy focuses on speed, efficiency and compatibility with the original CPython interpreter. The interpreter uses black magic to make Python very fast without having to add in additional type information.\n* [PySec](https://github.com/ebranca/owasp-pysec) - Hardened version of python that makes it easier for security professionals and developers to write applications more resilient to attacks and manipulations.\n* [Pyston](https://github.com/dropbox/pyston) - A Python implementation built using LLVM and modern JIT techniques with the goal of achieving good performance.\n* [Stackless Python](https://github.com/stackless-dev/stackless/wiki) - An enhanced version of the Python programming language which allows programmers to reap the benefits of thread-based programming without the performance and complexity problems associated with conventional threads.\n\n## Interactive Interpreter\n\n*Interactive Python interpreters (REPL).*\n\n* [bpython](https://github.com/bpython/bpython) - A fancy interface to the Python interpreter.\n* [Jupyter Notebook (IPython)](https://jupyter.org) - A rich toolkit to help you make the most out of using Python interactively.\n    * [awesome-jupyter](https://github.com/markusschanta/awesome-jupyter)\n* [ptpython](https://github.com/jonathanslenders/ptpython) - Advanced Python REPL built on top of the [python-prompt-toolkit](https://github.com/jonathanslenders/python-prompt-toolkit).\n\n## Internationalization\n\n*Libraries for working with i18n.*\n\n* [Babel](http://babel.pocoo.org/en/latest/) - An internationalization library for Python.\n* [PyICU](https://github.com/ovalhub/pyicu) - A wrapper of International Components for Unicode C++ library ([ICU](http://site.icu-project.org/)).\n\n## Job Scheduler\n\n*Libraries for scheduling jobs.*\n\n* [APScheduler](http://apscheduler.readthedocs.io/en/latest/) - A light but powerful in-process task scheduler that lets you schedule functions.\n* [django-schedule](https://github.com/thauber/django-schedule) - A calendaring app for Django.\n* [doit](http://pydoit.org/) - A task runner and build tool.\n* [gunnery](https://github.com/gunnery/gunnery) - Multipurpose task execution tool for distributed systems with web-based interface.\n* [Joblib](http://pythonhosted.org/joblib/index.html) - A set of tools to provide lightweight pipelining in Python.\n* [Plan](https://github.com/fengsp/plan) - Writing crontab file in Python like a charm.\n* [schedule](https://github.com/dbader/schedule) - Python job scheduling for humans.\n* [Spiff](https://github.com/knipknap/SpiffWorkflow) - A powerful workflow engine implemented in pure Python.\n* [TaskFlow](https://docs.openstack.org/developer/taskflow/) - A Python library that helps to make task execution easy, consistent and reliable.\n\n## Logging\n\n*Libraries for generating and working with logs.*\n\n* [Eliot](https://github.com/ScatterHQ/eliot) - Logging for complex \u0026 distributed systems.\n* [logbook](http://logbook.readthedocs.io/en/stable/) - Logging replacement for Python.\n* [logging](https://docs.python.org/3/library/logging.html) - (Python standard library) Logging facility for Python.\n* [raven](https://github.com/getsentry/raven-python) - Python client for Sentry, a log/error tracking, crash reporting and aggregation platform for web applications.\n\n## Machine Learning\n\n*Libraries for Machine Learning. See: [awesome-machine-learning](https://github.com/josephmisiti/awesome-machine-learning#python).*\n\n* [H2O](https://github.com/h2oai/h2o-3) - Open Source Fast Scalable Machine Learning Platform.\n* [Metrics](https://github.com/benhamner/Metrics) - Machine learning evaluation metrics.\n* [NuPIC](https://github.com/numenta/nupic) - Numenta Platform for Intelligent Computing.\n* [scikit-learn](http://scikit-learn.org/) - The most popular Python library for Machine Learning.\n* [Spark ML](http://spark.apache.org/docs/latest/ml-guide.html) - [Apache Spark](http://spark.apache.org/)'s scalable Machine Learning library.\n* [vowpal_porpoise](https://github.com/josephreisinger/vowpal_porpoise) - A lightweight Python wrapper for [Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit/).\n* [xgboost](https://github.com/dmlc/xgboost) - A scalable, portable, and distributed gradient boosting library.\n\n## Microsoft Windows\n\n*Python programming on Microsoft Windows.*\n\n* [Python(x,y)](http://python-xy.github.io/) - Scientific-applications-oriented Python Distribution based on Qt and Spyder.\n* [pythonlibs](http://www.lfd.uci.edu/~gohlke/pythonlibs/) - Unofficial Windows binaries for Python extension packages.\n* [PythonNet](https://github.com/pythonnet/pythonnet) - Python Integration with the .NET Common Language Runtime (CLR).\n* [PyWin32](https://sourceforge.net/projects/pywin32/) - Python Extensions for Windows.\n* [WinPython](https://winpython.github.io/) - Portable development environment for Windows 7/8.\n\n## Miscellaneous\n\n*Useful libraries or tools that don't fit in the categories above.*\n\n* [blinker](https://github.com/jek/blinker) - A fast Python in-process signal/event dispatching system.\n* [itsdangerous](https://github.com/pallets/itsdangerous) - Various helpers to pass trusted data to untrusted environments.\n* [pluginbase](https://github.com/mitsuhiko/pluginbase) - A simple but flexible plugin system for Python.\n* [Pychievements](https://github.com/PacketPerception/pychievements) - A framework for creating and tracking achievements.\n* [Tryton](http://www.tryton.org/) - A general purpose business framework.\n\n## Natural Language Processing\n\n*Libraries for working with human languages.*\n\n* [gensim](https://github.com/RaRe-Technologies/gensim) - Topic Modelling for Humans.\n* [Jieba](https://github.com/fxsjy/jieba) - Chinese text segmentation.\n* [langid.py](https://github.com/saffsd/langid.py) - Stand-alone language identification system.\n* [NLTK](http://www.nltk.org/) - A leading platform for building Python programs to work with human language data.\n* [Pattern](http://www.clips.ua.ac.be/pattern) - A web mining module for the Python.\n* [polyglot](https://github.com/aboSamoor/polyglot) - Natural language pipeline supporting hundreds of languages.\n* [SnowNLP](https://github.com/isnowfy/snownlp) - A library for processing Chinese text.\n* [spaCy](https://spacy.io/) - A library for industrial-strength natural language processing in Python and Cython.\n* [TextBlob](https://github.com/sloria/TextBlob) - Providing a consistent API for diving into common NLP tasks.\n* [PyTorch-NLP](https://github.com/PetrochukM/PyTorch-NLP) - A toolkit enabling rapid deep learning NLP prototyping for research.\n\n## Network Virtualization\n\n*Tools and libraries for Virtual Networking and SDN (Software Defined Networking).*\n\n* [Mininet](http://mininet.org/) - A popular network emulator and API written in Python.\n* [POX](https://github.com/noxrepo/pox) - An open source development platform for Python-based Software Defined Networking (SDN) control applications, such as OpenFlow SDN controllers.\n* [Pyretic](http://frenetic-lang.org/pyretic/) - A member of the Frenetic family of SDN programming languages that provides powerful abstractions over network switches or emulators.\n* [SDX Platform](https://github.com/sdn-ixp/internet2award) - SDN based IXP implementation that leverages Mininet, POX and Pyretic.\n\n## Networking\n\n*Libraries for networking programming.*\n\n* [asyncio](https://docs.python.org/3/library/asyncio.html) - (Python standard library) Asynchronous I/O, event loop, coroutines and tasks.\n    - [awesome-asyncio](https://github.com/timofurrer/awesome-asyncio)\n* [diesel](https://github.com/dieseldev/diesel) - Greenlet-based event I/O Framework for Python.\n* [pulsar](https://github.com/quantmind/pulsar) - Event-driven concurrent framework for Python.\n* [pyzmq](http://zeromq.github.io/pyzmq/) - A Python wrapper for the ZeroMQ message library.\n* [Twisted](https://twistedmatrix.com/trac/) - An event-driven networking engine.\n* [txZMQ](https://github.com/smira/txZMQ) - Twisted based wrapper for the ZeroMQ message library.\n* [NAPALM](https://github.com/napalm-automation/napalm) - Cross-vendor API to manipulate network devices.\n\n## News Feed\n\n*Libraries for building user's activities.*\n\n* [django-activity-stream](https://github.com/justquick/django-activity-stream) - Generating generic activity streams from the actions on your site.\n* [Stream-Framework](https://github.com/tschellenbach/Stream-Framework) - Building newsfeed and notification systems using Cassandra and Redis.\n\n## ORM\n\n*Libraries that implement Object-Relational Mapping or data mapping techniques.*\n\n* Relational Databases\n    * [Django Models](https://docs.djangoproject.com/en/dev/topics/db/models/) - A part of Django.\n    * [SQLAlchemy](http://www.sqlalchemy.org/) - The Python SQL Toolkit and Object Relational Mapper.\n        * [awesome-sqlalchemy](https://github.com/dahlia/awesome-sqlalchemy)\n    * [Orator](https://orator-orm.com) -  The Orator ORM provides a simple yet beautiful ActiveRecord implementation.\n    * [Peewee](https://github.com/coleifer/peewee) - A small, expressive ORM.\n    * [PonyORM](https://ponyorm.com/) - ORM that provides a generator-oriented interface to SQL.\n    * [pyDAL](https://github.com/web2py/pydal/) - A pure Python Database Abstraction Layer.\n    * [python-sql](https://pypi.python.org/pypi/python-sql) - Write SQL queries pythonically.\n* NoSQL Databases\n    * [django-mongodb-engine](https://github.com/django-nonrel/mongodb-engine) - Django MongoDB Backend.\n    * [flywheel](https://github.com/stevearc/flywheel) - Object mapper for Amazon DynamoDB.\n    * [hot-redis](https://github.com/stephenmcd/hot-redis) - Rich Python data types for Redis.\n    * [MongoEngine](http://mongoengine.org/) - A Python Object-Document-Mapper for working with MongoDB.\n    * [PynamoDB](https://github.com/pynamodb/PynamoDB) - A Pythonic interface for [Amazon DynamoDB](https://aws.amazon.com/dynamodb/).\n    * [redisco](https://github.com/kiddouk/redisco) - A Python Library for Simple Models and Containers Persisted in Redis.\n* Others\n    * [butterdb](https://github.com/terrible-ideas/butterdb) - A Python ORM for Google Drive Spreadsheets.\n    * [dataset](https://github.com/pudo/dataset) - A JSON-based database.\n\n## Package Management\n\n*Libraries for package and dependency management.*\n\n* [pip](https://pip.pypa.io/en/stable/) - The Python package and dependency manager.\n    * [Python Package Index](https://pypi.python.org/pypi)\n* [conda](https://github.com/conda/conda/) - Cross-platform, Python-agnostic binary package manager.\n* [Curdling](http://clarete.li/curdling/) - Curdling is a command line tool for managing Python packages.\n* [pip-tools](https://github.com/jazzband/pip-tools) - A set of tools to keep your pinned Python dependencies fresh.\n* [wheel](http://pythonwheels.com/) - The new standard of Python distribution and are intended to replace eggs.\n\n## Package Repositories\n\n*Local PyPI repository server and proxies.*\n\n* [warehouse](https://github.com/pypa/warehouse) - Next generation Python Package Repository (PyPI).\n    * [Warehouse](https://pypi.org/)\n* [bandersnatch](https://bitbucket.org/pypa/bandersnatch) - PyPI mirroring tool provided by Python Packaging Authority (PyPA).\n* [devpi](http://doc.devpi.net/latest/) - PyPI server and packaging/testing/release tool.\n* [localshop](https://github.com/jazzband/localshop) - Local PyPI server (custom packages and auto-mirroring of pypi).\n\n## Permissions\n\n*Libraries that allow or deny users access to data or functionality.*\n\n* [Carteblanche](https://github.com/neuman/python-carteblanche/) - Module to align code with thoughts of users and designers. Also magically handles navigation and permissions.\n* [django-guardian](https://github.com/django-guardian/django-guardian) - Implementation of per object permissions for Django 1.2+\n* [django-rules](https://github.com/dfunckt/django-rules) - A tiny but powerful app providing object-level permissions to Django, without requiring a database.\n\n## Processes\n\n*Libraries for starting and communicating with OS processes.*\n\n* [delegator.py](https://github.com/kennethreitz/delegator.py) - [Subprocesses](https://docs.python.org/3.6/library/subprocess.html) for Humans™ 2.0.\n* [sarge](http://sarge.readthedocs.io/en/latest/) - Yet another wrapper for subprocess.\n* [sh](https://github.com/amoffat/sh) - A full-fledged subprocess replacement for Python.\n\n## Queue\n\n*Libraries for working with event and task queues.*\n\n* [celery](http://www.celeryproject.org/) - An asynchronous task queue/job queue based on distributed message passing.\n* [huey](https://github.com/coleifer/huey) - Little multi-threaded task queue.\n* [mrq](https://github.com/pricingassistant/mrq) - Mr. Queue - A distributed worker task queue in Python using Redis \u0026 gevent.\n* [rq](http://python-rq.org/) - Simple job queues for Python.\n* [simpleq](https://github.com/rdegges/simpleq) - A simple, infinitely scalable, Amazon SQS based queue.\n\n## Recommender Systems\n\n*Libraries for building recommender systems.*\n\n* [annoy](https://github.com/spotify/annoy) - Approximate Nearest Neighbors in C++/Python optimized for memory usage.\n* [fastFM](https://github.com/ibayer/fastFM) - A library for Factorization Machines.\n* [implicit](https://github.com/benfred/implicit) - A fast Python implementation of collaborative filtering for implicit datasets.\n* [libffm](https://github.com/guestwalk/libffm) - A library for Field-aware Factorization Machine (FFM).\n* [LightFM](https://github.com/lyst/lightfm) - A Python implementation of a number of popular recommendation algorithms.\n* [Spotlight](https://github.com/maciejkula/spotlight) - Deep recommender models using PyTorch.\n* [surprise](http://surpriselib.com) - A scikit for building and analyzing recommender systems.\n* [TensorRec](https://github.com/jfkirk/tensorrec) - A Recommendation Engine Framework in TensorFlow.\n\n## RESTful API\n\n*Libraries for developing RESTful APIs.*\n\n* Django\n    * [django-rest-framework](http://www.django-rest-framework.org/) - A powerful and flexible toolkit to build web APIs.\n    * [django-tastypie](http://tastypieapi.org/) - Creating delicious APIs for Django apps.\n* Flask\n    * [eve](https://github.com/pyeve/eve) - REST API framework powered by Flask, MongoDB and good intentions.\n    * [flask-api-utils](https://github.com/marselester/flask-api-utils) - Taking care of API representation and authentication for Flask.\n    * [flask-api](http://www.flaskapi.org/) - Browsable Web APIs for Flask.\n    * [flask-restful](https://github.com/flask-restful/flask-restful) - Quickly building REST APIs for Flask.\n    * [flask-restless](https://github.com/jfinkels/flask-restless) - Generating RESTful APIs for database models defined with SQLAlchemy.\n* Pyramid\n    * [cornice](https://github.com/Cornices/cornice) - A RESTful framework for Pyramid.\n* Framework agnostic\n    * [falcon](http://falconframework.org/) - A high-performance framework for building cloud APIs and web app backends.\n    * [hug](https://github.com/timothycrosley/hug) - A Python3 framework for cleanly exposing APIs over HTTP and the Command Line with automatic documentation and validation.\n    * [restless](https://github.com/toastdriven/restless) - Framework agnostic REST framework based on lessons learned from Tastypie.\n    * [ripozo](https://github.com/vertical-knowledge/ripozo) - Quickly creating REST/HATEOAS/Hypermedia APIs.\n    * [sandman](https://github.com/jeffknupp/sandman) - Automated REST APIs for existing database-driven systems.\n    * [apistar](https://github.com/encode/apistar) - A smart Web API framework, designed for Python 3.\n\n## Robotics\n\n*Libraries for robotics.*\n\n* [PythonRobotics](https://github.com/AtsushiSakai/PythonRobotics) - This is a compilation of various robotics algorithms with visualizations.\n* [rospy](http://wiki.ros.org/rospy) - This is a library for ROS (Robot Operating System).\n\n## RPC Servers\n\n*RPC-compatible servers.*\n\n* [SimpleJSONRPCServer](https://github.com/joshmarshall/jsonrpclib/) - This library is an implementation of the JSON-RPC specification.\n* [SimpleXMLRPCServer](https://docs.python.org/3/library/xmlrpc.server.html) - (Python standard library) Simple XML-RPC server implementation, single-threaded.\n* [zeroRPC](https://github.com/0rpc/zerorpc-python) - zerorpc is a flexible RPC implementation based on [ZeroMQ](http://zeromq.org/) and [MessagePack](http://msgpack.org/).\n\n## Science\n\n*Libraries for scientific computing.*\n\n* [astropy](http://www.astropy.org/) - A community Python library for Astronomy.\n* [bcbio-nextgen](https://github.com/chapmanb/bcbio-nextgen) - Providing best-practice pipelines for fully automated high throughput sequencing analysis.\n* [bccb](https://github.com/chapmanb/bcbb) - Collection of useful code related to biological analysis.\n* [Biopython](http://biopython.org/wiki/Main_Page) - Biopython is a set of freely available tools for biological computation.\n* [cclib](http://cclib.github.io/) - A library for parsing and interpreting the results of computational chemistry packages.\n* [Colour](http://colour-science.org/) - A colour science package implementing a comprehensive number of colour theory transformations and algorithms.\n* [NetworkX](https://networkx.github.io/) - A high-productivity software for complex networks.\n* [NIPY](http://nipy.org) - A collection of neuroimaging toolkits.\n* [NumPy](http://www.numpy.org/) - A fundamental package for scientific computing with Python.\n* [Open Babel](http://openbabel.org/wiki/Main_Page) - A chemical toolbox designed to speak the many languages of chemical data.\n* [ObsPy](https://github.com/obspy/obspy/wiki/) - A Python toolbox for seismology.\n* [PyDy](http://www.pydy.org/) - Short for Python Dynamics, used to assist with workflow in the modeling of dynamic motion.\n* [PyMC](https://github.com/pymc-devs/pymc3) - Markov Chain Monte Carlo sampling toolkit.\n* [QuTiP](http://qutip.org/) - Quantum Toolbox in Python.\n* [RDKit](http://www.rdkit.org/) - Cheminformatics and Machine Learning Software.\n* [SciPy](https://www.scipy.org/) - A Python-based ecosystem of open-source software for mathematics, science, and engineering.\n* [statsmodels](https://github.com/statsmodels/statsmodels) - Statistical modeling and econometrics in Python.\n* [SymPy](https://github.com/sympy/sympy) - A Python library for symbolic mathematics.\n* [Zipline](https://github.com/quantopian/zipline) - A Pythonic algorithmic trading library.\n* [SimPy](https://bitbucket.org/simpy/simpy) -  A process-based discrete-event simulation framework.\n\n## Search\n\n*Libraries and software for indexing and performing search queries on data.*\n\n* [django-haystack](https://github.com/django-haystack/django-haystack) - Modular search for Django.\n* [elasticsearch-dsl-py](https://github.com/elastic/elasticsearch-dsl-py) - The official high-level Python client for Elasticsearch.\n* [elasticsearch-py](https://www.elastic.co/guide/en/elasticsearch/client/python-api/current/index.html) - The official low-level Python client for [Elasticsearch](https://www.elastic.co/products/elasticsearch).\n* [esengine](https://github.com/seek-ai/esengine) - ElasticSearch ODM (Object Document Mapper) for Python.\n* [pysolr](https://github.com/django-haystack/pysolr) - A lightweight Python wrapper for Apache Solr (incl. SolrCloud awareness).\n* [solrpy](https://github.com/edsu/solrpy) - A Python client for [solr](http://lucene.apache.org/solr/).\n* [Whoosh](http://whoosh.readthedocs.io/en/latest/) - A fast, pure Python search engine library.\n\n## Serialization\n\n*Libraries for serializing complex data types*\n\n* [marshmallow](https://github.com/marshmallow-code/marshmallow) - marshmallow is an ORM/ODM/framework-agnostic library for converting complex datatypes, such as objects, to and from native Python datatypes.\n\n## Serverless Frameworks\n\n*Frameworks for developing serverless Python code.*\n\n* [apex](https://github.com/apex/apex) - Build, deploy, and manage [AWS Lambda](https://aws.amazon.com/lambda/) functions with ease.\n* [python-lambda](https://github.com/nficano/python-lambda) - A toolkit for developing and deploying Python code in AWS Lambda.\n* [Zappa](https://github.com/Miserlou/Zappa) - A tool for deploying WSGI applications on AWS Lambda and API Gateway.\n\n## Specific Formats Processing\n\n*Libraries for parsing and manipulating specific text formats.*\n\n* General\n    * [tablib](https://github.com/kennethreitz/tablib) - A module for Tabular Datasets in XLS, CSV, JSON, YAML.\n* Office\n    * [Marmir](https://github.com/brianray/mm) - Takes Python data structures and turns them into spreadsheets.\n    * [openpyxl](https://openpyxl.readthedocs.io/en/stable/) - A library for reading and writing Excel 2010 xlsx/xlsm/xltx/xltm files.\n    * [pyexcel](https://github.com/pyexcel/pyexcel) - Providing one API for reading, manipulating and writing csv, ods, xls, xlsx and xlsm files.\n    * [python-docx](https://github.com/python-openxml/python-docx) - Reads, queries and modifies Microsoft Word 2007/2008 docx files.\n    * [python-pptx](https://github.com/scanny/python-pptx) - Python library for creating and updating PowerPoint (.pptx) files.\n    * [relatorio](http://relatorio.tryton.org/) - Templating OpenDocument files.\n    * [unoconv](https://github.com/dagwieers/unoconv) - Convert between any document format supported by LibreOffice/OpenOffice.\n    * [XlsxWriter](https://xlsxwriter.readthedocs.io) - A Python module for creating Excel .xlsx files.\n    * [xlwings](https://www.xlwings.org) - A BSD-licensed library that makes it easy to call Python from Excel and vice versa.\n    * [xlwt](https://github.com/python-excel/xlwt) / [xlrd](https://github.com/python-excel/xlrd) - Writing and reading data and formatting information from Excel files.\n* PDF\n    * [PDFMiner](https://github.com/euske/pdfminer) - A tool for extracting information from PDF documents.\n    * [PyPDF2](https://github.com/mstamy2/PyPDF2) - A library capable of splitting, merging and transforming PDF pages.\n    * [ReportLab](http://www.reportlab.com/opensource/) - Allowing Rapid creation of rich PDF documents.\n* Markdown\n    * [Mistune](https://github.com/lepture/mistune) - Fastest and full featured pure Python parsers of Markdown.\n    * [Python-Markdown](https://github.com/waylan/Python-Markdown) - A Python implementation of John Gruber’s Markdown.\n* YAML\n    * [PyYAML](http://pyyaml.org/) - YAML implementations for Python.\n* CSV\n    * [csvkit](https://github.com/wireservice/csvkit) - Utilities for converting to and working with CSV.\n* Archive\n    * [unp](https://github.com/mitsuhiko/unp) - A command line tool that can unpack archives easily.\n\n## Static Site Generator\n\n*Static site generator is a software that takes some text + templates as input and produces HTML files on the output.*\n\n* [Cactus](https://github.com/eudicots/Cactus) - Static site generator for designers.\n* [Hyde](http://hyde.github.io/) - Jinja2-based static web site generator.\n* [Lektor](https://www.getlektor.com/) - An easy to use static CMS and blog engine.\n* [Nikola](https://www.getnikola.com/) - A static website and blog generator.\n* [Pelican](https://blog.getpelican.com/) - Uses Markdown or ReST for content and Jinja 2 for themes. Supports DVCS, Disqus. AGPL.\n* [Tinkerer](http://tinkerer.me/) - Tinkerer is a blogging engine/.static website generator powered by Sphinx.\n\n## Tagging\n\n*Libraries for tagging items.*\n\n* [django-taggit](https://github.com/alex/django-taggit) - Simple tagging for Django.\n\n## Template Engine\n\n*Libraries and tools for templating and lexing.*\n\n* [Genshi](https://genshi.edgewall.org/) - Python templating toolkit for generation of web-aware output.\n* [Jinja2](https://github.com/pallets/jinja) - A modern and designer friendly templating language.\n* [Mako](http://www.makotemplates.org/) - Hyperfast and lightweight templating for the Python platform.\n\n## Testing\n\n*Libraries for testing codebases and generating test data.*\n\n* Testing Frameworks\n    * [hypothesis](https://github.com/HypothesisWorks/hypothesis-python) - Hypothesis is an advanced Quickcheck style property based testing library.\n    * [mamba](http://nestorsalceda.github.io/mamba/) - The definitive testing tool for Python. Born under the banner of BDD.\n    * [nose](https://github.com/nose-devs/nose) - A nicer unittest for Python.\n    * [nose2](https://github.com/nose-devs/nose2) - The successor to nose, based on unittest2.\n    * [pytest](https://docs.pytest.org/en/latest/) - A mature full-featured Python testing tool.\n    * [Robot Framework](https://github.com/robotframework/robotframework) - A generic test automation framework.\n    * [unittest](https://docs.python.org/3/library/unittest.html) - (Python standard library) Unit testing framework.\n* Test Runners\n    * [green](https://github.com/CleanCut/green) - A clean, colorful test runner.\n    * [tox](https://tox.readthedocs.io/en/latest/) - Auto builds and tests distributions in multiple Python versions\n* GUI / Web Testing\n    * [locust](https://github.com/locustio/locust) - Scalable user load testing tool written in Python.\n    * [PyAutoGUI](https://github.com/asweigart/pyautogui) - PyAutoGUI is a cross-platform GUI automation Python module for human beings.\n    * [Selenium](https://pypi.python.org/pypi/selenium) - Python bindings for [Selenium](http://www.seleniumhq.org/) WebDriver.\n    * [sixpack](https://github.com/seatgeek/sixpack) - A language-agnostic A/B Testing framework.\n    * [splinter](https://github.com/cobrateam/splinter) - Open source tool for testing web applications.\n* Mock\n    * [doublex](https://pypi.python.org/pypi/doublex) - Powerful test doubles framework for Python.\n    * [freezegun](https://github.com/spulec/freezegun) - Travel through time by mocking the datetime module.\n    * [httmock](https://github.com/patrys/httmock) - A mocking library for requests for Python 2.6+ and 3.2+.\n    * [httpretty](https://github.com/gabrielfalcao/HTTPretty) - HTTP request mock tool for Python.\n    * [mock](https://docs.python.org/3/library/unittest.mock.html) - (Python standard library) A mocking and patching library.\n    * [Mocket](https://github.com/mindflayer/python-mocket) - Socket Mock Framework plus HTTP[S]/asyncio/gevent mocking library with recording/replaying capability.\n    * [responses](https://github.com/getsentry/responses) - A utility library for mocking out the requests Python library.\n    * [VCR.py](https://github.com/kevin1024/vcrpy) - Record and replay HTTP interactions on your tests.\n* Object Factories\n    * [factory_boy](https://github.com/FactoryBoy/factory_boy) - A test fixtures replacement for Python.\n    * [mixer](https://github.com/klen/mixer) - Another fixtures replacement. Supported Django, Flask, SQLAlchemy, Peewee and etc.\n    * [model_mommy](https://github.com/vandersonmota/model_mommy) - Creating random fixtures for testing in Django.\n* Code Coverage\n    * [coverage](https://pypi.python.org/pypi/coverage) - Code coverage measurement.\n* Fake Data\n    * [mimesis](https://github.com/lk-geimfari/mimesis) - is a Python library that help you generate fake data.\n    * [fake2db](https://github.com/emirozer/fake2db) - Fake database generator.\n    * [faker](https://github.com/joke2k/faker) - A Python package that generates fake data.\n    * [radar](https://pypi.python.org/pypi/radar) - Generate random datetime / time.\n* Error Handler\n    * [FuckIt.py](https://github.com/ajalt/fuckitpy) - FuckIt.py uses state-of-the-art technology to make sure your Python code runs whether it has any right to or not.\n\n## Text Processing\n\n*Libraries for parsing and manipulating plain texts.*\n\n* General\n    * [chardet](https://github.com/chardet/chardet) - Python 2/3 compatible character encoding detector.\n    * [difflib](https://docs.python.org/3/library/difflib.html) - (Python standard library) Helpers for computing deltas.\n    * [ftfy](https://github.com/LuminosoInsight/python-ftfy) - Makes Unicode text less broken and more consistent automagically.\n    * [fuzzywuzzy](https://github.com/seatgeek/fuzzywuzzy) - Fuzzy String Matching.\n    * [hashids](https://github.com/davidaurelio/hashids-python) - Implementation of [hashids](http://hashids.org) in Python.\n    * [Levenshtein](https://github.com/ztane/python-Levenshtein/) - Fast computation of Levenshtein distance and string similarity.\n    * [pangu.py](https://github.com/vinta/pangu.py) - Spacing texts for CJK and alphanumerics.\n    * [pyfiglet](https://github.com/pwaller/pyfiglet) - An implementation of figlet written in Python.\n    * [pypinyin](https://github.com/mozillazg/python-pinyin) - Convert Chinese hanzi to pinyin.\n    * [shortuuid](https://github.com/skorokithakis/shortuuid) - A generator library for concise, unambiguous and URL-safe UUIDs.\n    * [textdistance](https://github.com/orsinium/textdistance) - Compute distance between sequences. 30+ algorithms, pure python implementation, common interface, optional external libs usage.\n    * [unidecode](https://pypi.python.org/pypi/Unidecode) - ASCII transliterations of Unicode text.\n    * [uniout](https://github.com/moskytw/uniout) - Print readable chars instead of the escaped string.\n    * [xpinyin](https://github.com/lxneng/xpinyin) - A library to translate Chinese hanzi (漢字) to pinyin (拼音).\n* Slugify\n    * [awesome-slugify](https://github.com/dimka665/awesome-slugify) - A Python slugify library that can preserve unicode.\n    * [python-slugify](https://github.com/un33k/python-slugify) - A Python slugify library that translates unicode to ASCII.\n    * [unicode-slugify](https://github.com/mozilla/unicode-slugify) - A slugifier that generates unicode slugs with Django as a dependency.\n* Parser\n    * [phonenumbers](https://github.com/daviddrysdale/python-phonenumbers) - Parsing, formatting, storing and validating international phone numbers.\n    * [PLY](http://www.dabeaz.com/ply/) - Implementation of lex and yacc parsing tools for Python.\n    * [Pygments](http://pygments.org/) - A generic syntax highlighter.\n    * [pyparsing](https://github.com/pyparsing/pyparsing) - A general purpose framework for generating parsers.\n    * [python-nameparser](https://github.com/derek73/python-nameparser) - Parsing human names into their individual components.\n    * [python-user-agents](https://github.com/selwin/python-user-agents) - Browser user agent parser.\n    * [sqlparse](https://github.com/andialbrecht/sqlparse) - A non-validating SQL parser.\n\n## Third-party APIs\n\n*Libraries for accessing third party services APIs. See: [List of Python API Wrappers and Libraries](https://github.com/realpython/list-of-python-api-wrappers).*\n\n* [apache-libcloud](https://libcloud.apache.org/) - One Python library for all clouds.\n* [boto3](https://github.com/boto/boto3) - Python interface to Amazon Web Services.\n* [django-wordpress](https://github.com/istrategylabs/django-wordpress) - WordPress models and views for Django.\n* [facebook-sdk](https://github.com/mobolic/facebook-sdk) - Facebook Platform Python SDK.\n* [facepy](https://github.com/jgorset/facepy) - Facepy makes it really easy to interact with Facebook's Graph API\n* [gmail](https://github.com/charlierguo/gmail) - A Pythonic interface for Gmail.\n* [google-api-python-client](https://github.com/google/google-api-python-client) - Google APIs Client Library for Python.\n* [gspread](https://github.com/burnash/gspread) - Google Spreadsheets Python API.\n* [twython](https://github.com/ryanmcgrath/twython) - A Python wrapper for the Twitter API.\n\n## URL Manipulation\n\n*Libraries for parsing URLs.*\n\n* [furl](https://github.com/gruns/furl) - A small Python library that makes parsing and manipulating URLs easy.\n* [purl](https://github.com/codeinthehole/purl) - A simple, immutable URL class with a clean API for interrogation and manipulation.\n* [pyshorteners](https://github.com/ellisonleao/pyshorteners) - A pure Python URL shortening lib.\n* [short_url](https://github.com/Alir3z4/python-short_url) - Python implementation for generating Tiny URL and bit.ly-like URLs.\n* [webargs](https://github.com/sloria/webargs) - A friendly library for parsing HTTP request arguments, with built-in support for popular web frameworks, including Flask, Django, Bottle, Tornado, and Pyramid.\n\n## Video\n\n*Libraries for manipulating video and GIFs.*\n\n* [moviepy](http://zulko.github.io/moviepy/) - A module for script-based movie editing with many formats, including animated GIFs.\n* [scikit-video](https://github.com/aizvorski/scikit-video) - Video processing routines for SciPy.\n\n## WSGI Servers\n\n*WSGI-compatible web servers.*\n\n* [bjoern](https://pypi.python.org/pypi/bjoern) - Asynchronous, very fast and written in C.\n* [fapws3](http://www.fapws.org/) - Asynchronous (network side only), written in C.\n* [gunicorn](https://pypi.python.org/pypi/gunicorn) - Pre-forked, partly written in C.\n* [meinheld](https://pypi.python.org/pypi/meinheld) - Asynchronous, partly written in C.\n* [netius](https://github.com/hivesolutions/netius) - Asynchronous, very fast.\n* [rocket](https://pypi.python.org/pypi/rocket) - Multi-threaded.\n* [uWSGI](https://uwsgi-docs.readthedocs.io/en/latest/) - A project aims at developing a full stack for building hosting services, written in C.\n* [waitress](https://waitress.readthedocs.io/en/latest/) - Multi-threaded, powers Pyramid.\n* [Werkzeug](http://werkzeug.pocoo.org/) - A WSGI utility library for Python that powers Flask and can easily be embedded into your own projects.\n\n## Web Content Extracting\n\n*Libraries for extracting web contents.*\n\n* [Haul](https://github.com/vinta/Haul) - An Extensible Image Crawler.\n* [html2text](https://github.com/Alir3z4/html2text) - Convert HTML to Markdown-formatted text.\n* [lassie](https://github.com/michaelhelmick/lassie) - Web Content Retrieval for Humans.\n* [micawber](https://github.com/coleifer/micawber) - A small library for extracting rich content from URLs.\n* [newspaper](https://github.com/codelucas/newspaper) - News extraction, article extraction and content curation in Python.\n* [python-goose](https://github.com/grangier/python-goose) - HTML Content/Article Extractor.\n* [python-readability](https://github.com/buriy/python-readability) - Fast Python port of arc90's readability tool.\n* [requests-html](https://github.com/kennethreitz/requests-html) - Pythonic HTML Parsing for Humans.\n* [sanitize](https://github.com/Alir3z4/python-sanitize) - Bringing sanity to world of messed-up data.\n* [sumy](https://github.com/miso-belica/sumy) - A module for automatic summarization of text documents and HTML pages.\n* [textract](https://github.com/deanmalmgren/textract) - Extract text from any document, Word, PowerPoint, PDFs, etc.\n* [toapi](https://github.com/gaojiuli/toapi) - Every web site provides APIs.\n\n## Web Crawling \u0026 Web Scraping\n\n*Libraries to automate data extraction from websites.*\n\n* [cola](https://github.com/chineking/cola) - A distributed crawling framework.\n* [Demiurge](https://github.com/matiasb/demiurge) - PyQuery-based scraping micro-framework.\n* [feedparser](http://pythonhosted.org/feedparser/) - Universal feed parser.\n* [Grab](http://grablib.org/) - Site scraping framework.\n* [MechanicalSoup](https://github.com/hickford/MechanicalSoup) - A Python library for automating interaction with websites.\n* [portia](https://github.com/scrapinghub/portia) - Visual scraping for Scrapy.\n* [pyspider](https://github.com/binux/pyspider) - A powerful spider system.\n* [RoboBrowser](https://github.com/jmcarp/robobrowser) - A simple, Pythonic library for browsing the web without a standalone web browser.\n* [Scrapy](https://scrapy.org/) - A fast high-level screen scraping and web crawling framework.\n\n## Web Frameworks\n\n*Full stack web frameworks.*\n\n* [Django](https://www.djangoproject.com/) - The most popular web framework in Python.\n    * [awesome-django](https://github.com/rosarior/awesome-django)\n* [Flask](http://flask.pocoo.org/) - A microframework for Python.\n    * [awesome-flask](https://github.com/humiaozuzu/awesome-flask)\n* [Pyramid](https://pylonsproject.org/) - A small, fast, down-to-earth, open source Python web framework.\n    * [awesome-pyramid](https://github.com/uralbash/awesome-pyramid)\n* [Sanic](https://github.com/channelcat/sanic) - Web server that's written to go fast.\n* [Tornado](http://www.tornadoweb.org/en/latest/) - A Web framework and asynchronous networking library.\n* [Vibora](https://vibora.io/) - Fast, efficient and asynchronous Web framework inspired by Flask.\n\n## WebSocket\n\n*Libraries for working with WebSocket.*\n\n* [AutobahnPython](https://github.com/crossbario/autobahn-python) - WebSocket \u0026 WAMP for Python on Twisted and [asyncio](https://docs.python.org/3/library/asyncio.html).\n* [Crossbar](https://github.com/crossbario/crossbar/) - Open-source Unified Application Router (Websocket \u0026 WAMP for Python on Autobahn).\n* [django-channels](https://github.com/django/channels) - Developer-friendly asynchrony for Django.\n* [django-socketio](https://github.com/stephenmcd/django-socketio) - WebSockets for Django.\n* [WebSocket-for-Python](https://github.com/Lawouach/WebSocket-for-Python) - WebSocket client and server library for Python 2 and 3 as well as PyPy.\n\n# Services\n\nOnline tools and APIs to simplify development.\n\n## Continuous Integration\n\n*See: [awesome-CIandCD](https://github.com/ciandcd/awesome-ciandcd#online-build-system).*\n\n* [CircleCI](https://circleci.com/) - A CI service that can run very fast parallel testing. (GitHub only)\n* [Travis CI](https://travis-ci.org) - A popular CI service for your open source and [private](https://travis-ci.com) projects. (GitHub only)\n* [Vexor CI](https://vexor.io) - A continuous integration tool for private apps with pay-per-minute billing model.\n* [Wercker](http://www.wercker.com/) - A Docker-based platform for building and deploying applications and microservices.\n\n## Code Quality\n\n* [Codacy](https://www.codacy.com/) - Automated Code Review to ship better code, faster.\n* [Codecov](https://codecov.io/) - Code coverage dashboard.\n* [CodeFactor](https://www.codefactor.io/) - Automated Code Review for Git.\n* [Landscape](https://landscape.io/) - Hosted continuous Python code metrics.\n\n# Resources\n\nWhere to discover new Python libraries.\n\n## Podcasts\n\n* [From Python Import Podcast](http://frompythonimportpodcast.com/)\n* [Podcast.init](https://podcastinit.com/)\n* [Python Bytes](https://pythonbytes.fm)\n* [Python Testing](http://pythontesting.net)\n* [Radio Free Python](http://radiofreepython.com/)\n* [Talk Python To Me](https://talkpython.fm/)\n\n## Twitter\n\n* [@codetengu](https://twitter.com/codetengu)\n* [@getpy](https://twitter.com/getpy)\n* [@importpython](https://twitter.com/importpython)\n* [@planetpython](https://twitter.com/planetpython)\n* [@pycoders](https://twitter.com/pycoders)\n* [@pypi](https://twitter.com/pypi)\n* [@pythontrending](https://twitter.com/pythontrending)\n* [@PythonWeekly](https://twitter.com/PythonWeekly)\n* [@TalkPython](https://twitter.com/talkpython)\n* [@realpython](https://twitter.com/realpython)\n\n## Websites\n\n* [/r/CoolGithubProjects](https://www.reddit.com/r/coolgithubprojects/)\n* [/r/Python](https://www.reddit.com/r/python)\n* [Awesome Python @LibHunt](https://python.libhunt.com/)\n* [Django Packages](https://djangopackages.org/)\n* [Full Stack Python](https://www.fullstackpython.com/)\n* [PyPI Ranking](http://pypi-ranking.info/alltime)\n* [Python 3 Wall of Superpowers](http://python3wos.appspot.com/)\n* [Python Hackers](http://www.oss.io/open-source/)\n* [Python ZEEF](https://python.zeef.com/alan.richmond)\n* [Python 开发社区](https://www.ctolib.com/python/)\n* [Real Python](https://realpython.com)\n* [Trending Python repositories on GitHub today](https://github.com/trending?l=python)\n\n## Weekly\n\n* [CodeTengu Weekly 碼天狗週刊](https://weekly.codetengu.com/)\n* [Import Python Newsletter](http://importpython.com/newsletter/)\n* [Pycoder's Weekly](http://pycoders.com/)\n* [Python Weekly](http://www.pythonweekly.com/)\n* [Python Tricks](https://realpython.com/python-tricks/)\n\n# Other Awesome Lists\n\nList of lists.\n\n* Monty\n    * [awesome](https://github.com/sindresorhus/awesome)\n    * [awesomo](https://github.com/lk-geimfari/awesomo)\n    * [lists](https://github.com/jnv/lists)\n* Python\n    * [pycrumbs](https://github.com/kirang89/pycrumbs)\n    * [python-github-projects](https://github.com/checkcheckzz/python-github-projects)\n    * [python_reference](https://github.com/rasbt/python_reference)\n    * [pythonidae](https://github.com/svaksha/pythonidae)\n    * [Python Podcasts](https://www.cybrhome.com/topic/python-podcasts)\n    * [Python for Social Good](https://github.com/metakermit/awesome-python-for-social-good)\n\n# Contributing\n\nYour contributions are always welcome! Please take a look at the [contribution guidelines](https://github.com/vinta/awesome-python/blob/master/CONTRIBUTING.md) first.\n\nI will keep some pull requests open if I'm not sure whether those libraries are awesome, you could [vote for them](https://github.com/vinta/awesome-python/pulls) by adding :+1: to them. Pull requests will be merged when their votes reach **20**.\n\n- - -\n\nIf you have any question about this opinionated list, do not hesitate to contact me [@vinta](https://twitter.com/vinta) on Twitter or open an issue on GitHub.\n"
  },
  {
    "repo": "donnemartin/system-design-primer",
    "content": "*[English](README.md) ∙ [日本語](README-ja.md) ∙ [简体中文](README-zh-Hans.md) ∙ [繁體中文](README-zh-TW.md) | [Arabic](https://github.com/donnemartin/system-design-primer/issues/170) ∙ [Brazilian Portuguese](https://github.com/donnemartin/system-design-primer/issues/40) ∙ [German](https://github.com/donnemartin/system-design-primer/issues/186) ∙ [Greek](https://github.com/donnemartin/system-design-primer/issues/130) ∙ [Italian](https://github.com/donnemartin/system-design-primer/issues/104) ∙ [Korean](https://github.com/donnemartin/system-design-primer/issues/102) ∙ [Persian](https://github.com/donnemartin/system-design-primer/issues/110) ∙ [Polish](https://github.com/donnemartin/system-design-primer/issues/68) ∙ [Russian](https://github.com/donnemartin/system-design-primer/issues/87) ∙ [Spanish](https://github.com/donnemartin/system-design-primer/issues/136) ∙ [Thai](https://github.com/donnemartin/system-design-primer/issues/187) ∙ [Turkish](https://github.com/donnemartin/system-design-primer/issues/39) ∙ [Vietnamese](https://github.com/donnemartin/system-design-primer/issues/127) | [Add Translation](https://github.com/donnemartin/system-design-primer/issues/28)*\n\n# The System Design Primer\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/jj3A5N8.png\"\u003e\n  \u003cbr/\u003e\n\u003c/p\u003e\n\n## Motivation\n\n\u003e Learn how to design large-scale systems.\n\u003e\n\u003e Prep for the system design interview.\n\n### Learn how to design large-scale systems\n\nLearning how to design scalable systems will help you become a better engineer.\n\nSystem design is a broad topic.  There is a **vast amount of resources scattered throughout the web** on system design principles.\n\nThis repo is an **organized collection** of resources to help you learn how to build systems at scale.\n\n### Learn from the open source community\n\nThis is a continually updated, open source project.\n\n[Contributions](#contributing) are welcome!\n\n### Prep for the system design interview\n\nIn addition to coding interviews, system design is a **required component** of the **technical interview process** at many tech companies.\n\n**Practice common system design interview questions** and **compare** your results with **sample solutions**: discussions, code, and diagrams.\n\nAdditional topics for interview prep:\n\n* [Study guide](#study-guide)\n* [How to approach a system design interview question](#how-to-approach-a-system-design-interview-question)\n* [System design interview questions, **with solutions**](#system-design-interview-questions-with-solutions)\n* [Object-oriented design interview questions, **with solutions**](#object-oriented-design-interview-questions-with-solutions)\n* [Additional system design interview questions](#additional-system-design-interview-questions)\n\n## Anki flashcards\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/zdCAkB3.png\"\u003e\n  \u003cbr/\u003e\n\u003c/p\u003e\n\nThe provided [Anki flashcard decks](https://apps.ankiweb.net/) use spaced repetition to help you retain key system design concepts.\n\n* [System design deck](https://github.com/donnemartin/system-design-primer/tree/master/resources/flash_cards/System%20Design.apkg)\n* [System design exercises deck](https://github.com/donnemartin/system-design-primer/tree/master/resources/flash_cards/System%20Design%20Exercises.apkg)\n* [Object oriented design exercises deck](https://github.com/donnemartin/system-design-primer/tree/master/resources/flash_cards/OO%20Design.apkg)\n\nGreat for use while on-the-go.\n\n### Coding Resource: Interactive Coding Challenges\n\nLooking for resources to help you prep for the [**Coding Interview**](https://github.com/donnemartin/interactive-coding-challenges)?\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/b4YtAEN.png\"\u003e\n  \u003cbr/\u003e\n\u003c/p\u003e\n\nCheck out the sister repo [**Interactive Coding Challenges**](https://github.com/donnemartin/interactive-coding-challenges), which contains an additional Anki deck:\n\n* [Coding deck](https://github.com/donnemartin/interactive-coding-challenges/tree/master/anki_cards/Coding.apkg)\n\n## Contributing\n\n\u003e Learn from the community.\n\nFeel free to submit pull requests to help:\n\n* Fix errors\n* Improve sections\n* Add new sections\n* [Translate](https://github.com/donnemartin/system-design-primer/issues/28)\n\nContent that needs some polishing is placed [under development](#under-development).\n\nReview the [Contributing Guidelines](CONTRIBUTING.md).\n\n## Index of system design topics\n\n\u003e Summaries of various system design topics, including pros and cons.  **Everything is a trade-off**.\n\u003e\n\u003e Each section contains links to more in-depth resources.\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/jrUBAF7.png\"\u003e\n  \u003cbr/\u003e\n\u003c/p\u003e\n\n* [System design topics: start here](#system-design-topics-start-here)\n    * [Step 1: Review the scalability video lecture](#step-1-review-the-scalability-video-lecture)\n    * [Step 2: Review the scalability article](#step-2-review-the-scalability-article)\n    * [Next steps](#next-steps)\n* [Performance vs scalability](#performance-vs-scalability)\n* [Latency vs throughput](#latency-vs-throughput)\n* [Availability vs consistency](#availability-vs-consistency)\n    * [CAP theorem](#cap-theorem)\n        * [CP - consistency and partition tolerance](#cp---consistency-and-partition-tolerance)\n        * [AP - availability and partition tolerance](#ap---availability-and-partition-tolerance)\n* [Consistency patterns](#consistency-patterns)\n    * [Weak consistency](#weak-consistency)\n    * [Eventual consistency](#eventual-consistency)\n    * [Strong consistency](#strong-consistency)\n* [Availability patterns](#availability-patterns)\n    * [Fail-over](#fail-over)\n    * [Replication](#replication)\n* [Domain name system](#domain-name-system)\n* [Content delivery network](#content-delivery-network)\n    * [Push CDNs](#push-cdns)\n    * [Pull CDNs](#pull-cdns)\n* [Load balancer](#load-balancer)\n    * [Active-passive](#active-passive)\n    * [Active-active](#active-active)\n    * [Layer 4 load balancing](#layer-4-load-balancing)\n    * [Layer 7 load balancing](#layer-7-load-balancing)\n    * [Horizontal scaling](#horizontal-scaling)\n* [Reverse proxy (web server)](#reverse-proxy-web-server)\n    * [Load balancer vs reverse proxy](#load-balancer-vs-reverse-proxy)\n* [Application layer](#application-layer)\n    * [Microservices](#microservices)\n    * [Service discovery](#service-discovery)\n* [Database](#database)\n    * [Relational database management system (RDBMS)](#relational-database-management-system-rdbms)\n        * [Master-slave replication](#master-slave-replication)\n        * [Master-master replication](#master-master-replication)\n        * [Federation](#federation)\n        * [Sharding](#sharding)\n        * [Denormalization](#denormalization)\n        * [SQL tuning](#sql-tuning)\n    * [NoSQL](#nosql)\n        * [Key-value store](#key-value-store)\n        * [Document store](#document-store)\n        * [Wide column store](#wide-column-store)\n        * [Graph Database](#graph-database)\n    * [SQL or NoSQL](#sql-or-nosql)\n* [Cache](#cache)\n    * [Client caching](#client-caching)\n    * [CDN caching](#cdn-caching)\n    * [Web server caching](#web-server-caching)\n    * [Database caching](#database-caching)\n    * [Application caching](#application-caching)\n    * [Caching at the database query level](#caching-at-the-database-query-level)\n    * [Caching at the object level](#caching-at-the-object-level)\n    * [When to update the cache](#when-to-update-the-cache)\n        * [Cache-aside](#cache-aside)\n        * [Write-through](#write-through)\n        * [Write-behind (write-back)](#write-behind-write-back)\n        * [Refresh-ahead](#refresh-ahead)\n* [Asynchronism](#asynchronism)\n    * [Message queues](#message-queues)\n    * [Task queues](#task-queues)\n    * [Back pressure](#back-pressure)\n* [Communication](#communication)\n    * [Transmission control protocol (TCP)](#transmission-control-protocol-tcp)\n    * [User datagram protocol (UDP)](#user-datagram-protocol-udp)\n    * [Remote procedure call (RPC)](#remote-procedure-call-rpc)\n    * [Representational state transfer (REST)](#representational-state-transfer-rest)\n* [Security](#security)\n* [Appendix](#appendix)\n    * [Powers of two table](#powers-of-two-table)\n    * [Latency numbers every programmer should know](#latency-numbers-every-programmer-should-know)\n    * [Additional system design interview questions](#additional-system-design-interview-questions)\n    * [Real world architectures](#real-world-architectures)\n    * [Company architectures](#company-architectures)\n    * [Company engineering blogs](#company-engineering-blogs)\n* [Under development](#under-development)\n* [Credits](#credits)\n* [Contact info](#contact-info)\n* [License](#license)\n\n## Study guide\n\n\u003e Suggested topics to review based on your interview timeline (short, medium, long).\n\n![Imgur](http://i.imgur.com/OfVllex.png)\n\n**Q: For interviews, do I need to know everything here?**\n\n**A: No, you don't need to know everything here to prepare for the interview**.\n\nWhat you are asked in an interview depends on variables such as:\n\n* How much experience you have\n* What your technical background is\n* What positions you are interviewing for\n* Which companies you are interviewing with\n* Luck\n\nMore experienced candidates are generally expected to know more about system design.  Architects or team leads might be expected to know more than individual contributors.  Top tech companies are likely to have one or more design interview rounds.\n\nStart broad and go deeper in a few areas.  It helps to know a little about various key system design topics.  Adjust the following guide based on your timeline, experience, what positions you are interviewing for, and which companies you are interviewing with.\n\n* **Short timeline** - Aim for **breadth** with system design topics.  Practice by solving **some** interview questions.\n* **Medium timeline** - Aim for **breadth** and **some depth** with system design topics.  Practice by solving **many** interview questions.\n* **Long timeline** - Aim for **breadth** and **more depth** with system design topics.  Practice by solving **most** interview questions.\n\n| | Short | Medium | Long |\n|---|---|---|---|\n| Read through the [System design topics](#index-of-system-design-topics) to get a broad understanding of how systems work | :+1: | :+1: | :+1: |\n| Read through a few articles in the [Company engineering blogs](#company-engineering-blogs) for the companies you are interviewing with | :+1: | :+1: | :+1: |\n| Read through a few [Real world architectures](#real-world-architectures) | :+1: | :+1: | :+1: |\n| Review [How to approach a system design interview question](#how-to-approach-a-system-design-interview-question) | :+1: | :+1: | :+1: |\n| Work through [System design interview questions with solutions](#system-design-interview-questions-with-solutions) | Some | Many | Most |\n| Work through [Object-oriented design interview questions with solutions](#object-oriented-design-interview-questions-with-solutions) | Some | Many | Most |\n| Review [Additional system design interview questions](#additional-system-design-interview-questions) | Some | Many | Most |\n\n## How to approach a system design interview question\n\n\u003e How to tackle a system design interview question.\n\nThe system design interview is an **open-ended conversation**.  You are expected to lead it.\n\nYou can use the following steps to guide the discussion.  To help solidify this process, work through the [System design interview questions with solutions](#system-design-interview-questions-with-solutions) section using the following steps.\n\n### Step 1: Outline use cases, constraints, and assumptions\n\nGather requirements and scope the problem.  Ask questions to clarify use cases and constraints.  Discuss assumptions.\n\n* Who is going to use it?\n* How are they going to use it?\n* How many users are there?\n* What does the system do?\n* What are the inputs and outputs of the system?\n* How much data do we expect to handle?\n* How many requests per second do we expect?\n* What is the expected read to write ratio?\n\n### Step 2: Create a high level design\n\nOutline a high level design with all important components.\n\n* Sketch the main components and connections\n* Justify your ideas\n\n### Step 3: Design core components\n\nDive into details for each core component.  For example, if you were asked to [design a url shortening service](solutions/system_design/pastebin/README.md), discuss:\n\n* Generating and storing a hash of the full url\n    * [MD5](solutions/system_design/pastebin/README.md) and [Base62](solutions/system_design/pastebin/README.md)\n    * Hash collisions\n    * SQL or NoSQL\n    * Database schema\n* Translating a hashed url to the full url\n    * Database lookup\n* API and object-oriented design\n\n### Step 4: Scale the design\n\nIdentify and address bottlenecks, given the constraints.  For example, do you need the following to address scalability issues?\n\n* Load balancer\n* Horizontal scaling\n* Caching\n* Database sharding\n\nDiscuss potential solutions and trade-offs.  Everything is a trade-off.  Address bottlenecks using [principles of scalable system design](#index-of-system-design-topics).\n\n### Back-of-the-envelope calculations\n\nYou might be asked to do some estimates by hand.  Refer to the [Appendix](#appendix) for the following resources:\n\n* [Use back of the envelope calculations](http://highscalability.com/blog/2011/1/26/google-pro-tip-use-back-of-the-envelope-calculations-to-choo.html)\n* [Powers of two table](#powers-of-two-table)\n* [Latency numbers every programmer should know](#latency-numbers-every-programmer-should-know)\n\n### Source(s) and further reading\n\nCheck out the following links to get a better idea of what to expect:\n\n* [How to ace a systems design interview](https://www.palantir.com/2011/10/how-to-rock-a-systems-design-interview/)\n* [The system design interview](http://www.hiredintech.com/system-design)\n* [Intro to Architecture and Systems Design Interviews](https://www.youtube.com/watch?v=ZgdS0EUmn70)\n\n## System design interview questions with solutions\n\n\u003e Common system design interview questions with sample discussions, code, and diagrams.\n\u003e\n\u003e Solutions linked to content in the `solutions/` folder.\n\n| Question | |\n|---|---|\n| Design Pastebin.com (or Bit.ly) | [Solution](solutions/system_design/pastebin/README.md) |\n| Design the Twitter timeline and search (or Facebook feed and search) | [Solution](solutions/system_design/twitter/README.md) |\n| Design a web crawler | [Solution](solutions/system_design/web_crawler/README.md) |\n| Design Mint.com | [Solution](solutions/system_design/mint/README.md) |\n| Design the data structures for a social network | [Solution](solutions/system_design/social_graph/README.md) |\n| Design a key-value store for a search engine | [Solution](solutions/system_design/query_cache/README.md) |\n| Design Amazon's sales ranking by category feature | [Solution](solutions/system_design/sales_rank/README.md) |\n| Design a system that scales to millions of users on AWS | [Solution](solutions/system_design/scaling_aws/README.md) |\n| Add a system design question | [Contribute](#contributing) |\n\n### Design Pastebin.com (or Bit.ly)\n\n[View exercise and solution](solutions/system_design/pastebin/README.md)\n\n![Imgur](http://i.imgur.com/4edXG0T.png)\n\n### Design the Twitter timeline and search (or Facebook feed and search)\n\n[View exercise and solution](solutions/system_design/twitter/README.md)\n\n![Imgur](http://i.imgur.com/jrUBAF7.png)\n\n### Design a web crawler\n\n[View exercise and solution](solutions/system_design/web_crawler/README.md)\n\n![Imgur](http://i.imgur.com/bWxPtQA.png)\n\n### Design Mint.com\n\n[View exercise and solution](solutions/system_design/mint/README.md)\n\n![Imgur](http://i.imgur.com/V5q57vU.png)\n\n### Design the data structures for a social network\n\n[View exercise and solution](solutions/system_design/social_graph/README.md)\n\n![Imgur](http://i.imgur.com/cdCv5g7.png)\n\n### Design a key-value store for a search engine\n\n[View exercise and solution](solutions/system_design/query_cache/README.md)\n\n![Imgur](http://i.imgur.com/4j99mhe.png)\n\n### Design Amazon's sales ranking by category feature\n\n[View exercise and solution](solutions/system_design/sales_rank/README.md)\n\n![Imgur](http://i.imgur.com/MzExP06.png)\n\n### Design a system that scales to millions of users on AWS\n\n[View exercise and solution](solutions/system_design/scaling_aws/README.md)\n\n![Imgur](http://i.imgur.com/jj3A5N8.png)\n\n## Object-oriented design interview questions with solutions\n\n\u003e Common object-oriented design interview questions with sample discussions, code, and diagrams.\n\u003e\n\u003e Solutions linked to content in the `solutions/` folder.\n\n\u003e**Note: This section is under development**\n\n| Question | |\n|---|---|\n| Design a hash map | [Solution](solutions/object_oriented_design/hash_table/hash_map.ipynb)  |\n| Design a least recently used cache | [Solution](solutions/object_oriented_design/lru_cache/lru_cache.ipynb)  |\n| Design a call center | [Solution](solutions/object_oriented_design/call_center/call_center.ipynb)  |\n| Design a deck of cards | [Solution](solutions/object_oriented_design/deck_of_cards/deck_of_cards.ipynb)  |\n| Design a parking lot | [Solution](solutions/object_oriented_design/parking_lot/parking_lot.ipynb)  |\n| Design a chat server | [Solution](solutions/object_oriented_design/online_chat/online_chat.ipynb)  |\n| Design a circular array | [Contribute](#contributing)  |\n| Add an object-oriented design question | [Contribute](#contributing) |\n\n## System design topics: start here\n\nNew to system design?\n\nFirst, you'll need a basic understanding of common principles, learning about what they are, how they are used, and their pros and cons.\n\n### Step 1: Review the scalability video lecture\n\n[Scalability Lecture at Harvard](https://www.youtube.com/watch?v=-W9F__D3oY4)\n\n* Topics covered:\n    * Vertical scaling\n    * Horizontal scaling\n    * Caching\n    * Load balancing\n    * Database replication\n    * Database partitioning\n\n### Step 2: Review the scalability article\n\n[Scalability](http://www.lecloud.net/tagged/scalability/chrono)\n\n* Topics covered:\n    * [Clones](http://www.lecloud.net/post/7295452622/scalability-for-dummies-part-1-clones)\n    * [Databases](http://www.lecloud.net/post/7994751381/scalability-for-dummies-part-2-database)\n    * [Caches](http://www.lecloud.net/post/9246290032/scalability-for-dummies-part-3-cache)\n    * [Asynchronism](http://www.lecloud.net/post/9699762917/scalability-for-dummies-part-4-asynchronism)\n\n### Next steps\n\nNext, we'll look at high-level trade-offs:\n\n* **Performance** vs **scalability**\n* **Latency** vs **throughput**\n* **Availability** vs **consistency**\n\nKeep in mind that **everything is a trade-off**.\n\nThen we'll dive into more specific topics such as DNS, CDNs, and load balancers.\n\n## Performance vs scalability\n\nA service is **scalable** if it results in increased **performance** in a manner proportional to resources added. Generally, increasing performance means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow.\u003csup\u003e\u003ca href=http://www.allthingsdistributed.com/2006/03/a_word_on_scalability.html\u003e1\u003c/a\u003e\u003c/sup\u003e\n\nAnother way to look at performance vs scalability:\n\n* If you have a **performance** problem, your system is slow for a single user.\n* If you have a **scalability** problem, your system is fast for a single user but slow under heavy load.\n\n### Source(s) and further reading\n\n* [A word on scalability](http://www.allthingsdistributed.com/2006/03/a_word_on_scalability.html)\n* [Scalability, availability, stability, patterns](http://www.slideshare.net/jboner/scalability-availability-stability-patterns/)\n\n## Latency vs throughput\n\n**Latency** is the time to perform some action or to produce some result.\n\n**Throughput** is the number of such actions or results per unit of time.\n\nGenerally, you should aim for **maximal throughput** with **acceptable latency**.\n\n### Source(s) and further reading\n\n* [Understanding latency vs throughput](https://community.cadence.com/cadence_blogs_8/b/sd/archive/2010/09/13/understanding-latency-vs-throughput)\n\n## Availability vs consistency\n\n### CAP theorem\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/bgLMI2u.png\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=http://robertgreiner.com/2014/08/cap-theorem-revisited\u003eSource: CAP theorem revisited\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\nIn a distributed computer system, you can only support two of the following guarantees:\n\n* **Consistency** - Every read receives the most recent write or an error\n* **Availability** - Every request receives a response, without guarantee that it contains the most recent version of the information\n* **Partition Tolerance** - The system continues to operate despite arbitrary partitioning due to network failures\n\n*Networks aren't reliable, so you'll need to support partition tolerance.  You'll need to make a software tradeoff between consistency and availability.*\n\n#### CP - consistency and partition tolerance\n\nWaiting for a response from the partitioned node might result in a timeout error.  CP is a good choice if your business needs require atomic reads and writes.\n\n#### AP - availability and partition tolerance\n\nResponses return the most recent version of the data available on a node, which might not be the latest.  Writes might take some time to propagate when the partition is resolved.\n\nAP is a good choice if the business needs allow for [eventual consistency](#eventual-consistency) or when the system needs to continue working despite external errors.\n\n### Source(s) and further reading\n\n* [CAP theorem revisited](http://robertgreiner.com/2014/08/cap-theorem-revisited/)\n* [A plain english introduction to CAP theorem](http://ksat.me/a-plain-english-introduction-to-cap-theorem/)\n* [CAP FAQ](https://github.com/henryr/cap-faq)\n\n## Consistency patterns\n\nWith multiple copies of the same data, we are faced with options on how to synchronize them so clients have a consistent view of the data.  Recall the definition of consistency from the [CAP theorem](#cap-theorem) - Every read receives the most recent write or an error.\n\n### Weak consistency\n\nAfter a write, reads may or may not see it.  A best effort approach is taken.\n\nThis approach is seen in systems such as memcached.  Weak consistency works well in real time use cases such as VoIP, video chat, and realtime multiplayer games.  For example, if you are on a phone call and lose reception for a few seconds, when you regain connection you do not hear what was spoken during connection loss.\n\n### Eventual consistency\n\nAfter a write, reads will eventually see it (typically within milliseconds).  Data is replicated asynchronously.\n\nThis approach is seen in systems such as DNS and email.  Eventual consistency works well in highly available systems.\n\n### Strong consistency\n\nAfter a write, reads will see it.  Data is replicated synchronously.\n\nThis approach is seen in file systems and RDBMSes.  Strong consistency works well in systems that need transactions.\n\n### Source(s) and further reading\n\n* [Transactions across data centers](http://snarfed.org/transactions_across_datacenters_io.html)\n\n## Availability patterns\n\nThere are two main patterns to support high availability: **fail-over** and **replication**.\n\n### Fail-over\n\n#### Active-passive\n\nWith active-passive fail-over, heartbeats are sent between the active and the passive server on standby.  If the heartbeat is interrupted, the passive server takes over the active's IP address and resumes service.\n\nThe length of downtime is determined by whether the passive server is already running in 'hot' standby or whether it needs to start up from 'cold' standby.  Only the active server handles traffic.\n\nActive-passive failover can also be referred to as master-slave failover.\n\n#### Active-active\n\nIn active-active, both servers are managing traffic, spreading the load between them.\n\nIf the servers are public-facing, the DNS would need to know about the public IPs of both servers.  If the servers are internal-facing, application logic would need to know about both servers.\n\nActive-active failover can also be referred to as master-master failover.\n\n### Disadvantage(s): failover\n\n* Fail-over adds more hardware and additional complexity.\n* There is a potential for loss of data if the active system fails before any newly written data can be replicated to the passive.\n\n### Replication\n\n#### Master-slave and master-master\n\nThis topic is further discussed in the [Database](#database) section:\n\n* [Master-slave replication](#master-slave-replication)\n* [Master-master replication](#master-master-replication)\n\n## Domain name system\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/IOyLj4i.jpg\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=http://www.slideshare.net/srikrupa5/dns-security-presentation-issa\u003eSource: DNS security presentation\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\nA Domain Name System (DNS) translates a domain name such as www.example.com to an IP address.\n\nDNS is hierarchical, with a few authoritative servers at the top level.  Your router or ISP provides information about which DNS server(s) to contact when doing a lookup.  Lower level DNS servers cache mappings, which could become stale due to DNS propagation delays.  DNS results can also be cached by your browser or OS for a certain period of time, determined by the [time to live (TTL)](https://en.wikipedia.org/wiki/Time_to_live).\n\n* **NS record (name server)** - Specifies the DNS servers for your domain/subdomain.\n* **MX record (mail exchange)** - Specifies the mail servers for accepting messages.\n* **A record (address)** - Points a name to an IP address.\n* **CNAME (canonical)** - Points a name to another name or `CNAME` (example.com to www.example.com) or to an `A` record.\n\nServices such as [CloudFlare](https://www.cloudflare.com/dns/) and [Route 53](https://aws.amazon.com/route53/) provide managed DNS services.  Some DNS services can route traffic through various methods:\n\n* [Weighted round robin](http://g33kinfo.com/info/archives/2657)\n    * Prevent traffic from going to servers under maintenance\n    * Balance between varying cluster sizes\n    * A/B testing\n* Latency-based\n* Geolocation-based\n\n### Disadvantage(s): DNS\n\n* Accessing a DNS server introduces a slight delay, although mitigated by caching described above.\n* DNS server management could be complex and is generally managed by [governments, ISPs, and large companies](http://superuser.com/questions/472695/who-controls-the-dns-servers/472729).\n* DNS services have recently come under [DDoS attack](http://dyn.com/blog/dyn-analysis-summary-of-friday-october-21-attack/), preventing users from accessing websites such as Twitter without knowing Twitter's IP address(es).\n\n### Source(s) and further reading\n\n* [DNS architecture](https://technet.microsoft.com/en-us/library/dd197427(v=ws.10).aspx)\n* [Wikipedia](https://en.wikipedia.org/wiki/Domain_Name_System)\n* [DNS articles](https://support.dnsimple.com/categories/dns/)\n\n## Content delivery network\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/h9TAuGI.jpg\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=https://www.creative-artworks.eu/why-use-a-content-delivery-network-cdn/\u003eSource: Why use a CDN\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\nA content delivery network (CDN) is a globally distributed network of proxy servers, serving content from locations closer to the user.  Generally, static files such as HTML/CSS/JS, photos, and videos are served from CDN, although some CDNs such as Amazon's CloudFront support dynamic content.  The site's DNS resolution will tell clients which server to contact.\n\nServing content from CDNs can significantly improve performance in two ways:\n\n* Users receive content at data centers close to them\n* Your servers do not have to serve requests that the CDN fulfills\n\n### Push CDNs\n\nPush CDNs receive new content whenever changes occur on your server.  You take full responsibility for providing content, uploading directly to the CDN and rewriting URLs to point to the CDN.  You can configure when content expires and when it is updated.  Content is uploaded only when it is new or changed, minimizing traffic, but maximizing storage.\n\nSites with a small amount of traffic or sites with content that isn't often updated work well with push CDNs.  Content is placed on the CDNs once, instead of being re-pulled at regular intervals.\n\n### Pull CDNs\n\nPull CDNs grab new content from your server when the first user requests the content.  You leave the content on your server and rewrite URLs to point to the CDN.  This results in a slower request until the content is cached on the CDN.\n\nA [time-to-live (TTL)](https://en.wikipedia.org/wiki/Time_to_live) determines how long content is cached.  Pull CDNs minimize storage space on the CDN, but can create redundant traffic if files expire and are pulled before they have actually changed.\n\nSites with heavy traffic work well with pull CDNs, as traffic is spread out more evenly with only recently-requested content remaining on the CDN.\n\n### Disadvantage(s): CDN\n\n* CDN costs could be significant depending on traffic, although this should be weighed with additional costs you would incur not using a CDN.\n* Content might be stale if it is updated before the TTL expires it.\n* CDNs require changing URLs for static content to point to the CDN.\n\n### Source(s) and further reading\n\n* [Globally distributed content delivery](https://figshare.com/articles/Globally_distributed_content_delivery/6605972)\n* [The differences between push and pull CDNs](http://www.travelblogadvice.com/technical/the-differences-between-push-and-pull-cdns/)\n* [Wikipedia](https://en.wikipedia.org/wiki/Content_delivery_network)\n\n## Load balancer\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/h81n9iK.png\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=http://horicky.blogspot.com/2010/10/scalable-system-design-patterns.html\u003eSource: Scalable system design patterns\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\nLoad balancers distribute incoming client requests to computing resources such as application servers and databases.  In each case, the load balancer returns the response from the computing resource to the appropriate client.  Load balancers are effective at:\n\n* Preventing requests from going to unhealthy servers\n* Preventing overloading resources\n* Helping eliminate single points of failure\n\nLoad balancers can be implemented with hardware (expensive) or with software such as HAProxy.\n\nAdditional benefits include:\n\n* **SSL termination** - Decrypt incoming requests and encrypt server responses so backend servers do not have to perform these potentially expensive operations\n    * Removes the need to install [X.509 certificates](https://en.wikipedia.org/wiki/X.509) on each server\n* **Session persistence** - Issue cookies and route a specific client's requests to same instance if the web apps do not keep track of sessions\n\nTo protect against failures, it's common to set up multiple load balancers, either in [active-passive](#active-passive) or [active-active](#active-active) mode.\n\nLoad balancers can route traffic based on various metrics, including:\n\n* Random\n* Least loaded\n* Session/cookies\n* [Round robin or weighted round robin](http://g33kinfo.com/info/archives/2657)\n* [Layer 4](#layer-4-load-balancing)\n* [Layer 7](#layer-7-load-balancing)\n\n### Layer 4 load balancing\n\nLayer 4 load balancers look at info at the [transport layer](#communication) to decide how to distribute requests.  Generally, this involves the source, destination IP addresses, and ports in the header, but not the contents of the packet.  Layer 4 load balancers forward network packets to and from the upstream server, performing [Network Address Translation (NAT)](https://www.nginx.com/resources/glossary/layer-4-load-balancing/).\n\n### Layer 7 load balancing\n\nLayer 7 load balancers look at the [application layer](#communication) to decide how to distribute requests.  This can involve contents of the header, message, and cookies.  Layer 7 load balancers terminates network traffic, reads the message, makes a load-balancing decision, then opens a connection to the selected server.  For example, a layer 7 load balancer can direct video traffic to servers that host videos while directing more sensitive user billing traffic to security-hardened servers.\n\nAt the cost of flexibility, layer 4 load balancing requires less time and computing resources than Layer 7, although the performance impact can be minimal on modern commodity hardware.\n\n### Horizontal scaling\n\nLoad balancers can also help with horizontal scaling, improving performance and availability.  Scaling out using commodity machines is more cost efficient and results in higher availability than scaling up a single server on more expensive hardware, called **Vertical Scaling**.  It is also easier to hire for talent working on commodity hardware than it is for specialized enterprise systems.\n\n#### Disadvantage(s): horizontal scaling\n\n* Scaling horizontally introduces complexity and involves cloning servers\n    * Servers should be stateless: they should not contain any user-related data like sessions or profile pictures\n    * Sessions can be stored in a centralized data store such as a [database](#database) (SQL, NoSQL) or a persistent [cache](#cache) (Redis, Memcached)\n* Downstream servers such as caches and databases need to handle more simultaneous connections as upstream servers scale out\n\n### Disadvantage(s): load balancer\n\n* The load balancer can become a performance bottleneck if it does not have enough resources or if it is not configured properly.\n* Introducing a load balancer to help eliminate single points of failure results in increased complexity.\n* A single load balancer is a single point of failure, configuring multiple load balancers further increases complexity.\n\n### Source(s) and further reading\n\n* [NGINX architecture](https://www.nginx.com/blog/inside-nginx-how-we-designed-for-performance-scale/)\n* [HAProxy architecture guide](http://www.haproxy.org/download/1.2/doc/architecture.txt)\n* [Scalability](http://www.lecloud.net/post/7295452622/scalability-for-dummies-part-1-clones)\n* [Wikipedia](https://en.wikipedia.org/wiki/Load_balancing_(computing))\n* [Layer 4 load balancing](https://www.nginx.com/resources/glossary/layer-4-load-balancing/)\n* [Layer 7 load balancing](https://www.nginx.com/resources/glossary/layer-7-load-balancing/)\n* [ELB listener config](http://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-listener-config.html)\n\n## Reverse proxy (web server)\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/n41Azff.png\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=https://upload.wikimedia.org/wikipedia/commons/6/67/Reverse_proxy_h2g2bob.svg\u003eSource: Wikipedia\u003c/a\u003e\u003c/i\u003e\n  \u003cbr/\u003e\n\u003c/p\u003e\n\nA reverse proxy is a web server that centralizes internal services and provides unified interfaces to the public.  Requests from clients are forwarded to a server that can fulfill it before the reverse proxy returns the server's response to the client.\n\nAdditional benefits include:\n\n* **Increased security** - Hide information about backend servers, blacklist IPs, limit number of connections per client\n* **Increased scalability and flexibility** - Clients only see the reverse proxy's IP, allowing you to scale servers or change their configuration\n* **SSL termination** - Decrypt incoming requests and encrypt server responses so backend servers do not have to perform these potentially expensive operations\n    * Removes the need to install [X.509 certificates](https://en.wikipedia.org/wiki/X.509) on each server\n* **Compression** - Compress server responses\n* **Caching** - Return the response for cached requests\n* **Static content** - Serve static content directly\n    * HTML/CSS/JS\n    * Photos\n    * Videos\n    * Etc\n\n### Load balancer vs reverse proxy\n\n* Deploying a load balancer is useful when you have multiple servers.  Often, load balancers  route traffic to a set of servers serving the same function.\n* Reverse proxies can be useful even with just one web server or application server, opening up the benefits described in the previous section.\n* Solutions such as NGINX and HAProxy can support both layer 7 reverse proxying and load balancing.\n\n### Disadvantage(s): reverse proxy\n\n* Introducing a reverse proxy results in increased complexity.\n* A single reverse proxy is a single point of failure, configuring multiple reverse proxies (ie a [failover](https://en.wikipedia.org/wiki/Failover)) further increases complexity.\n\n### Source(s) and further reading\n\n* [Reverse proxy vs load balancer](https://www.nginx.com/resources/glossary/reverse-proxy-vs-load-balancer/)\n* [NGINX architecture](https://www.nginx.com/blog/inside-nginx-how-we-designed-for-performance-scale/)\n* [HAProxy architecture guide](http://www.haproxy.org/download/1.2/doc/architecture.txt)\n* [Wikipedia](https://en.wikipedia.org/wiki/Reverse_proxy)\n\n## Application layer\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/yB5SYwm.png\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=http://lethain.com/introduction-to-architecting-systems-for-scale/#platform_layer\u003eSource: Intro to architecting systems for scale\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\nSeparating out the web layer from the application layer (also known as platform layer) allows you to scale and configure both layers independently.  Adding a new API results in adding application servers without necessarily adding additional web servers.  The **single responsibility principle** advocates for small and autonomous services that work together.  Small teams with small services can plan more aggressively for rapid growth.\n\nWorkers in the application layer also help enable [asynchronism](#asynchronism).\n\n### Microservices\n\nRelated to this discussion are [microservices](https://en.wikipedia.org/wiki/Microservices), which can be described as a suite of independently deployable, small, modular services.  Each service runs a unique process and communicates through a well-defined, lightweight mechanism to serve a business goal. \u003csup\u003e\u003ca href=https://smartbear.com/learn/api-design/what-are-microservices\u003e1\u003c/a\u003e\u003c/sup\u003e\n\nPinterest, for example, could have the following microservices: user profile, follower, feed, search, photo upload, etc.\n\n### Service Discovery\n\nSystems such as [Consul](https://www.consul.io/docs/index.html), [Etcd](https://coreos.com/etcd/docs/latest), and [Zookeeper](http://www.slideshare.net/sauravhaloi/introduction-to-apache-zookeeper) can help services find each other by keeping track of registered names, addresses, and ports.  [Health checks](https://www.consul.io/intro/getting-started/checks.html) help verify service integrity and are often done using an [HTTP](#hypertext-transfer-protocol-http) endpoint.  Both Consul and Etcd have a built in [key-value store](#key-value-store) that can be useful for storing config values and other shared data.\n\n### Disadvantage(s): application layer\n\n* Adding an application layer with loosely coupled services requires a different approach from an architectural, operations, and process viewpoint (vs a monolithic system).\n* Microservices can add complexity in terms of deployments and operations.\n\n### Source(s) and further reading\n\n* [Intro to architecting systems for scale](http://lethain.com/introduction-to-architecting-systems-for-scale)\n* [Crack the system design interview](http://www.puncsky.com/blog/2016-02-13-crack-the-system-design-interview)\n* [Service oriented architecture](https://en.wikipedia.org/wiki/Service-oriented_architecture)\n* [Introduction to Zookeeper](http://www.slideshare.net/sauravhaloi/introduction-to-apache-zookeeper)\n* [Here's what you need to know about building microservices](https://cloudncode.wordpress.com/2016/07/22/msa-getting-started/)\n\n## Database\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/Xkm5CXz.png\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=https://www.youtube.com/watch?v=w95murBkYmU\u003eSource: Scaling up to your first 10 million users\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\n### Relational database management system (RDBMS)\n\nA relational database like SQL is a collection of data items organized in tables.\n\n**ACID** is a set of properties of relational database [transactions](https://en.wikipedia.org/wiki/Database_transaction).\n\n* **Atomicity** - Each transaction is all or nothing\n* **Consistency** - Any transaction will bring the database from one valid state to another\n* **Isolation** - Executing transactions concurrently has the same results as if the transactions were executed serially\n* **Durability** - Once a transaction has been committed, it will remain so\n\nThere are many techniques to scale a relational database: **master-slave replication**, **master-master replication**, **federation**, **sharding**, **denormalization**, and **SQL tuning**.\n\n#### Master-slave replication\n\nThe master serves reads and writes, replicating writes to one or more slaves, which serve only reads.  Slaves can also replicate to additional slaves in a tree-like fashion.  If the master goes offline, the system can continue to operate in read-only mode until a slave is promoted to a master or a new master is provisioned.\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/C9ioGtn.png\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=http://www.slideshare.net/jboner/scalability-availability-stability-patterns/\u003eSource: Scalability, availability, stability, patterns\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\n##### Disadvantage(s): master-slave replication\n\n* Additional logic is needed to promote a slave to a master.\n* See [Disadvantage(s): replication](#disadvantages-replication) for points related to **both** master-slave and master-master.\n\n#### Master-master replication\n\nBoth masters serve reads and writes and coordinate with each other on writes.  If either master goes down, the system can continue to operate with both reads and writes.\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/krAHLGg.png\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=http://www.slideshare.net/jboner/scalability-availability-stability-patterns/\u003eSource: Scalability, availability, stability, patterns\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\n##### Disadvantage(s): master-master replication\n\n* You'll need a load balancer or you'll need to make changes to your application logic to determine where to write.\n* Most master-master systems are either loosely consistent (violating ACID) or have increased write latency due to synchronization.\n* Conflict resolution comes more into play as more write nodes are added and as latency increases.\n* See [Disadvantage(s): replication](#disadvantages-replication) for points related to **both** master-slave and master-master.\n\n##### Disadvantage(s): replication\n\n* There is a potential for loss of data if the master fails before any newly written data can be replicated to other nodes.\n* Writes are replayed to the read replicas.  If there are a lot of writes, the read replicas can get bogged down with replaying writes and can't do as many reads.\n* The more read slaves, the more you have to replicate, which leads to greater replication lag.\n* On some systems, writing to the master can spawn multiple threads to write in parallel, whereas read replicas only support writing sequentially with a single thread.\n* Replication adds more hardware and additional complexity.\n\n##### Source(s) and further reading: replication\n\n* [Scalability, availability, stability, patterns](http://www.slideshare.net/jboner/scalability-availability-stability-patterns/)\n* [Multi-master replication](https://en.wikipedia.org/wiki/Multi-master_replication)\n\n#### Federation\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/U3qV33e.png\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=https://www.youtube.com/watch?v=w95murBkYmU\u003eSource: Scaling up to your first 10 million users\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\nFederation (or functional partitioning) splits up databases by function.  For example, instead of a single, monolithic database, you could have three databases: **forums**, **users**, and **products**, resulting in less read and write traffic to each database and therefore less replication lag.  Smaller databases result in more data that can fit in memory, which in turn results in more cache hits due to improved cache locality.  With no single central master serializing writes you can write in parallel, increasing throughput.\n\n##### Disadvantage(s): federation\n\n* Federation is not effective if your schema requires huge functions or tables.\n* You'll need to update your application logic to determine which database to read and write.\n* Joining data from two databases is more complex with a [server link](http://stackoverflow.com/questions/5145637/querying-data-by-joining-two-tables-in-two-database-on-different-servers).\n* Federation adds more hardware and additional complexity.\n\n##### Source(s) and further reading: federation\n\n* [Scaling up to your first 10 million users](https://www.youtube.com/watch?v=w95murBkYmU)\n\n#### Sharding\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/wU8x5Id.png\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=http://www.slideshare.net/jboner/scalability-availability-stability-patterns/\u003eSource: Scalability, availability, stability, patterns\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\nSharding distributes data across different databases such that each database can only manage a subset of the data.  Taking a users database as an example, as the number of users increases, more shards are added to the cluster.\n\nSimilar to the advantages of [federation](#federation), sharding results in less read and write traffic, less replication, and more cache hits.  Index size is also reduced, which generally improves performance with faster queries.  If one shard goes down, the other shards are still operational, although you'll want to add some form of replication to avoid data loss.  Like federation, there is no single central master serializing writes, allowing you to write in parallel with increased throughput.\n\nCommon ways to shard a table of users is either through the user's last name initial or the user's geographic location.\n\n##### Disadvantage(s): sharding\n\n* You'll need to update your application logic to work with shards, which could result in complex SQL queries.\n* Data distribution can become lopsided in a shard.  For example, a set of power users on a shard could result in increased load to that shard compared to others.\n    * Rebalancing adds additional complexity.  A sharding function based on [consistent hashing](http://www.paperplanes.de/2011/12/9/the-magic-of-consistent-hashing.html) can reduce the amount of transferred data.\n* Joining data from multiple shards is more complex.\n* Sharding adds more hardware and additional complexity.\n\n##### Source(s) and further reading: sharding\n\n* [The coming of the shard](http://highscalability.com/blog/2009/8/6/an-unorthodox-approach-to-database-design-the-coming-of-the.html)\n* [Shard database architecture](https://en.wikipedia.org/wiki/Shard_(database_architecture))\n* [Consistent hashing](http://www.paperplanes.de/2011/12/9/the-magic-of-consistent-hashing.html)\n\n#### Denormalization\n\nDenormalization attempts to improve read performance at the expense of some write performance.  Redundant copies of the data are written in multiple tables to avoid expensive joins.  Some RDBMS such as [PostgreSQL](https://en.wikipedia.org/wiki/PostgreSQL) and Oracle support [materialized views](https://en.wikipedia.org/wiki/Materialized_view) which handle the work of storing redundant information and keeping redundant copies consistent.\n\nOnce data becomes distributed with techniques such as [federation](#federation) and [sharding](#sharding), managing joins across data centers further increases complexity.  Denormalization might circumvent the need for such complex joins.\n\nIn most systems, reads can heavily outnumber writes 100:1 or even 1000:1.  A read resulting in a complex database join can be very expensive, spending a significant amount of time on disk operations.\n\n##### Disadvantage(s): denormalization\n\n* Data is duplicated.\n* Constraints can help redundant copies of information stay in sync, which increases complexity of the database design.\n* A denormalized database under heavy write load might perform worse than its normalized counterpart.\n\n###### Source(s) and further reading: denormalization\n\n* [Denormalization](https://en.wikipedia.org/wiki/Denormalization)\n\n#### SQL tuning\n\nSQL tuning is a broad topic and many [books](https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps\u0026field-keywords=sql+tuning) have been written as reference.\n\nIt's important to **benchmark** and **profile** to simulate and uncover bottlenecks.\n\n* **Benchmark** - Simulate high-load situations with tools such as [ab](http://httpd.apache.org/docs/2.2/programs/ab.html).\n* **Profile** - Enable tools such as the [slow query log](http://dev.mysql.com/doc/refman/5.7/en/slow-query-log.html) to help track performance issues.\n\nBenchmarking and profiling might point you to the following optimizations.\n\n##### Tighten up the schema\n\n* MySQL dumps to disk in contiguous blocks for fast access.\n* Use `CHAR` instead of `VARCHAR` for fixed-length fields.\n    * `CHAR` effectively allows for fast, random access, whereas with `VARCHAR`, you must find the end of a string before moving onto the next one.\n* Use `TEXT` for large blocks of text such as blog posts.  `TEXT` also allows for boolean searches.  Using a `TEXT` field results in storing a pointer on disk that is used to locate the text block.\n* Use `INT` for larger numbers up to 2^32 or 4 billion.\n* Use `DECIMAL` for currency to avoid floating point representation errors.\n* Avoid storing large `BLOBS`, store the location of where to get the object instead.\n* `VARCHAR(255)` is the largest number of characters that can be counted in an 8 bit number, often maximizing the use of a byte in some RDBMS.\n* Set the `NOT NULL` constraint where applicable to [improve search performance](http://stackoverflow.com/questions/1017239/how-do-null-values-affect-performance-in-a-database-search).\n\n##### Use good indices\n\n* Columns that you are querying (`SELECT`, `GROUP BY`, `ORDER BY`, `JOIN`) could be faster with indices.\n* Indices are usually represented as self-balancing [B-tree](https://en.wikipedia.org/wiki/B-tree) that keeps data sorted and allows searches, sequential access, insertions, and deletions in logarithmic time.\n* Placing an index can keep the data in memory, requiring more space.\n* Writes could also be slower since the index also needs to be updated.\n* When loading large amounts of data, it might be faster to disable indices, load the data, then rebuild the indices.\n\n##### Avoid expensive joins\n\n* [Denormalize](#denormalization) where performance demands it.\n\n##### Partition tables\n\n* Break up a table by putting hot spots in a separate table to help keep it in memory.\n\n##### Tune the query cache\n\n* In some cases, the [query cache](https://dev.mysql.com/doc/refman/5.7/en/query-cache.html) could lead to [performance issues](https://www.percona.com/blog/2016/10/12/mysql-5-7-performance-tuning-immediately-after-installation/).\n\n##### Source(s) and further reading: SQL tuning\n\n* [Tips for optimizing MySQL queries](http://aiddroid.com/10-tips-optimizing-mysql-queries-dont-suck/)\n* [Is there a good reason i see VARCHAR(255) used so often?](http://stackoverflow.com/questions/1217466/is-there-a-good-reason-i-see-varchar255-used-so-often-as-opposed-to-another-l)\n* [How do null values affect performance?](http://stackoverflow.com/questions/1017239/how-do-null-values-affect-performance-in-a-database-search)\n* [Slow query log](http://dev.mysql.com/doc/refman/5.7/en/slow-query-log.html)\n\n### NoSQL\n\nNoSQL is a collection of data items represented in a **key-value store**, **document-store**, **wide column store**, or a **graph database**.  Data is denormalized, and joins are generally done in the application code.  Most NoSQL stores lack true ACID transactions and favor [eventual consistency](#eventual-consistency).\n\n**BASE** is often used to describe the properties of NoSQL databases.  In comparison with the [CAP Theorem](#cap-theorem), BASE chooses availability over consistency.\n\n* **Basically available** - the system guarantees availability.\n* **Soft state** - the state of the system may change over time, even without input.\n* **Eventual consistency** - the system will become consistent over a period of time, given that the system doesn't receive input during that period.\n\nIn addition to choosing between [SQL or NoSQL](#sql-or-nosql), it is helpful to understand which type of NoSQL database best fits your use case(s).  We'll review **key-value stores**, **document-stores**, **wide column stores**, and **graph databases** in the next section.\n\n#### Key-value store\n\n\u003e Abstraction: hash table\n\nA key-value store generally allows for O(1) reads and writes and is often backed by memory or SSD.  Data stores can maintain keys in [lexicographic order](https://en.wikipedia.org/wiki/Lexicographical_order), allowing efficient retrieval of key ranges.  Key-value stores can allow for storing of metadata with a value.\n\nKey-value stores provide high performance and are often used for simple data models or for rapidly-changing data, such as an in-memory cache layer.  Since they offer only a limited set of operations, complexity is shifted to the application layer if additional operations are needed.\n\nA key-value store is the basis for more complex systems such as a document store, and in some cases, a graph database.\n\n##### Source(s) and further reading: key-value store\n\n* [Key-value database](https://en.wikipedia.org/wiki/Key-value_database)\n* [Disadvantages of key-value stores](http://stackoverflow.com/questions/4056093/what-are-the-disadvantages-of-using-a-key-value-table-over-nullable-columns-or)\n* [Redis architecture](http://qnimate.com/overview-of-redis-architecture/)\n* [Memcached architecture](https://www.adayinthelifeof.nl/2011/02/06/memcache-internals/)\n\n#### Document store\n\n\u003e Abstraction: key-value store with documents stored as values\n\nA document store is centered around documents (XML, JSON, binary, etc), where a document stores all information for a given object.  Document stores provide APIs or a query language to query based on the internal structure of the document itself.  *Note, many key-value stores include features for working with a value's metadata, blurring the lines between these two storage types.*\n\nBased on the underlying implementation, documents are organized in either collections, tags, metadata, or directories.  Although documents can be organized or grouped together, documents may have fields that are completely different from each other.\n\nSome document stores like [MongoDB](https://www.mongodb.com/mongodb-architecture) and [CouchDB](https://blog.couchdb.org/2016/08/01/couchdb-2-0-architecture/) also provide a SQL-like language to perform complex queries.  [DynamoDB](http://www.read.seas.harvard.edu/~kohler/class/cs239-w08/decandia07dynamo.pdf) supports both key-values and documents.\n\nDocument stores provide high flexibility and are often used for working with occasionally changing data.\n\n##### Source(s) and further reading: document store\n\n* [Document-oriented database](https://en.wikipedia.org/wiki/Document-oriented_database)\n* [MongoDB architecture](https://www.mongodb.com/mongodb-architecture)\n* [CouchDB architecture](https://blog.couchdb.org/2016/08/01/couchdb-2-0-architecture/)\n* [Elasticsearch architecture](https://www.elastic.co/blog/found-elasticsearch-from-the-bottom-up)\n\n#### Wide column store\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/n16iOGk.png\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=http://blog.grio.com/2015/11/sql-nosql-a-brief-history.html\u003eSource: SQL \u0026 NoSQL, a brief history\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\n\u003e Abstraction: nested map `ColumnFamily\u003cRowKey, Columns\u003cColKey, Value, Timestamp\u003e\u003e`\n\nA wide column store's basic unit of data is a column (name/value pair).  A column can be grouped in column families (analogous to a SQL table).  Super column families further group column families.  You can access each column independently with a row key, and columns with the same row key form a row.  Each value contains a timestamp for versioning and for conflict resolution.\n\nGoogle introduced [Bigtable](http://www.read.seas.harvard.edu/~kohler/class/cs239-w08/chang06bigtable.pdf) as the first wide column store, which influenced the open-source [HBase](https://www.mapr.com/blog/in-depth-look-hbase-architecture) often-used in the Hadoop ecosystem, and [Cassandra](http://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archIntro.html) from Facebook.  Stores such as BigTable, HBase, and Cassandra maintain keys in lexicographic order, allowing efficient retrieval of selective key ranges.\n\nWide column stores offer high availability and high scalability.  They are often used for very large data sets.\n\n##### Source(s) and further reading: wide column store\n\n* [SQL \u0026 NoSQL, a brief history](http://blog.grio.com/2015/11/sql-nosql-a-brief-history.html)\n* [Bigtable architecture](http://www.read.seas.harvard.edu/~kohler/class/cs239-w08/chang06bigtable.pdf)\n* [HBase architecture](https://www.mapr.com/blog/in-depth-look-hbase-architecture)\n* [Cassandra architecture](http://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archIntro.html)\n\n#### Graph database\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/fNcl65g.png\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=https://en.wikipedia.org/wiki/File:GraphDatabase_PropertyGraph.png\u003eSource: Graph database\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\n\u003e Abstraction: graph\n\nIn a graph database, each node is a record and each arc is a relationship between two nodes.  Graph databases are optimized to represent complex relationships with many foreign keys or many-to-many relationships.\n\nGraphs databases offer high performance for data models with complex relationships, such as a social network.  They are relatively new and are not yet widely-used; it might be more difficult to find development tools and resources.  Many graphs can only be accessed with [REST APIs](#representational-state-transfer-rest).\n\n##### Source(s) and further reading: graph\n\n* [Graph database](https://en.wikipedia.org/wiki/Graph_database)\n* [Neo4j](https://neo4j.com/)\n* [FlockDB](https://blog.twitter.com/2010/introducing-flockdb)\n\n#### Source(s) and further reading: NoSQL\n\n* [Explanation of base terminology](http://stackoverflow.com/questions/3342497/explanation-of-base-terminology)\n* [NoSQL databases a survey and decision guidance](https://medium.com/baqend-blog/nosql-databases-a-survey-and-decision-guidance-ea7823a822d#.wskogqenq)\n* [Scalability](http://www.lecloud.net/post/7994751381/scalability-for-dummies-part-2-database)\n* [Introduction to NoSQL](https://www.youtube.com/watch?v=qI_g07C_Q5I)\n* [NoSQL patterns](http://horicky.blogspot.com/2009/11/nosql-patterns.html)\n\n### SQL or NoSQL\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/wXGqG5f.png\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=https://www.infoq.com/articles/Transition-RDBMS-NoSQL/\u003eSource: Transitioning from RDBMS to NoSQL\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\nReasons for **SQL**:\n\n* Structured data\n* Strict schema\n* Relational data\n* Need for complex joins\n* Transactions\n* Clear patterns for scaling\n* More established: developers, community, code, tools, etc\n* Lookups by index are very fast\n\nReasons for **NoSQL**:\n\n* Semi-structured data\n* Dynamic or flexible schema\n* Non-relational data\n* No need for complex joins\n* Store many TB (or PB) of data\n* Very data intensive workload\n* Very high throughput for IOPS\n\nSample data well-suited for NoSQL:\n\n* Rapid ingest of clickstream and log data\n* Leaderboard or scoring data\n* Temporary data, such as a shopping cart\n* Frequently accessed ('hot') tables\n* Metadata/lookup tables\n\n##### Source(s) and further reading: SQL or NoSQL\n\n* [Scaling up to your first 10 million users](https://www.youtube.com/watch?v=w95murBkYmU)\n* [SQL vs NoSQL differences](https://www.sitepoint.com/sql-vs-nosql-differences/)\n\n## Cache\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/Q6z24La.png\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=http://horicky.blogspot.com/2010/10/scalable-system-design-patterns.html\u003eSource: Scalable system design patterns\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\nCaching improves page load times and can reduce the load on your servers and databases.  In this model, the dispatcher will first lookup if the request has been made before and try to find the previous result to return, in order to save the actual execution.\n\nDatabases often benefit from a uniform distribution of reads and writes across its partitions.  Popular items can skew the distribution, causing bottlenecks.  Putting a cache in front of a database can help absorb uneven loads and spikes in traffic.\n\n### Client caching\n\nCaches can be located on the client side (OS or browser), [server side](#reverse-proxy-web-server), or in a distinct cache layer.\n\n### CDN caching\n\n[CDNs](#content-delivery-network) are considered a type of cache.\n\n### Web server caching\n\n[Reverse proxies](#reverse-proxy-web-server) and caches such as [Varnish](https://www.varnish-cache.org/) can serve static and dynamic content directly.  Web servers can also cache requests, returning responses without having to contact application servers.\n\n### Database caching\n\nYour database usually includes some level of caching in a default configuration, optimized for a generic use case.  Tweaking these settings for specific usage patterns can further boost performance.\n\n### Application caching\n\nIn-memory caches such as Memcached and Redis are key-value stores between your application and your data storage.  Since the data is held in RAM, it is much faster than typical databases where data is stored on disk.  RAM is more limited than disk, so [cache invalidation](https://en.wikipedia.org/wiki/Cache_algorithms) algorithms such as [least recently used (LRU)](https://en.wikipedia.org/wiki/Cache_algorithms#Least_Recently_Used) can help invalidate 'cold' entries and keep 'hot' data in RAM.\n\nRedis has the following additional features:\n\n* Persistence option\n* Built-in data structures such as sorted sets and lists\n\nThere are multiple levels you can cache that fall into two general categories: **database queries** and **objects**:\n\n* Row level\n* Query-level\n* Fully-formed serializable objects\n* Fully-rendered HTML\n\nGenerally, you should try to avoid file-based caching, as it makes cloning and auto-scaling more difficult.\n\n### Caching at the database query level\n\nWhenever you query the database, hash the query as a key and store the result to the cache.  This approach suffers from expiration issues:\n\n* Hard to delete a cached result with complex queries\n* If one piece of data changes such as a table cell, you need to delete all cached queries that might include the changed cell\n\n### Caching at the object level\n\nSee your data as an object, similar to what you do with your application code.  Have your application assemble the dataset from the database into a class instance or a data structure(s):\n\n* Remove the object from cache if its underlying data has changed\n* Allows for asynchronous processing: workers assemble objects by consuming the latest cached object\n\nSuggestions of what to cache:\n\n* User sessions\n* Fully rendered web pages\n* Activity streams\n* User graph data\n\n### When to update the cache\n\nSince you can only store a limited amount of data in cache, you'll need to determine which cache update strategy works best for your use case.\n\n#### Cache-aside\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/ONjORqk.png\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=http://www.slideshare.net/tmatyashovsky/from-cache-to-in-memory-data-grid-introduction-to-hazelcast\u003eSource: From cache to in-memory data grid\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\nThe application is responsible for reading and writing from storage.  The cache does not interact with storage directly.  The application does the following:\n\n* Look for entry in cache, resulting in a cache miss\n* Load entry from the database\n* Add entry to cache\n* Return entry\n\n```\ndef get_user(self, user_id):\n    user = cache.get(\"user.{0}\", user_id)\n    if user is None:\n        user = db.query(\"SELECT * FROM users WHERE user_id = {0}\", user_id)\n        if user is not None:\n            key = \"user.{0}\".format(user_id)\n            cache.set(key, json.dumps(user))\n    return user\n```\n\n[Memcached](https://memcached.org/) is generally used in this manner.\n\nSubsequent reads of data added to cache are fast.  Cache-aside is also referred to as lazy loading.  Only requested data is cached, which avoids filling up the cache with data that isn't requested.\n\n##### Disadvantage(s): cache-aside\n\n* Each cache miss results in three trips, which can cause a noticeable delay.\n* Data can become stale if it is updated in the database.  This issue is mitigated by setting a time-to-live (TTL) which forces an update of the cache entry, or by using write-through.\n* When a node fails, it is replaced by a new, empty node, increasing latency.\n\n#### Write-through\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/0vBc0hN.png\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=http://www.slideshare.net/jboner/scalability-availability-stability-patterns/\u003eSource: Scalability, availability, stability, patterns\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\nThe application uses the cache as the main data store, reading and writing data to it, while the cache is responsible for reading and writing to the database:\n\n* Application adds/updates entry in cache\n* Cache synchronously writes entry to data store\n* Return\n\nApplication code:\n\n```\nset_user(12345, {\"foo\":\"bar\"})\n```\n\nCache code:\n\n```\ndef set_user(user_id, values):\n    user = db.query(\"UPDATE Users WHERE id = {0}\", user_id, values)\n    cache.set(user_id, user)\n```\n\nWrite-through is a slow overall operation due to the write operation, but subsequent reads of just written data are fast.  Users are generally more tolerant of latency when updating data than reading data.  Data in the cache is not stale.\n\n##### Disadvantage(s): write through\n\n* When a new node is created due to failure or scaling, the new node will not cache entries until the entry is updated in the database.  Cache-aside in conjunction with write through can mitigate this issue.\n* Most data written might never read, which can be minimized with a TTL.\n\n#### Write-behind (write-back)\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/rgSrvjG.png\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=http://www.slideshare.net/jboner/scalability-availability-stability-patterns/\u003eSource: Scalability, availability, stability, patterns\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\nIn write-behind, the application does the following:\n\n* Add/update entry in cache\n* Asynchronously write entry to the data store, improving write performance\n\n##### Disadvantage(s): write-behind\n\n* There could be data loss if the cache goes down prior to its contents hitting the data store.\n* It is more complex to implement write-behind than it is to implement cache-aside or write-through.\n\n#### Refresh-ahead\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/kxtjqgE.png\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=http://www.slideshare.net/tmatyashovsky/from-cache-to-in-memory-data-grid-introduction-to-hazelcast\u003eSource: From cache to in-memory data grid\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\nYou can configure the cache to automatically refresh any recently accessed cache entry prior to its expiration.\n\nRefresh-ahead can result in reduced latency vs read-through if the cache can accurately predict which items are likely to be needed in the future.\n\n##### Disadvantage(s): refresh-ahead\n\n* Not accurately predicting which items are likely to be needed in the future can result in reduced performance than without refresh-ahead.\n\n### Disadvantage(s): cache\n\n* Need to maintain consistency between caches and the source of truth such as the database through [cache invalidation](https://en.wikipedia.org/wiki/Cache_algorithms).\n* Cache invalidation is a difficult problem, there is additional complexity associated with when to update the cache.\n* Need to make application changes such as adding Redis or memcached.\n\n### Source(s) and further reading\n\n* [From cache to in-memory data grid](http://www.slideshare.net/tmatyashovsky/from-cache-to-in-memory-data-grid-introduction-to-hazelcast)\n* [Scalable system design patterns](http://horicky.blogspot.com/2010/10/scalable-system-design-patterns.html)\n* [Introduction to architecting systems for scale](http://lethain.com/introduction-to-architecting-systems-for-scale/)\n* [Scalability, availability, stability, patterns](http://www.slideshare.net/jboner/scalability-availability-stability-patterns/)\n* [Scalability](http://www.lecloud.net/post/9246290032/scalability-for-dummies-part-3-cache)\n* [AWS ElastiCache strategies](http://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/Strategies.html)\n* [Wikipedia](https://en.wikipedia.org/wiki/Cache_(computing))\n\n## Asynchronism\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/54GYsSx.png\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=http://lethain.com/introduction-to-architecting-systems-for-scale/#platform_layer\u003eSource: Intro to architecting systems for scale\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\nAsynchronous workflows help reduce request times for expensive operations that would otherwise be performed in-line.  They can also help by doing time-consuming work in advance, such as periodic aggregation of data.\n\n### Message queues\n\nMessage queues receive, hold, and deliver messages.  If an operation is too slow to perform inline, you can use a message queue with the following workflow:\n\n* An application publishes a job to the queue, then notifies the user of job status\n* A worker picks up the job from the queue, processes it, then signals the job is complete\n\nThe user is not blocked and the job is processed in the background.  During this time, the client might optionally do a small amount of processing to make it seem like the task has completed.  For example, if posting a tweet, the tweet could be instantly posted to your timeline, but it could take some time before your tweet is actually delivered to all of your followers.\n\n**[Redis](https://redis.io/)** is useful as a simple message broker but messages can be lost.\n\n**[RabbitMQ](https://www.rabbitmq.com/)** is popular but requires you to adapt to the 'AMQP' protocol and manage your own nodes.\n\n**[Amazon SQS](https://aws.amazon.com/sqs/)** is hosted but can have high latency and has the possibility of messages being delivered twice.\n\n### Task queues\n\nTasks queues receive tasks and their related data, runs them, then delivers their results.  They can support scheduling and can be used to run computationally-intensive jobs in the background.\n\n**Celery** has support for scheduling and primarily has python support.\n\n### Back pressure\n\nIf queues start to grow significantly, the queue size can become larger than memory, resulting in cache misses, disk reads, and even slower performance.  [Back pressure](http://mechanical-sympathy.blogspot.com/2012/05/apply-back-pressure-when-overloaded.html) can help by limiting the queue size, thereby maintaining a high throughput rate and good response times for jobs already in the queue.  Once the queue fills up, clients get a server busy or HTTP 503 status code to try again later.  Clients can retry the request at a later time, perhaps with [exponential backoff](https://en.wikipedia.org/wiki/Exponential_backoff).\n\n### Disadvantage(s): asynchronism\n\n* Use cases such as inexpensive calculations and realtime workflows might be better suited for synchronous operations, as introducing queues can add delays and complexity.\n\n### Source(s) and further reading\n\n* [It's all a numbers game](https://www.youtube.com/watch?v=1KRYH75wgy4)\n* [Applying back pressure when overloaded](http://mechanical-sympathy.blogspot.com/2012/05/apply-back-pressure-when-overloaded.html)\n* [Little's law](https://en.wikipedia.org/wiki/Little%27s_law)\n* [What is the difference between a message queue and a task queue?](https://www.quora.com/What-is-the-difference-between-a-message-queue-and-a-task-queue-Why-would-a-task-queue-require-a-message-broker-like-RabbitMQ-Redis-Celery-or-IronMQ-to-function)\n\n## Communication\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/5KeocQs.jpg\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=http://www.escotal.com/osilayer.html\u003eSource: OSI 7 layer model\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\n### Hypertext transfer protocol (HTTP)\n\nHTTP is a method for encoding and transporting data between a client and a server.  It is a request/response protocol: clients issue requests and servers issue responses with relevant content and completion status info about the request.  HTTP is self-contained, allowing requests and responses to flow through many intermediate routers and servers that perform load balancing, caching, encryption, and compression.\n\nA basic HTTP request consists of a verb (method) and a resource (endpoint).  Below are common HTTP verbs:\n\n| Verb | Description | Idempotent* | Safe | Cacheable |\n|---|---|---|---|---|\n| GET | Reads a resource | Yes | Yes | Yes |\n| POST | Creates a resource or trigger a process that handles data | No | No | Yes if response contains freshness info |\n| PUT | Creates or replace a resource | Yes | No | No |\n| PATCH | Partially updates a resource | No | No | Yes if response contains freshness info |\n| DELETE | Deletes a resource | Yes | No | No |\n\n*Can be called many times without different outcomes.\n\nHTTP is an application layer protocol relying on lower-level protocols such as **TCP** and **UDP**.\n\n#### Source(s) and further reading: HTTP\n\n* [What is HTTP?](https://www.nginx.com/resources/glossary/http/)\n* [Difference between HTTP and TCP](https://www.quora.com/What-is-the-difference-between-HTTP-protocol-and-TCP-protocol)\n* [Difference between PUT and PATCH](https://laracasts.com/discuss/channels/general-discussion/whats-the-differences-between-put-and-patch?page=1)\n\n### Transmission control protocol (TCP)\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/JdAsdvG.jpg\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=http://www.wildbunny.co.uk/blog/2012/10/09/how-to-make-a-multi-player-game-part-1/\u003eSource: How to make a multiplayer game\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\nTCP is a connection-oriented protocol over an [IP network](https://en.wikipedia.org/wiki/Internet_Protocol).  Connection is established and terminated using a [handshake](https://en.wikipedia.org/wiki/Handshaking).  All packets sent are guaranteed to reach the destination in the original order and without corruption through:\n\n* Sequence numbers and [checksum fields](https://en.wikipedia.org/wiki/Transmission_Control_Protocol#Checksum_computation) for each packet\n* [Acknowledgement](https://en.wikipedia.org/wiki/Acknowledgement_(data_networks)) packets and automatic retransmission\n\nIf the sender does not receive a correct response, it will resend the packets.  If there are multiple timeouts, the connection is dropped.  TCP also implements [flow control](https://en.wikipedia.org/wiki/Flow_control_(data)) and [congestion control](https://en.wikipedia.org/wiki/Network_congestion#Congestion_control).  These guarantees cause delays and generally result in less efficient transmission than UDP.\n\nTo ensure high throughput, web servers can keep a large number of TCP connections open, resulting in high memory usage.  It can be expensive to have a large number of open connections between web server threads and say, a [memcached](https://memcached.org/) server.  [Connection pooling](https://en.wikipedia.org/wiki/Connection_pool) can help in addition to switching to UDP where applicable.\n\nTCP is useful for applications that require high reliability but are less time critical.  Some examples include web servers, database info, SMTP, FTP, and SSH.\n\nUse TCP over UDP when:\n\n* You need all of the data to arrive intact\n* You want to automatically make a best estimate use of the network throughput\n\n### User datagram protocol (UDP)\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/yzDrJtA.jpg\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=http://www.wildbunny.co.uk/blog/2012/10/09/how-to-make-a-multi-player-game-part-1/\u003eSource: How to make a multiplayer game\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\nUDP is connectionless.  Datagrams (analogous to packets) are guaranteed only at the datagram level.  Datagrams might reach their destination out of order or not at all.  UDP does not support congestion control.  Without the guarantees that TCP support, UDP is generally more efficient.\n\nUDP can broadcast, sending datagrams to all devices on the subnet.  This is useful with [DHCP](https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol) because the client has not yet received an IP address, thus preventing a way for TCP to stream without the IP address.\n\nUDP is less reliable but works well in real time use cases such as VoIP, video chat, streaming, and realtime multiplayer games.\n\nUse UDP over TCP when:\n\n* You need the lowest latency\n* Late data is worse than loss of data\n* You want to implement your own error correction\n\n#### Source(s) and further reading: TCP and UDP\n\n* [Networking for game programming](http://gafferongames.com/networking-for-game-programmers/udp-vs-tcp/)\n* [Key differences between TCP and UDP protocols](http://www.cyberciti.biz/faq/key-differences-between-tcp-and-udp-protocols/)\n* [Difference between TCP and UDP](http://stackoverflow.com/questions/5970383/difference-between-tcp-and-udp)\n* [Transmission control protocol](https://en.wikipedia.org/wiki/Transmission_Control_Protocol)\n* [User datagram protocol](https://en.wikipedia.org/wiki/User_Datagram_Protocol)\n* [Scaling memcache at Facebook](http://www.cs.bu.edu/~jappavoo/jappavoo.github.com/451/papers/memcache-fb.pdf)\n\n### Remote procedure call (RPC)\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/iF4Mkb5.png\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=http://www.puncsky.com/blog/2016-02-13-crack-the-system-design-interview\u003eSource: Crack the system design interview\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\nIn an RPC, a client causes a procedure to execute on a different address space, usually a remote server.  The procedure is coded as if it were a local procedure call, abstracting away the details of how to communicate with the server from the client program.  Remote calls are usually slower and less reliable than local calls so it is helpful to distinguish RPC calls from local calls.  Popular RPC frameworks include [Protobuf](https://developers.google.com/protocol-buffers/), [Thrift](https://thrift.apache.org/), and [Avro](https://avro.apache.org/docs/current/).\n\nRPC is a request-response protocol:\n\n* **Client program** - Calls the client stub procedure.  The parameters are pushed onto the stack like a local procedure call.\n* **Client stub procedure** - Marshals (packs) procedure id and arguments into a request message.\n* **Client communication module** - OS sends the message from the client to the server.\n* **Server communication module** - OS passes the incoming packets to the server stub procedure.\n* **Server stub procedure** -  Unmarshalls the results, calls the server procedure matching the procedure id and passes the given arguments.\n* The server response repeats the steps above in reverse order.\n\nSample RPC calls:\n\n```\nGET /someoperation?data=anId\n\nPOST /anotheroperation\n{\n  \"data\":\"anId\";\n  \"anotherdata\": \"another value\"\n}\n```\n\nRPC is focused on exposing behaviors.  RPCs are often used for performance reasons with internal communications, as you can hand-craft native calls to better fit your use cases.\n\nChoose a native library (aka SDK) when:\n\n* You know your target platform.\n* You want to control how your \"logic\" is accessed.\n* You want to control how error control happens off your library.\n* Performance and end user experience is your primary concern.\n\nHTTP APIs following **REST** tend to be used more often for public APIs.\n\n#### Disadvantage(s): RPC\n\n* RPC clients become tightly coupled to the service implementation.\n* A new API must be defined for every new operation or use case.\n* It can be difficult to debug RPC.\n* You might not be able to leverage existing technologies out of the box.  For example, it might require additional effort to ensure [RPC calls are properly cached](http://etherealbits.com/2012/12/debunking-the-myths-of-rpc-rest/) on caching servers such as [Squid](http://www.squid-cache.org/).\n\n### Representational state transfer (REST)\n\nREST is an architectural style enforcing a client/server model where the client acts on a set of resources managed by the server.  The server provides a representation of resources and actions that can either manipulate or get a new representation of resources.  All communication must be stateless and cacheable.\n\nThere are four qualities of a RESTful interface:\n\n* **Identify resources (URI in HTTP)** - use the same URI regardless of any operation.\n* **Change with representations (Verbs in HTTP)** - use verbs, headers, and body.\n* **Self-descriptive error message (status response in HTTP)** - Use status codes, don't reinvent the wheel.\n* **[HATEOAS](http://restcookbook.com/Basics/hateoas/) (HTML interface for HTTP)** - your web service should be fully accessible in a browser.\n\nSample REST calls:\n\n```\nGET /someresources/anId\n\nPUT /someresources/anId\n{\"anotherdata\": \"another value\"}\n```\n\nREST is focused on exposing data.  It minimizes the coupling between client/server and is often used for public HTTP APIs.  REST uses a more generic and uniform method of exposing resources through URIs, [representation through headers](https://github.com/for-GET/know-your-http-well/blob/master/headers.md), and actions through verbs such as GET, POST, PUT, DELETE, and PATCH.  Being stateless, REST is great for horizontal scaling and partitioning.\n\n#### Disadvantage(s): REST\n\n* With REST being focused on exposing data, it might not be a good fit if resources are not naturally organized or accessed in a simple hierarchy.  For example, returning all updated records from the past hour matching a particular set of events is not easily expressed as a path.  With REST, it is likely to be implemented with a combination of URI path, query parameters, and possibly the request body.\n* REST typically relies on a few verbs (GET, POST, PUT, DELETE, and PATCH) which sometimes doesn't fit your use case.  For example, moving expired documents to the archive folder might not cleanly fit within these verbs.\n* Fetching complicated resources with nested hierarchies requires multiple round trips between the client and server to render single views, e.g. fetching content of a blog entry and the comments on that entry. For mobile applications operating in variable network conditions, these multiple roundtrips are highly undesirable.\n* Over time, more fields might be added to an API response and older clients will receive all new data fields, even those that they do not need, as a result, it bloats the payload size and leads to larger latencies.\n\n### RPC and REST calls comparison\n\n| Operation | RPC | REST |\n|---|---|---|\n| Signup    | **POST** /signup | **POST** /persons |\n| Resign    | **POST** /resign\u003cbr/\u003e{\u003cbr/\u003e\"personid\": \"1234\"\u003cbr/\u003e} | **DELETE** /persons/1234 |\n| Read a person | **GET** /readPerson?personid=1234 | **GET** /persons/1234 |\n| Read a person’s items list | **GET** /readUsersItemsList?personid=1234 | **GET** /persons/1234/items |\n| Add an item to a person’s items | **POST** /addItemToUsersItemsList\u003cbr/\u003e{\u003cbr/\u003e\"personid\": \"1234\";\u003cbr/\u003e\"itemid\": \"456\"\u003cbr/\u003e} | **POST** /persons/1234/items\u003cbr/\u003e{\u003cbr/\u003e\"itemid\": \"456\"\u003cbr/\u003e} |\n| Update an item    | **POST** /modifyItem\u003cbr/\u003e{\u003cbr/\u003e\"itemid\": \"456\";\u003cbr/\u003e\"key\": \"value\"\u003cbr/\u003e} | **PUT** /items/456\u003cbr/\u003e{\u003cbr/\u003e\"key\": \"value\"\u003cbr/\u003e} |\n| Delete an item | **POST** /removeItem\u003cbr/\u003e{\u003cbr/\u003e\"itemid\": \"456\"\u003cbr/\u003e} | **DELETE** /items/456 |\n\n\u003cp align=\"center\"\u003e\n  \u003ci\u003e\u003ca href=https://apihandyman.io/do-you-really-know-why-you-prefer-rest-over-rpc/\u003eSource: Do you really know why you prefer REST over RPC\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\n#### Source(s) and further reading: REST and RPC\n\n* [Do you really know why you prefer REST over RPC](https://apihandyman.io/do-you-really-know-why-you-prefer-rest-over-rpc/)\n* [When are RPC-ish approaches more appropriate than REST?](http://programmers.stackexchange.com/a/181186)\n* [REST vs JSON-RPC](http://stackoverflow.com/questions/15056878/rest-vs-json-rpc)\n* [Debunking the myths of RPC and REST](http://etherealbits.com/2012/12/debunking-the-myths-of-rpc-rest/)\n* [What are the drawbacks of using REST](https://www.quora.com/What-are-the-drawbacks-of-using-RESTful-APIs)\n* [Crack the system design interview](http://www.puncsky.com/blog/2016-02-13-crack-the-system-design-interview)\n* [Thrift](https://code.facebook.com/posts/1468950976659943/)\n* [Why REST for internal use and not RPC](http://arstechnica.com/civis/viewtopic.php?t=1190508)\n\n## Security\n\nThis section could use some updates.  Consider [contributing](#contributing)!\n\nSecurity is a broad topic.  Unless you have considerable experience, a security background, or are applying for a position that requires knowledge of security, you probably won't need to know more than the basics:\n\n* Encrypt in transit and at rest.\n* Sanitize all user inputs or any input parameters exposed to user to prevent [XSS](https://en.wikipedia.org/wiki/Cross-site_scripting) and [SQL injection](https://en.wikipedia.org/wiki/SQL_injection).\n* Use parameterized queries to prevent SQL injection.\n* Use the principle of [least privilege](https://en.wikipedia.org/wiki/Principle_of_least_privilege).\n\n### Source(s) and further reading\n\n* [Security guide for developers](https://github.com/FallibleInc/security-guide-for-developers)\n* [OWASP top ten](https://www.owasp.org/index.php/OWASP_Top_Ten_Cheat_Sheet)\n\n## Appendix\n\nYou'll sometimes be asked to do 'back-of-the-envelope' estimates.  For example, you might need to determine how long it will take to generate 100 image thumbnails from disk or how much memory a data structure will take.  The **Powers of two table** and **Latency numbers every programmer should know** are handy references.\n\n### Powers of two table\n\n```\nPower           Exact Value         Approx Value        Bytes\n---------------------------------------------------------------\n7                             128\n8                             256\n10                           1024   1 thousand           1 KB\n16                         65,536                       64 KB\n20                      1,048,576   1 million            1 MB\n30                  1,073,741,824   1 billion            1 GB\n32                  4,294,967,296                        4 GB\n40              1,099,511,627,776   1 trillion           1 TB\n```\n\n#### Source(s) and further reading\n\n* [Powers of two](https://en.wikipedia.org/wiki/Power_of_two)\n\n### Latency numbers every programmer should know\n\n```\nLatency Comparison Numbers\n--------------------------\nL1 cache reference                           0.5 ns\nBranch mispredict                            5   ns\nL2 cache reference                           7   ns                      14x L1 cache\nMutex lock/unlock                          100   ns\nMain memory reference                      100   ns                      20x L2 cache, 200x L1 cache\nCompress 1K bytes with Zippy            10,000   ns       10 us\nSend 1 KB bytes over 1 Gbps network     10,000   ns       10 us\nRead 4 KB randomly from SSD*           150,000   ns      150 us          ~1GB/sec SSD\nRead 1 MB sequentially from memory     250,000   ns      250 us\nRound trip within same datacenter      500,000   ns      500 us\nRead 1 MB sequentially from SSD*     1,000,000   ns    1,000 us    1 ms  ~1GB/sec SSD, 4X memory\nDisk seek                           10,000,000   ns   10,000 us   10 ms  20x datacenter roundtrip\nRead 1 MB sequentially from 1 Gbps  10,000,000   ns   10,000 us   10 ms  40x memory, 10X SSD\nRead 1 MB sequentially from disk    30,000,000   ns   30,000 us   30 ms 120x memory, 30X SSD\nSend packet CA-\u003eNetherlands-\u003eCA    150,000,000   ns  150,000 us  150 ms\n\nNotes\n-----\n1 ns = 10^-9 seconds\n1 us = 10^-6 seconds = 1,000 ns\n1 ms = 10^-3 seconds = 1,000 us = 1,000,000 ns\n```\n\nHandy metrics based on numbers above:\n\n* Read sequentially from disk at 30 MB/s\n* Read sequentially from 1 Gbps Ethernet at 100 MB/s\n* Read sequentially from SSD at 1 GB/s\n* Read sequentially from main memory at 4 GB/s\n* 6-7 world-wide round trips per second\n* 2,000 round trips per second within a data center\n\n#### Latency numbers visualized\n\n![](https://camo.githubusercontent.com/77f72259e1eb58596b564d1ad823af1853bc60a3/687474703a2f2f692e696d6775722e636f6d2f6b307431652e706e67)\n\n#### Source(s) and further reading\n\n* [Latency numbers every programmer should know - 1](https://gist.github.com/jboner/2841832)\n* [Latency numbers every programmer should know - 2](https://gist.github.com/hellerbarde/2843375)\n* [Designs, lessons, and advice from building large distributed systems](http://www.cs.cornell.edu/projects/ladis2009/talks/dean-keynote-ladis2009.pdf)\n* [Software Engineering Advice from Building Large-Scale Distributed Systems](https://static.googleusercontent.com/media/research.google.com/en//people/jeff/stanford-295-talk.pdf)\n\n### Additional system design interview questions\n\n\u003e Common system design interview questions, with links to resources on how to solve each.\n\n| Question | Reference(s) |\n|---|---|\n| Design a file sync service like Dropbox | [youtube.com](https://www.youtube.com/watch?v=PE4gwstWhmc) |\n| Design a search engine like Google | [queue.acm.org](http://queue.acm.org/detail.cfm?id=988407)\u003cbr/\u003e[stackexchange.com](http://programmers.stackexchange.com/questions/38324/interview-question-how-would-you-implement-google-search)\u003cbr/\u003e[ardendertat.com](http://www.ardendertat.com/2012/01/11/implementing-search-engines/)\u003cbr\u003e[stanford.edu](http://infolab.stanford.edu/~backrub/google.html) |\n| Design a scalable web crawler like Google | [quora.com](https://www.quora.com/How-can-I-build-a-web-crawler-from-scratch) |\n| Design Google docs | [code.google.com](https://code.google.com/p/google-mobwrite/)\u003cbr/\u003e[neil.fraser.name](https://neil.fraser.name/writing/sync/) |\n| Design a key-value store like Redis | [slideshare.net](http://www.slideshare.net/dvirsky/introduction-to-redis) |\n| Design a cache system like Memcached | [slideshare.net](http://www.slideshare.net/oemebamo/introduction-to-memcached) |\n| Design a recommendation system like Amazon's | [hulu.com](https://web.archive.org/web/20170406065247/http://tech.hulu.com/blog/2011/09/19/recommendation-system.html)\u003cbr/\u003e[ijcai13.org](http://ijcai13.org/files/tutorial_slides/td3.pdf) |\n| Design a tinyurl system like Bitly | [n00tc0d3r.blogspot.com](http://n00tc0d3r.blogspot.com/) |\n| Design a chat app like WhatsApp | [highscalability.com](http://highscalability.com/blog/2014/2/26/the-whatsapp-architecture-facebook-bought-for-19-billion.html)\n| Design a picture sharing system like Instagram | [highscalability.com](http://highscalability.com/flickr-architecture)\u003cbr/\u003e[highscalability.com](http://highscalability.com/blog/2011/12/6/instagram-architecture-14-million-users-terabytes-of-photos.html) |\n| Design the Facebook news feed function | [quora.com](http://www.quora.com/What-are-best-practices-for-building-something-like-a-News-Feed)\u003cbr/\u003e[quora.com](http://www.quora.com/Activity-Streams/What-are-the-scaling-issues-to-keep-in-mind-while-developing-a-social-network-feed)\u003cbr/\u003e[slideshare.net](http://www.slideshare.net/danmckinley/etsy-activity-feeds-architecture) |\n| Design the Facebook timeline function | [facebook.com](https://www.facebook.com/note.php?note_id=10150468255628920)\u003cbr/\u003e[highscalability.com](http://highscalability.com/blog/2012/1/23/facebook-timeline-brought-to-you-by-the-power-of-denormaliza.html) |\n| Design the Facebook chat function | [erlang-factory.com](http://www.erlang-factory.com/upload/presentations/31/EugeneLetuchy-ErlangatFacebook.pdf)\u003cbr/\u003e[facebook.com](https://www.facebook.com/note.php?note_id=14218138919\u0026id=9445547199\u0026index=0) |\n| Design a graph search function like Facebook's | [facebook.com](https://www.facebook.com/notes/facebook-engineering/under-the-hood-building-out-the-infrastructure-for-graph-search/10151347573598920)\u003cbr/\u003e[facebook.com](https://www.facebook.com/notes/facebook-engineering/under-the-hood-indexing-and-ranking-in-graph-search/10151361720763920)\u003cbr/\u003e[facebook.com](https://www.facebook.com/notes/facebook-engineering/under-the-hood-the-natural-language-interface-of-graph-search/10151432733048920) |\n| Design a content delivery network like CloudFlare | [figshare.com](https://figshare.com/articles/Globally_distributed_content_delivery/6605972) |\n| Design a trending topic system like Twitter's | [michael-noll.com](http://www.michael-noll.com/blog/2013/01/18/implementing-real-time-trending-topics-in-storm/)\u003cbr/\u003e[snikolov .wordpress.com](http://snikolov.wordpress.com/2012/11/14/early-detection-of-twitter-trends/) |\n| Design a random ID generation system | [blog.twitter.com](https://blog.twitter.com/2010/announcing-snowflake)\u003cbr/\u003e[github.com](https://github.com/twitter/snowflake/) |\n| Return the top k requests during a time interval | [cs.ucsb.edu](https://www.cs.ucsb.edu/sites/cs.ucsb.edu/files/docs/reports/2005-23.pdf)\u003cbr/\u003e[wpi.edu](http://davis.wpi.edu/xmdv/docs/EDBT11-diyang.pdf) |\n| Design a system that serves data from multiple data centers | [highscalability.com](http://highscalability.com/blog/2009/8/24/how-google-serves-data-from-multiple-datacenters.html) |\n| Design an online multiplayer card game | [indieflashblog.com](http://www.indieflashblog.com/how-to-create-an-asynchronous-multiplayer-game.html)\u003cbr/\u003e[buildnewgames.com](http://buildnewgames.com/real-time-multiplayer/) |\n| Design a garbage collection system | [stuffwithstuff.com](http://journal.stuffwithstuff.com/2013/12/08/babys-first-garbage-collector/)\u003cbr/\u003e[washington.edu](http://courses.cs.washington.edu/courses/csep521/07wi/prj/rick.pdf) |\n| Design an API rate limiter | [https://stripe.com/blog/](https://stripe.com/blog/rate-limiters) |\n| Add a system design question | [Contribute](#contributing) |\n\n### Real world architectures\n\n\u003e Articles on how real world systems are designed.\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"http://i.imgur.com/TcUo2fw.png\"\u003e\n  \u003cbr/\u003e\n  \u003ci\u003e\u003ca href=https://www.infoq.com/presentations/Twitter-Timeline-Scalability\u003eSource: Twitter timelines at scale\u003c/a\u003e\u003c/i\u003e\n\u003c/p\u003e\n\n**Don't focus on nitty gritty details for the following articles, instead:**\n\n* Identify shared principles, common technologies, and patterns within these articles\n* Study what problems are solved by each component, where it works, where it doesn't\n* Review the lessons learned\n\n|Type | System | Reference(s) |\n|---|---|---|\n| Data processing | **MapReduce** - Distributed data processing from Google | [research.google.com](http://static.googleusercontent.com/media/research.google.com/zh-CN/us/archive/mapreduce-osdi04.pdf) |\n| Data processing | **Spark** - Distributed data processing from Databricks | [slideshare.net](http://www.slideshare.net/AGrishchenko/apache-spark-architecture) |\n| Data processing | **Storm** - Distributed data processing from Twitter | [slideshare.net](http://www.slideshare.net/previa/storm-16094009) |\n| | | |\n| Data store | **Bigtable** - Distributed column-oriented database from Google | [harvard.edu](http://www.read.seas.harvard.edu/~kohler/class/cs239-w08/chang06bigtable.pdf) |\n| Data store | **HBase** - Open source implementation of Bigtable | [slideshare.net](http://www.slideshare.net/alexbaranau/intro-to-hbase) |\n| Data store | **Cassandra** - Distributed column-oriented database from Facebook | [slideshare.net](http://www.slideshare.net/planetcassandra/cassandra-introduction-features-30103666)\n| Data store | **DynamoDB** - Document-oriented database from Amazon | [harvard.edu](http://www.read.seas.harvard.edu/~kohler/class/cs239-w08/decandia07dynamo.pdf) |\n| Data store | **MongoDB** - Document-oriented database | [slideshare.net](http://www.slideshare.net/mdirolf/introduction-to-mongodb) |\n| Data store | **Spanner** - Globally-distributed database from Google | [research.google.com](http://research.google.com/archive/spanner-osdi2012.pdf) |\n| Data store | **Memcached** - Distributed memory caching system | [slideshare.net](http://www.slideshare.net/oemebamo/introduction-to-memcached) |\n| Data store | **Redis** - Distributed memory caching system with persistence and value types | [slideshare.net](http://www.slideshare.net/dvirsky/introduction-to-redis) |\n| | | |\n| File system | **Google File System (GFS)** - Distributed file system | [research.google.com](http://static.googleusercontent.com/media/research.google.com/zh-CN/us/archive/gfs-sosp2003.pdf) |\n| File system | **Hadoop File System (HDFS)** - Open source implementation of GFS | [apache.org](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html) |\n| | | |\n| Misc | **Chubby** - Lock service for loosely-coupled distributed systems from Google | [research.google.com](http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/chubby-osdi06.pdf) |\n| Misc | **Dapper** - Distributed systems tracing infrastructure | [research.google.com](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36356.pdf)\n| Misc | **Kafka** - Pub/sub message queue from LinkedIn | [slideshare.net](http://www.slideshare.net/mumrah/kafka-talk-tri-hug) |\n| Misc | **Zookeeper** - Centralized infrastructure and services enabling synchronization | [slideshare.net](http://www.slideshare.net/sauravhaloi/introduction-to-apache-zookeeper) |\n| | Add an architecture | [Contribute](#contributing) |\n\n### Company architectures\n\n| Company | Reference(s) |\n|---|---|\n| Amazon | [Amazon architecture](http://highscalability.com/amazon-architecture) |\n| Cinchcast | [Producing 1,500 hours of audio every day](http://highscalability.com/blog/2012/7/16/cinchcast-architecture-producing-1500-hours-of-audio-every-d.html) |\n| DataSift | [Realtime datamining At 120,000 tweets per second](http://highscalability.com/blog/2011/11/29/datasift-architecture-realtime-datamining-at-120000-tweets-p.html) |\n| DropBox | [How we've scaled Dropbox](https://www.youtube.com/watch?v=PE4gwstWhmc) |\n| ESPN | [Operating At 100,000 duh nuh nuhs per second](http://highscalability.com/blog/2013/11/4/espns-architecture-at-scale-operating-at-100000-duh-nuh-nuhs.html) |\n| Google | [Google architecture](http://highscalability.com/google-architecture) |\n| Instagram | [14 million users, terabytes of photos](http://highscalability.com/blog/2011/12/6/instagram-architecture-14-million-users-terabytes-of-photos.html)\u003cbr/\u003e[What powers Instagram](http://instagram-engineering.tumblr.com/post/13649370142/what-powers-instagram-hundreds-of-instances) |\n| Justin.tv | [Justin.Tv's live video broadcasting architecture](http://highscalability.com/blog/2010/3/16/justintvs-live-video-broadcasting-architecture.html) |\n| Facebook | [Scaling memcached at Facebook](https://cs.uwaterloo.ca/~brecht/courses/854-Emerging-2014/readings/key-value/fb-memcached-nsdi-2013.pdf)\u003cbr/\u003e[TAO: Facebook’s distributed data store for the social graph](https://cs.uwaterloo.ca/~brecht/courses/854-Emerging-2014/readings/data-store/tao-facebook-distributed-datastore-atc-2013.pdf)\u003cbr/\u003e[Facebook’s photo storage](https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Beaver.pdf)\u003cbr/\u003e[How Facebook Live Streams To 800,000 Simultaneous Viewers](http://highscalability.com/blog/2016/6/27/how-facebook-live-streams-to-800000-simultaneous-viewers.html) |\n| Flickr | [Flickr architecture](http://highscalability.com/flickr-architecture) |\n| Mailbox | [From 0 to one million users in 6 weeks](http://highscalability.com/blog/2013/6/18/scaling-mailbox-from-0-to-one-million-users-in-6-weeks-and-1.html) |\n| Netflix | [A 360 Degree View Of The Entire Netflix Stack](http://highscalability.com/blog/2015/11/9/a-360-degree-view-of-the-entire-netflix-stack.html)\u003cbr/\u003e[Netflix: What Happens When You Press Play?](http://highscalability.com/blog/2017/12/11/netflix-what-happens-when-you-press-play.html) |\n| Pinterest | [From 0 To 10s of billions of page views a month](http://highscalability.com/blog/2013/4/15/scaling-pinterest-from-0-to-10s-of-billions-of-page-views-a.html)\u003cbr/\u003e[18 million visitors, 10x growth, 12 employees](http://highscalability.com/blog/2012/5/21/pinterest-architecture-update-18-million-visitors-10x-growth.html) |\n| Playfish | [50 million monthly users and growing](http://highscalability.com/blog/2010/9/21/playfishs-social-gaming-architecture-50-million-monthly-user.html) |\n| PlentyOfFish | [PlentyOfFish architecture](http://highscalability.com/plentyoffish-architecture) |\n| Salesforce | [How they handle 1.3 billion transactions a day](http://highscalability.com/blog/2013/9/23/salesforce-architecture-how-they-handle-13-billion-transacti.html) |\n| Stack Overflow | [Stack Overflow architecture](http://highscalability.com/blog/2009/8/5/stack-overflow-architecture.html) |\n| TripAdvisor | [40M visitors, 200M dynamic page views, 30TB data](http://highscalability.com/blog/2011/6/27/tripadvisor-architecture-40m-visitors-200m-dynamic-page-view.html) |\n| Tumblr | [15 billion page views a month](http://highscalability.com/blog/2012/2/13/tumblr-architecture-15-billion-page-views-a-month-and-harder.html) |\n| Twitter | [Making Twitter 10000 percent faster](http://highscalability.com/scaling-twitter-making-twitter-10000-percent-faster)\u003cbr/\u003e[Storing 250 million tweets a day using MySQL](http://highscalability.com/blog/2011/12/19/how-twitter-stores-250-million-tweets-a-day-using-mysql.html)\u003cbr/\u003e[150M active users, 300K QPS, a 22 MB/S firehose](http://highscalability.com/blog/2013/7/8/the-architecture-twitter-uses-to-deal-with-150m-active-users.html)\u003cbr/\u003e[Timelines at scale](https://www.infoq.com/presentations/Twitter-Timeline-Scalability)\u003cbr/\u003e[Big and small data at Twitter](https://www.youtube.com/watch?v=5cKTP36HVgI)\u003cbr/\u003e[Operations at Twitter: scaling beyond 100 million users](https://www.youtube.com/watch?v=z8LU0Cj6BOU) |\n| Uber | [How Uber scales their real-time market platform](http://highscalability.com/blog/2015/9/14/how-uber-scales-their-real-time-market-platform.html)\u003cbr/\u003e[Lessons Learned From Scaling Uber To 2000 Engineers, 1000 Services, And 8000 Git Repositories](http://highscalability.com/blog/2016/10/12/lessons-learned-from-scaling-uber-to-2000-engineers-1000-ser.html) |\n| WhatsApp | [The WhatsApp architecture Facebook bought for $19 billion](http://highscalability.com/blog/2014/2/26/the-whatsapp-architecture-facebook-bought-for-19-billion.html) |\n| YouTube | [YouTube scalability](https://www.youtube.com/watch?v=w5WVu624fY8)\u003cbr/\u003e[YouTube architecture](http://highscalability.com/youtube-architecture) |\n\n### Company engineering blogs\n\n\u003e Architectures for companies you are interviewing with.\n\u003e\n\u003e Questions you encounter might be from the same domain.\n\n* [Airbnb Engineering](http://nerds.airbnb.com/)\n* [Atlassian Developers](https://developer.atlassian.com/blog/)\n* [AWS Blog](https://aws.amazon.com/blogs/aws/)\n* [Bitly Engineering Blog](http://word.bitly.com/)\n* [Box Blogs](https://blog.box.com/blog/category/engineering)\n* [Cloudera Developer Blog](http://blog.cloudera.com/)\n* [Dropbox Tech Blog](https://tech.dropbox.com/)\n* [Engineering at Quora](http://engineering.quora.com/)\n* [Ebay Tech Blog](http://www.ebaytechblog.com/)\n* [Evernote Tech Blog](https://blog.evernote.com/tech/)\n* [Etsy Code as Craft](http://codeascraft.com/)\n* [Facebook Engineering](https://www.facebook.com/Engineering)\n* [Flickr Code](http://code.flickr.net/)\n* [Foursquare Engineering Blog](http://engineering.foursquare.com/)\n* [GitHub Engineering Blog](http://githubengineering.com/)\n* [Google Research Blog](http://googleresearch.blogspot.com/)\n* [Groupon Engineering Blog](https://engineering.groupon.com/)\n* [Heroku Engineering Blog](https://engineering.heroku.com/)\n* [Hubspot Engineering Blog](http://product.hubspot.com/blog/topic/engineering)\n* [High Scalability](http://highscalability.com/)\n* [Instagram Engineering](http://instagram-engineering.tumblr.com/)\n* [Intel Software Blog](https://software.intel.com/en-us/blogs/)\n* [Jane Street Tech Blog](https://blogs.janestreet.com/category/ocaml/)\n* [LinkedIn Engineering](http://engineering.linkedin.com/blog)\n* [Microsoft Engineering](https://engineering.microsoft.com/)\n* [Microsoft Python Engineering](https://blogs.msdn.microsoft.com/pythonengineering/)\n* [Netflix Tech Blog](http://techblog.netflix.com/)\n* [Paypal Developer Blog](https://devblog.paypal.com/category/engineering/)\n* [Pinterest Engineering Blog](https://medium.com/@Pinterest_Engineering)\n* [Quora Engineering](https://engineering.quora.com/)\n* [Reddit Blog](http://www.redditblog.com/)\n* [Salesforce Engineering Blog](https://developer.salesforce.com/blogs/engineering/)\n* [Slack Engineering Blog](https://slack.engineering/)\n* [Spotify Labs](https://labs.spotify.com/)\n* [Twilio Engineering Blog](http://www.twilio.com/engineering)\n* [Twitter Engineering](https://blog.twitter.com/engineering/)\n* [Uber Engineering Blog](http://eng.uber.com/)\n* [Yahoo Engineering Blog](http://yahooeng.tumblr.com/)\n* [Yelp Engineering Blog](http://engineeringblog.yelp.com/)\n* [Zynga Engineering Blog](https://www.zynga.com/blogs/engineering)\n\n#### Source(s) and further reading\n\nLooking to add a blog?  To avoid duplicating work, consider adding your company blog to the following repo:\n\n* [kilimchoi/engineering-blogs](https://github.com/kilimchoi/engineering-blogs)\n\n## Under development\n\nInterested in adding a section or helping complete one in-progress?  [Contribute](#contributing)!\n\n* Distributed computing with MapReduce\n* Consistent hashing\n* Scatter gather\n* [Contribute](#contributing)\n\n## Credits\n\nCredits and sources are provided throughout this repo.\n\nSpecial thanks to:\n\n* [Hired in tech](http://www.hiredintech.com/system-design/the-system-design-process/)\n* [Cracking the coding interview](https://www.amazon.com/dp/0984782850/)\n* [High scalability](http://highscalability.com/)\n* [checkcheckzz/system-design-interview](https://github.com/checkcheckzz/system-design-interview)\n* [shashank88/system_design](https://github.com/shashank88/system_design)\n* [mmcgrana/services-engineering](https://github.com/mmcgrana/services-engineering)\n* [System design cheat sheet](https://gist.github.com/vasanthk/485d1c25737e8e72759f)\n* [A distributed systems reading list](http://dancres.github.io/Pages/)\n* [Cracking the system design interview](http://www.puncsky.com/blog/2016-02-13-crack-the-system-design-interview)\n\n## Contact info\n\nFeel free to contact me to discuss any issues, questions, or comments.\n\nMy contact info can be found on my [GitHub page](https://github.com/donnemartin).\n\n## License\n\n*I am providing code and resources in this repository to you under an open source license.  Because this is my personal repository, the license you receive to my code and resources is from me and not my employer (Facebook).*\n\n    Copyright 2017 Donne Martin\n\n    Creative Commons Attribution 4.0 International License (CC BY 4.0)\n\n    http://creativecommons.org/licenses/by/4.0/\n"
  },
  {
    "repo": "tensorflow/models",
    "content": "# TensorFlow Models\n\nThis repository contains a number of different models implemented in [TensorFlow](https://www.tensorflow.org):\n\nThe [official models](official) are a collection of example models that use TensorFlow's high-level APIs. They are intended to be well-maintained, tested, and kept up to date with the latest stable TensorFlow API. They should also be reasonably optimized for fast performance while still being easy to read. We especially recommend newer TensorFlow users to start here.\n\nThe [research models](https://github.com/tensorflow/models/tree/master/research) are a large collection of models implemented in TensorFlow by researchers. They are not officially supported or available in release branches; it is up to the individual researchers to maintain the models and/or provide support on issues and pull requests.\n\nThe [samples folder](samples) contains code snippets and smaller models that demonstrate features of TensorFlow, including code presented in various blog posts.\n\nThe [tutorials folder](tutorials) is a collection of models described in the [TensorFlow tutorials](https://www.tensorflow.org/tutorials/).\n\n## Contribution guidelines\n\nIf you want to contribute to models, be sure to review the [contribution guidelines](CONTRIBUTING.md).\n\n## License\n\n[Apache License 2.0](LICENSE)\n"
  },
  {
    "repo": "toddmotto/public-apis",
    "content": "# Public APIs [![Build Status](https://api.travis-ci.org/toddmotto/public-apis.svg)](https://travis-ci.org/toddmotto/public-apis)\n\nA collective list of free APIs for use in software and web development.\n\nSponsor:\n\n\u003ca href=\"https://ultimatecourses.com?utm_source=github.com\"\u003e\u003cimg src=\"https://ultimatecourses.com/assets/img/banners/uc-leader.svg\" style=\"width:100%;max-width:100%\"\u003e\u003c/a\u003e\n\nA public API for this project can be found [here](https://github.com/davemachado/public-api) - thanks to [DigitalOcean](https://www.digitalocean.com/) for helping us provide this service!\n\nFor information on contributing to this project, please see the [contributing guide](.github/CONTRIBUTING.md).\n\nPlease note a passing build status indicates all listed APIs are available since the last update. A failing build status indicates that 1 or more services may be unavailable at the moment.\n\n## Index\n\n* [Animals](#animals)\n* [Anime](#anime)\n* [Anti-Malware](#anti-malware)\n* [Art \u0026 Design](#art--design)\n* [Books](#books)\n* [Business](#business)\n* [Calendar](#calendar)\n* [Cloud Storage \u0026 File Sharing](#cloud-storage--file-sharing)\n* [Continuous Integration](#continuous-integration)\n* [Cryptocurrency](#cryptocurrency)\n* [Currency Exchange](#currency-exchange)\n* [Data Validation](#data-validation)\n* [Development](#development)\n* [Dictionaries](#dictionaries)\n* [Documents \u0026 Productivity](#documents--productivity)\n* [Environment](#environment)\n* [Events](#events)\n* [Finance](#finance)\n* [Food \u0026 Drink](#food--drink)\n* [Fraud Prevention](#fraud-prevention)\n* [Games \u0026 Comics](#games--comics)\n* [Geocoding](#geocoding)\n* [Government](#government)\n* [Health](#health)\n* [Jobs](#jobs)\n* [Machine Learning](#machine-learning)\n* [Music](#music)\n* [News](#news)\n* [Open Data](#open-data)\n* [Open Source Projects](#open-source-projects)\n* [Patent](#patent)\n* [Personality](#personality)\n* [Photography](#photography)\n* [Science \u0026 Math](#science--math)\n* [Security](#security)\n* [Shopping](#shopping)\n* [Social](#social)\n* [Sports \u0026 Fitness](#sports--fitness)\n* [Test Data](#test-data)\n* [Text Analysis](#text-analysis)\n* [Tracking](#tracking)\n* [Transportation](#transportation)\n* [URL Shorteners](#url-shorteners)\n* [Vehicle](#vehicle)\n* [Video](#video)\n* [Weather](#weather)\n\n### Animals\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Cat Facts](https://alexwohlbruck.github.io/cat-facts/) | Daily cat facts | No | Yes | Unknown |\n| [Cats](https://thecatapi.com/docs.html) | Pictures of cats from Tumblr | `apiKey` | Yes | Unknown |\n| [Dogs](https://dog.ceo/dog-api/) | Based on the Stanford Dogs Dataset | No | Yes | Yes |\n| [HTTPCat](https://http.cat/) | Cat for every HTTP Status | No | Yes | Unknown |\n| [IUCN](http://apiv3.iucnredlist.org/api/v3/docs) | IUCN Red List of Threatened Species | `apiKey` | No | Unknown |\n| [Movebank](https://github.com/movebank/movebank-api-doc) | Movement and Migration data of animals | No | Yes | Unknown |\n| [Petfinder](https://www.petfinder.com/developers/api-docs/) | Adoption | `apiKey` | Yes | Unknown |\n| [RandomCat](https://aws.random.cat/meow) | Random pictures of cats | No | Yes | Yes |\n| [RandomDog](https://random.dog/woof.json) | Random pictures of dogs | No | Yes | Yes |\n| [RandomFox](https://randomfox.ca/floof/) | Random pictures of foxes | No | Yes | No |\n| [RescueGroups](https://userguide.rescuegroups.org/display/APIDG/API+Developers+Guide+Home) | Adoption | No | Yes | Unknown |\n| [Shibe.Online](http://shibe.online/) | Random pictures of Shibu Inu, cats or birds | No | No | No |\n\n### Anime\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [AniList](https://github.com/AniList/ApiV2-GraphQL-Docs) | Anime discovery \u0026 tracking | `OAuth` | Yes | Unknown |\n| [AnimeNewsNetwork](https://www.animenewsnetwork.com/encyclopedia/api.php) | Anime industry news | No | Yes | Yes |\n| [Jikan](https://jikan.moe) | Unofficial MyAnimeList API | No | Yes | Yes |\n| [Kitsu](http://docs.kitsu.apiary.io/) | Anime discovery platform | `OAuth` | Yes | Unknown |\n| [Studio Ghibli](https://ghibliapi.herokuapp.com) | Resources from Studio Ghibli films | No | Yes | Unknown |\n\n### Anti-Malware\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [AlienVault Open Threat Exchange (OTX)](https://otx.alienvault.com/api/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |\n| [Google Safe Browsing](https://developers.google.com/safe-browsing/) | Google Link/Domain Flagging | `apiKey` | Yes | Unknown |\n| [Metacert](https://metacert.com/) | Metacert Link Flagging | `apiKey` | Yes | Unknown |\n| [VirusTotal](https://www.virustotal.com/en/documentation/public-api/) | VirusTotal File/URL Analysis | `apiKey` | Yes | Unknown |\n| [Web Of Trust (WOT)](https://www.mywot.com/wiki/API) | Website reputation | `apiKey` | Yes | Unknown |\n\n### Art \u0026 Design\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Behance](https://www.behance.net/dev) | Design | `apiKey` | Yes | Unknown |\n| [Cooper Hewitt](https://collection.cooperhewitt.org/api) | Smithsonian Design Museum | `apiKey` | Yes | Unknown |\n| [Dribbble](http://developer.dribbble.com/v1/) | Design | `OAuth` | No | Unknown |\n| [Harvard Art Museums](https://github.com/harvardartmuseums/api-docs) | Art | `apiKey` | No | Unknown |\n| [Iconfinder](https://developer.iconfinder.com) | Icons | `apiKey` | Yes | Unknown |\n| [Icons8](http://docs.icons8.apiary.io/#reference/0/meta) | Icons | `OAuth` | Yes | Unknown |\n| [Noun Project](http://api.thenounproject.com/index.html) | Icons | `OAuth` | No | Unknown |\n| [Rijksmuseum](https://www.rijksmuseum.nl/en/api) | Art | `apiKey` | Yes | Unknown |\n\n### Books\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Bhagavad Gita](https://bhagavadgita.io/api) | Bhagavad Gita text | `OAuth` | Yes | Yes |\n| [BookNomads](https://www.booknomads.com/dev) | Books published in the Netherlands and Flanders (about 2.5 million), book covers and related data | No | Yes | Unknown |\n| [British National Bibliography](http://bnb.data.bl.uk/) | Books | No | No | Unknown |\n| [Goodreads](https://www.goodreads.com/api) | Books | `apiKey` | Yes | Unknown |\n| [Google Books](https://developers.google.com/books/) | Books | `OAuth` | Yes | Unknown |\n| [LibGen](http://garbage.world/posts/libgen/) | Library Genesis search engine | No | No | Unknown |\n| [Open Library](https://openlibrary.org/developers/api) | Books, book covers and related data | No | Yes | Unknown |\n| [Penguin Publishing](http://www.penguinrandomhouse.biz/webservices/rest/) | Books, book covers and related data | No | Yes | Unknown |\n\n### Business\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Charity Search](http://charityapi.orghunter.com/) | Non-profit charity data | `apiKey` | No | Unknown |\n| [Clearbit Logo](https://clearbit.com/docs#logo-api) | Search for company logos and embed them in your projects | `apiKey` | Yes | Unknown |\n| [Domainsdb.info](https://domainsdb.info/) | Registered Domain Names Search | No | Yes | Unknown |\n| [Gmail](https://developers.google.com/gmail/api/) | Flexible, RESTful access to the user's inbox | `OAuth` | Yes | Unknown |\n| [Google Analytics](https://developers.google.com/analytics/) | Collect, configure and analyze your data to reach the right audience | `OAuth` | Yes | Unknown |\n| [mailgun](https://www.mailgun.com/) | Email Service | `apiKey` | Yes | Unknown |\n| [markerapi](http://www.markerapi.com/) | Trademark Search | No | No | Unknown |\n| [Ticksel](https://ticksel.com) | Friendly website analytics made for humans | No | Yes | Unknown |\n| [Trello](https://developers.trello.com/) | Boards, lists and cards to help you organize and prioritize your projects | `OAuth` | Yes | Unknown |\n\n### Calendar\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Calendar Index](https://www.calendarindex.com/) | Worldwide Holidays and Working Days | `apiKey` | Yes | Yes |\n| [Church Calendar](http://calapi.inadiutorium.cz/) | Catholic liturgical calendar | No | No | Unknown |\n| [Czech Namedays Calendar](http://svatky.adresa.info/) | Lookup for a name and returns nameday date | No | No | Unknown |\n| [Google Calendar](https://developers.google.com/google-apps/calendar/) | Display, create and modify Google calendar events | `OAuth` | Yes | Unknown |\n| [Hebrew Calendar](https://www.hebcal.com/home/developer-apis) | Convert between Gregarian and Hebrew, fetch Shabbat and Holiday times, etc | No | No | Unknown |\n| [Holidays](https://holidayapi.com/) | Historical data regarding holidays | `apiKey` | Yes | Unknown |\n| [LectServe](http://www.lectserve.com) | Protestant liturgical calendar | No | No | Unknown |\n| [Nager.Date](https://date.nager.at) | Public holidays for more than 90 countries | No | Yes | No |\n| [Namedays Calendar](https://api.abalin.net/) | Provides namedays for multiple countries | No | Yes | Yes |\n| [Non-Working Days](https://github.com/gadael/icsdb) | Database of ICS files for non working days | No | Yes | Unknown |\n| [Russian Calendar](https://github.com/egno/work-calendar) | Check if a date is a Russian holiday or not | No | Yes | No |\n\n### Cloud Storage \u0026 File Sharing\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Box](https://developer.box.com/) | File Sharing and Storage | `OAuth` | Yes | Unknown |\n| [Dropbox](https://www.dropbox.com/developers) | File Sharing and Storage | `OAuth` | Yes | Unknown |\n| [Google Drive](https://developers.google.com/drive/) | File Sharing and Storage | `OAuth` | Yes | Unknown |\n| [OneDrive](https://dev.onedrive.com/) | File Sharing and Storage | `OAuth` | Yes | Unknown |\n| [Pastebin](https://pastebin.com/api/) | Plain Text Storage | `apiKey` | Yes | Unknown |\n| [WeTransfer](https://developers.wetransfer.com) | File Sharing | `apiKey` | Yes | Yes |\n\n### Continuous Integration\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [CircleCI](https://circleci.com/docs/api/v1-reference/) | Automate the software development process using continuous integration and continuous delivery | `apiKey` | Yes | Unknown |\n| [Codeship](https://apidocs.codeship.com/) | Codeship is a Continuous Integration Platform in the cloud | `apiKey` | Yes | Unknown |\n| [Travis CI](https://docs.travis-ci.com/api/) | Sync your GitHub projects with Travis CI to test your code in minutes | `apiKey` | Yes | Unknown |\n\n### Cryptocurrency\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Binance](https://github.com/binance-exchange/binance-official-api-docs) | Exchange for Trading Cryptocurrencies based in China | `apiKey` | Yes | Unknown |\n| [BitcoinAverage](https://apiv2.bitcoinaverage.com/) | Digital Asset Price Data for the blockchain industry | `apiKey` | Yes | Unknown |\n| [BitcoinCharts](https://bitcoincharts.com/about/exchanges/) | Financial and Technical Data related to the Bitcoin Network | No | Yes | Unknown |\n| [Bitfinex](https://docs.bitfinex.com/docs/introduction) | Cryptocurrency Trading Platform | `apiKey` | Yes | Unknown |\n| [Bitmex](https://www.bitmex.com/app/apiOverview) | Real-Time Cryptocurrency derivatives trading platform based in Hong Kong | `apiKey` | Yes | Unknown |\n| [Bittrex](https://bittrex.com/Home/Api) | Next Generation Crypto Trading Platform | `apiKey` | Yes | Unknown |\n| [Block](https://www.block.io/docs/basic) | Bitcoin Payment, Wallet \u0026 Transaction Data | `apiKey` | Yes | Unknown |\n| [Blockchain](https://www.blockchain.info/api) | Bitcoin Payment, Wallet \u0026 Transaction Data | No | Yes | Unknown |\n| [Chasing Coin](https://chasing-coins.com/api) | Cryptocurrency, coin and token resource on the web | No | Yes | Unknown |\n| [CoinAPI](https://docs.coinapi.io/) | All Currency Exchanges integrate under a single api | `apiKey` | Yes | No |\n| [Coinbase](https://developers.coinbase.com) | Bitcoin, Bitcoin Cash, Litecoin and Ethereum Prices | `apiKey` | Yes | Unknown |\n| [Coinbase Pro](https://docs.pro.coinbase.com/#api) | Cryptocurrency Trading Platform | `apiKey` | Yes | Unknown |\n| [CoinBin](https://coinbin.org/) | Cryptocurrency information | No | Yes | Unknown |\n| [CoinDesk](http://www.coindesk.com/api/) | Bitcoin Price Index | No | No | Unknown |\n| [Coinigy](https://coinigy.docs.apiary.io) | Interacting with Coinigy Accounts and Exchange Directly | `apiKey` | Yes | Unknown |\n| [CoinLayer](https://coinlayer.com) | Real-time Crypto Currency Exchange Rates | `apiKey` | Yes | Unknown |\n| [CoinMarketCap](https://coinmarketcap.com/api/) | Cryptocurrencies Prices | No | Yes | Unknown |\n| [Coinpaprika](https://api.coinpaprika.com) | Cryptocurrencies prices, volume and more | No | Yes | Yes |\n| [CoinRanking](https://docs.coinranking.com/) | Live Cryptocurrency data | No | Yes | Unknown |\n| [CryptoCompare](https://www.cryptocompare.com/api#) | Cryptocurrencies Comparison | No | Yes | Unknown |\n| [Cryptonator](https://www.cryptonator.com/api/) | Cryptocurrencies Exchange Rates | No | Yes | Unknown |\n| [Gemini](https://docs.gemini.com/rest-api/) | Cryptocurrencies Exchange | No | Yes | Unknown |\n| [ICObench](https://icobench.com/developers) | Various information on listing, ratings, stats, and more | `apiKey` | Yes | Unknown |\n| [Livecoin](https://www.livecoin.net/api) | Cryptocurrency Exchange | No | Yes | Unknown |\n| [MercadoBitcoin](https://www.mercadobitcoin.net/api-doc/) | Brazilian Cryptocurrency Information | No | Yes | Unknown |\n| [Nexchange](https://nexchange2.docs.apiary.io/) | Automated cryptocurrency exchange service | No | No | Yes |\n| [NiceHash](https://www.nicehash.com/doc-api) | Largest Crypto Mining Marketplace | `apiKey` | Yes | Unknown |\n| [Poloniex](https://poloniex.com/support/api/) | US based digital asset exchange | `apiKey` | Yes | Unknown |\n| [WorldCoinIndex](https://www.worldcoinindex.com/apiservice) | Cryptocurrencies Prices | `apiKey` | Yes | Unknown |\n| [Zloader](https://www.zloadr.com/cryptocurrency-developers.php) | Due diligence data platform | `apiKey` | Yes | Unknown |\n\n### Currency Exchange\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [1Forge](https://1forge.com/forex-data-api/api-documentation) | Forex currency market data | `apiKey` | Yes | Unknown |\n| [CryptoStandardizer](https://cryptostandardizer.com) | Standardize crypto coin symbols (e.g. BTC, XBT) across 100+ exchanges | `apiKey` | Yes | Unknown |\n| [Currencylayer](https://currencylayer.com/documentation) | Exchange rates and currency conversion | `apiKey` | Yes | Unknown |\n| [Czech National Bank](https://www.cnb.cz/cs/financni_trhy/devizovy_trh/kurzy_devizoveho_trhu/denni_kurz.xml) | A collection of exchange rates | No | Yes | Unknown |\n| [Exchangeratesapi.io](https://exchangeratesapi.io) | Exchange rates with currency conversion | No | Yes | Yes |\n| [Fixer.io](http://fixer.io) | Exchange rates and currency conversion | `apiKey` | Yes | Unknown |\n\n### Data Validation\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Cloudmersive Validate](https://cloudmersive.com/validate-api) | Validate email addresses, phone numbers, VAT numbers and domain names | `apiKey` | Yes | Yes |\n| [languagelayer](https://languagelayer.com) | Language detection | No | Yes | Unknown |\n| [Lob.com](https://lob.com/) | US Address Verification | `apiKey` | Yes | Unknown |\n| [mailboxlayer](https://mailboxlayer.com) | Email address validation | No | Yes | Unknown |\n| [NumValidate](https://numvalidate.com) | Open Source phone number validation | No | Yes | Unknown |\n| [numverify](https://numverify.com) | Phone number validation | No | Yes | Unknown |\n| [PurgoMalum](http://www.purgomalum.com) | Content validator against profanity \u0026 obscenity | No | No | Unknown |\n| [streetlayer](https://streetlayer.com) | Street address info and validation | `apiKey` | Yes | Unknown |\n| [vatlayer](https://vatlayer.com) | VAT number validation | No | Yes | Unknown |\n\n### Development\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [24 Pull Requests](https://24pullrequests.com/api) | Project to promote open source collaboration during December | No | Yes | Yes |\n| [ApiLeap](https://apileap.com/) | Make screenshots from web pages and HTML | `apiKey` | Yes | Unknown |\n| [Apility.io](https://apility.io/apidocs/) | IP, Domains and Emails anti-abuse API blocklist | No | Yes | Yes |\n| [APIs.guru](https://apis.guru/api-doc/) | Wikipedia for Web APIs, OpenAPI/Swagger specs for public APIs | No | Yes | Unknown |\n| [BetterMeta](http://bettermeta.io) | Return a site's meta tags in JSON format | `X-Mashape-Key` | Yes | Unknown |\n| [Bitbucket](https://api.bitbucket.org/2.0/users/karllhughes) | Pull public information for a Bitbucket account | No | Yes | Unknown |\n| [Bored](https://www.boredapi.com/) | Find random activities to fight boredom | No | Yes | Unknown |\n| [Browshot](https://browshot.com/api/documentation) | Easily make screenshots of web pages in any screen size, as any device | `apiKey` | Yes | Unknown |\n| [CDNJS](https://api.cdnjs.com/libraries/jquery) | Library info on CDNJS | No | Yes | Unknown |\n| [Changelogs.md](https://changelogs.md) | Structured changelog metadata from open source projects | No | Yes | Unknown |\n| [Count.io](https://count.io) | Persistent counting and A/B testing | No | Yes | Unknown |\n| [DigitalOcean Status](https://status.digitalocean.com/api/v1) | Status of all DigitalOcean services | No | Yes | Unknown |\n| [DomainDb Info](https://domainsdb.info/apidomainsdb/index.php) | Domain name search to find all domains containing particular words/phrases/etc | No | Yes | Unknown |\n| [Faceplusplus](https://www.faceplusplus.com/) | A tool to detect face | `OAuth` | Yes | Unknown |\n| [Genderize.io](https://genderize.io) | Determines a gender from a first name | No | Yes | Unknown |\n| [GitHub](https://developer.github.com/v3/) | Make use of GitHub repositories, code and user info programmatically | `OAuth` | Yes | Yes |\n| [Gitlab](https://docs.gitlab.com/ee/api/) | Automate GitLab interaction programmatically | `OAuth` | Yes | Unknown |\n| [Gitter](https://github.com/gitterHQ/docs) | Chat for GitHub | `OAuth` | Yes | Unknown |\n| [HTTP2.Pro](https://http2.pro/doc/api) | Test endpoints for client and server HTTP/2 protocol support | No | Yes | Unknown |\n| [IBM Text to Speech](https://console.bluemix.net/docs/services/text-to-speech/getting-started.html) | Convert text to speech | `apiKey` | Yes | Yes |\n| [import.io](http://api.docs.import.io/) | Retrieve structured data from a website or RSS feed | `apiKey` | Yes | Unknown |\n| [IPify](https://www.ipify.org/) | A simple IP Address API | No | Yes | Unknown |\n| [IPinfo](https://ipinfo.io/developers) | Another simple IP Address API | No | Yes | Unknown |\n| [JSON 2 JSONP](https://json2jsonp.com/) | Convert JSON to JSONP (on-the-fly) for easy cross-domain data requests using client-side JavaScript | No | Yes | Unknown |\n| [JSONbin.io](https://jsonbin.io) | Free JSON storage service. Ideal for small scale Web apps, Websites and Mobile apps | `apiKey` | Yes | Yes |\n| [Judge0](https://api.judge0.com/) | Compile and run source code | No | Yes | Unknown |\n| [Let's Validate](https://github.com/letsvalidate/api) | Uncovers the technologies used on websites and URL to thumbnail | No | Yes | Unknown |\n| [License-API](https://github.com/cmccandless/license-api/blob/master/README.md) | Unofficial REST API for choosealicense.com | No | Yes | No |\n| [LiveEdu](https://www.liveedu.tv/developer/applications/) | Live Coding Streaming | `OAuth` | Yes | Unknown |\n| [MAC address vendor lookup](https://macaddress.io) | Retrieve vendor details and other information regarding a given MAC address or an OUI | `apiKey` | Yes | Yes |\n| [Myjson](http://myjson.com/api) | A simple JSON store for your web or mobile app | No | No | Unknown |\n| [OOPSpam](https://oopspam.com/) | Multiple spam filtering service | No | Yes | Yes |\n| [Plino](https://plino.herokuapp.com/) | Spam filtering system | No | Yes | Unknown |\n| [Postman](https://docs.api.getpostman.com/) | Tool for testing APIs | `apiKey` | Yes | Unknown |\n| [ProxyCrawl](https://proxycrawl.com) | Scraping and crawling anticaptcha service | `apiKey` | Yes | Unknown |\n| [Public APIs](https://github.com/davemachado/public-api) | A collective list of free JSON APIs for use in web development | No | Yes | Unknown |\n| [Pusher Beams](https://pusher.com/beams) | Push notifications for Android \u0026 iOS | `apiKey` | Yes | Unknown |\n| [QR code](http://qrtag.net/api/) | Create an easy to read QR code and URL shortener | No | Yes | Yes |\n| [QR code](http://goqr.me/api/) | Generate and decode / read QR code graphics | No | Yes | Unknown |\n| [ReqRes](https://reqres.in/ ) | A hosted REST-API ready to respond to your AJAX requests | No | Yes | Unknown |\n| [Scrape Website Email](https://market.mashape.com/tommytcchan/scrape-website-email) | Grabs email addresses from a URL | `X-Mashape-Key` | Yes | Unknown |\n| [ScraperApi](https://www.scraperapi.com) | Easily build scalable web scrapers | `apiKey` | Yes | Unknown |\n| [SHOUTCLOUD](http://shoutcloud.io/) | ALL-CAPS AS A SERVICE | No | No | Unknown |\n| [StackExchange](https://api.stackexchange.com/) | Q\u0026A forum for developers | `OAuth` | Yes | Unknown |\n| [Verse](https://verse.pawelad.xyz/) | Check what's the latest version of your favorite open-source project | No | Yes | Unknown |\n| [XML to JSON](https://developers.wso2apistore.com/) | Integration developer utility APIs | No | Yes | Unknown |\n\n### Dictionaries\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Merriam-Webster](https://dictionaryapi.com/) | Dictionary and Thesaurus Data | `apiKey` | Yes | Unknown |\n| [Oxford](https://developer.oxforddictionaries.com/) | Dictionary Data | `apiKey` | Yes | No |\n| [Wordnik](http://developer.wordnik.com) | Dictionary Data | `apiKey` | No | Unknown |\n| [Words](https://www.wordsapi.com/) | Definitions and synonyms for more than 150,000 words | `apiKey` | Yes | Unknown |\n\n### Documents \u0026 Productivity\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Cloudmersive Document and Data Conversion](https://cloudmersive.com/convert-api) | HTML/URL to PDF/PNG, Office documents to PDF, image conversion | `apiKey` | Yes | Yes |\n| [File.io](https://www.file.io) | File Sharing | No | Yes | Unknown |\n| [Mercury](https://mercury.postlight.com/web-parser/) | Web parser | `apiKey` | Yes | Unknown |\n| [pdflayer](https://pdflayer.com) | HTML/URL to PDF | `apiKey` | Yes | Unknown |\n| [Pocket](https://getpocket.com/developer/) | Bookmarking service | `OAuth` | Yes | Unknown |\n| [PrexView](https://prexview.com) | Data from XML or JSON to PDF, HTML or Image | `apiKey` | Yes | Unknown |\n| [Restpack](https://restpack.io/) | Provides screenshot, HTML to PDF and content extraction APIs | `apiKey` | Yes | Unknown |\n| [Todoist](https://developer.todoist.com) | Todo Lists | `OAuth` | Yes | Unknown |\n| [Vector Express](http://vector.express) | Free vector file converting API | No | No | Yes |\n| [Wunderlist](https://developer.wunderlist.com/documentation) | Todo Lists | `OAuth` | Yes | Unknown |\n\n### Environment\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [AirVisual](https://airvisual.com/api) | Air quality and weather data | `apiKey` | Yes | Unknown |\n| [OpenAQ](https://docs.openaq.org/) | Open air quality data | `apiKey` | Yes | Unknown |\n| [PM2.5.in](http://www.pm25.in/api_doc) | Air quality of China | `apiKey` | No | Unknown |\n| [PVWatts](https://developer.nrel.gov/docs/solar/pvwatts-v5/) | Energy production photovoltaic (PV) energy systems | `apiKey` | Yes | Unknown |\n| [UK Carbon Intensity](https://carbon-intensity.github.io/api-definitions/#carbon-intensity-api-v1-0-0) | The Official Carbon Intensity API for Great Britain developed by National Grid | No | Yes | Unknown |\n\n### Events\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Eventbrite](https://www.eventbrite.com/developer/v3/) | Find events | `OAuth` | Yes | Unknown |\n| [Picatic](http://developer.picatic.com/?utm_medium=web\u0026utm_source=github\u0026utm_campaign=public-apis%20repo\u0026utm_content=toddmotto) | Sell tickets anywhere | `apiKey` | Yes | Unknown |\n| [Ticketmaster](http://developer.ticketmaster.com/products-and-docs/apis/getting-started/) | Search events, attractions, or venues | `apiKey` | Yes | Unknown |\n\n### Finance\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Alpha Vantage](https://www.alphavantage.co/) | Realtime and historical stock data | `apiKey` | Yes | Unknown |\n| [Barchart OnDemand](https://www.barchartondemand.com/free) | Stock, Futures and Forex Market Data | `apiKey` | Yes | Unknown |\n| [Consumer Financial Protection Bureau](https://data.consumerfinance.gov/resource/jhzv-w97w.json) | Financial services consumer complaint data | `apiKey` | Yes | Unknown |\n| [Financial Modeling Prep](https://financialmodelingprep.com/) | Stock information and data | No | Yes | Unknown |\n| [IEX](https://iextrading.com/developer/) | Stocks and Market Data | No | Yes | Unknown |\n| [IG](https://labs.ig.com/gettingstarted) | Spreadbetting and CFD Market Data | `apiKey` | Yes | Unknown |\n| [Plaid](https://plaid.com/) | Connect with users’ bank accounts and access transaction data | `apiKey` | Yes | Unknown |\n| [Razorpay IFSC](https://ifsc.razorpay.com/) | Indian Financial Systems Code (Bank Branch Codes) | No | Yes | Unknown |\n| [RoutingNumbers.info](https://www.routingnumbers.info/api/index.html) | ACH/NACHA Bank Routing Numbers | No | Yes | Unknown |\n| [VAT Rates](https://jsonvat.com/) | A collection of all VAT rates for EU countries | No | Yes | Unknown |\n\n### Food \u0026 Drink\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Edamam](https://developer.edamam.com/) | Recipe Search | `apiKey` | Yes | Unknown |\n| [Food2Fork](http://food2fork.com/about/api) | Recipe Search | `apiKey` | No | Unknown |\n| [LCBO](https://lcboapi.com/) | Alcohol | `apiKey` | Yes | Unknown |\n| [Open Brewery DB](https://www.openbrewerydb.org) | Breweries, Cideries and Craft Beer Bottle Shops | No | Yes | Yes |\n| [Open Food Facts](https://world.openfoodfacts.org/data) | Food Products Database | No | Yes | Unknown |\n| [PunkAPI](https://punkapi.com/) | Brewdog Beer Recipes | No | Yes | Unknown |\n| [Recipe Puppy](http://www.recipepuppy.com/about/api/) | Food | No | No | Unknown |\n| [TacoFancy](https://github.com/evz/tacofancy-api) | Community-driven taco database | No | No | Unknown |\n| [The Report of the Week](https://github.com/andyklimczak/TheReportOfTheWeek-API) | Food \u0026 Drink Reviews | No | Yes | Unknown |\n| [TheCocktailDB](https://www.thecocktaildb.com/api.php) | Cocktail Recipes | `apiKey` | Yes | Yes |\n| [TheMealDB](https://www.themealdb.com/api.php) | Meal Recipes | `apiKey` | Yes | Yes |\n| [What's on the menu?](http://nypl.github.io/menus-api/) | NYPL human-transcribed historical menu collection | `apiKey` | No | Unknown |\n| [Yummly](https://developer.yummly.com/) | Find food recipes | `apiKey` | Yes | Unknown |\n| [Zomato](https://developers.zomato.com/api) | Discover restaurants | `apiKey` | Yes | Unknown |\n\n\n### Fraud Prevention\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Whitepages Pro](https://pro.whitepages.com/developer/documentation/identity-check-api/) | Global identity verification with phone, address, email and IP | `apiKey` | Yes | Unknown |\n| [Whitepages Pro](https://pro.whitepages.com/developer/documentation/phone-reputation-api/) | Phone reputation to detect spammy phones | `apiKey` | Yes | Unknown |\n| [Whitepages Pro](https://pro.whitepages.com/developer/documentation/reverse-phone-api/) | Get an owner’s name, address, demographics based on the phone number | `apiKey` | Yes | Unknown |\n| [Whitepages Pro](https://pro.whitepages.com/developer/documentation/phone-intelligence-api/) | Phone number validation, line_type, carrier append | `apiKey` | Yes | Unknown |\n| [Whitepages Pro](https://pro.whitepages.com/developer/documentation/reverse-address-api/) | Get normalized physical address, residents, address type and validity | `apiKey` | Yes | Unknown |\n\n### Games \u0026 Comics\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [AmiiboAPI](http://www.amiiboapi.com/) | Amiibo Information | No | No | Yes |\n| [Battle.net](https://dev.battle.net/) | Blizzard Entertainment | `apiKey` | Yes | Unknown |\n| [Battlefield 4](https://bf4stats.com/api) | Battlefield 4 Information | No | Yes | Unknown |\n| [Chuck Norris Database](http://www.icndb.com/api/) | Jokes | No | No | Unknown |\n| [Clash of Clans](https://developer.clashofclans.com) | Clash of Clans Game Information | `apiKey` | Yes | Unknown |\n| [Clash Royale](https://github.com/martincarrera/clash-royale-api) | Clash Royale Game Information | No | Yes | Unknown |\n| [Comic Vine](https://comicvine.gamespot.com/api/documentation) | Comics | No | Yes | Unknown |\n| [Deck of Cards](http://deckofcardsapi.com/) | Deck of Cards | No | No | Unknown |\n| [Destiny The Game](https://github.com/Bungie-net/api) | Bungie Platform API | `apiKey` | Yes | Unknown |\n| [Dota 2](https://docs.opendota.com/) | Provides information about Player stats , Match stats, Rankings for Dota 2 | No | Yes | Unknown |\n| [Eve Online](https://esi.evetech.net/ui) | Third-Party Developer Documentation | `OAuth` | Yes | Unknown |\n| [Fortnite](https://fortnitetracker.com/site-api) | Fortnite Stats | `apiKey` | Yes | Unknown |\n| [Games](https://docs.gameapis.net/) | Minecraft and other server info \u0026 user info) | No | Yes | Unknown |\n| [Giant Bomb](https://www.giantbomb.com/api/documentation) | Video Games | No | Yes | Unknown |\n| [Guild Wars 2](https://wiki.guildwars2.com/wiki/API:Main) | Guild Wars 2 Game Information | `apiKey` | Yes | Unknown |\n| [Halo](https://developer.haloapi.com/) | Halo 5 and Halo Wars 2 Information | `apiKey` | Yes | Unknown |\n| [Hearthstone](http://hearthstoneapi.com/) | Hearthstone Cards Information | `X-Mashape-Key` | Yes | Unknown |\n| [IGDB.com](https://api.igdb.com/) | Video Game Database | `apiKey` | Yes | Unknown |\n| [Jokes](https://github.com/15Dkatz/official_joke_api) | Programming and general jokes | No | Yes | Unknown |\n| [Jservice](http://jservice.io) | Jeopardy Question Database | No | No | Unknown |\n| [Magic The Gathering](http://magicthegathering.io/) | Magic The Gathering Game Information | No | No | Unknown |\n| [Marvel](http://developer.marvel.com) | Marvel Comics | `apiKey` | No | Unknown |\n| [Open Trivia](https://opentdb.com/api_config.php) | Trivia Questions | No | Yes | Unknown |\n| [PandaScore](https://api.pandascore.co) | E-sports games and results | `apiKey` | Yes | Unknown |\n| [PlayerUnknown's Battlegrounds](https://pubgtracker.com/site-api) | PUBG Stats | `apiKey` | Yes | Unknown |\n| [Pokéapi](https://pokeapi.co) | Pokémon Information | No | Yes | Unknown |\n| [Pokémon TCG](https://pokemontcg.io) | Pokémon TCG Information | No | Yes | Unknown |\n| [Qriusity](https://qriusity.com/) | Quiz/Trivia Questions | No | Yes | Unknown |\n| [Rick and Morty](https://rickandmortyapi.com) | All the Rick and Morty information, including images | No | Yes | Yes |\n| [Riot Games](https://developer.riotgames.com/) | League of Legends Game Information | `apiKey` | Yes | Unknown |\n| [Steam](https://developer.valvesoftware.com/wiki/Steam_Web_API) | Steam Client Interaction | `OAuth` | Yes | Unknown |\n| [Vainglory](https://developer.vainglorygame.com/) | Vainglory Players, Matches and Telemetry | `apiKey` | Yes | Yes |\n| [Wargaming.net](https://developers.wargaming.net/) | Wargaming.net info and stats | `apiKey` | Yes | No |\n| [xkcd](https://xkcd.com/json.html) | Retrieve xkcd comics as JSON | No | Yes | Yes |\n\n### Geocoding\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [adresse.data.gouv.fr](https://adresse.data.gouv.fr) | Address database of France, geocoding and reverse | No | Yes | Unknown |\n| [Battuta](https://battuta.medunes.net) | A (country/region/city) in-cascade location API | `apiKey` | Yes | Unknown |\n| [Bing Maps](https://www.microsoft.com/maps/) | Create/customize digital maps based on Bing Maps data | `apiKey` | Yes | Unknown |\n| [City Context](https://www.citycontext.com/api-reference#/) | Crime, school and transportation data for US cities | `apiKey` | Yes | Unknown |\n| [CitySDK](http://www.citysdk.eu/citysdk-toolkit/) | Open APIs for select European cities | No | Yes | Unknown |\n| [Daum Maps](http://apis.map.daum.net/) | Daum Maps provide multiple APIs for Korean maps | `apiKey` | No | Unknown |\n| [GeoApi](https://api.gouv.fr/api/geoapi.html) | French geographical data | No | Yes | Unknown |\n| [Geocod.io](https://www.geocod.io/) | Address geocoding / reverce geocoding in bulk | `apiKey` | Yes | Unknown |\n| [Geocode.xyz](https://geocode.xyz/) | Provides worldwide forward/reverse geocoding, batch geocoding and geoparsing | No | Yes | Unknown |\n| [GeoJS](https://geojs.io/) | IP geolocation with ChatOps integration | No | Yes | Yes |\n| [GeoNames](http://www.geonames.org/export/web-services.html) | Place names and other geographical data | No | No | Unknown |\n| [geoPlugin](https://www.geoplugin.com) | IP geolocation and currency conversion | No | Yes | Yes |\n| [Google Earth Engine](https://developers.google.com/earth-engine/) | A cloud-based platform for planetary-scale environmental data analysis | `apiKey` | Yes | Unknown |\n| [Google Maps](https://developers.google.com/maps/) | Create/customize digital maps based on Google Maps data | `apiKey` | Yes | Unknown |\n| [GraphLoc](https://www.graphloc.com) | Free GraphQL IP Geolocation API | No | Yes | Unknown |\n| [HelloSalut](https://www.fourtonfish.com/hellosalut/hello/) | Get hello translation following user language | No | Yes | Unknown |\n| [HERE Maps](https://developer.here.com) | Create/customize digital maps based on HERE Maps data | `apiKey` | Yes | Unknown |\n| [IP 2 Country](https://ip2country.info) | Map an IP to a country | No | Yes | Unknown |\n| [IP Address Details](https://ipinfo.io/) | Find geolocation with ip address | No | Yes | Unknown |\n| [IP Location](https://ipapi.co/) | Find IP address location information | No | Yes | Unknown |\n| [IP Sidekick](https://ipsidekick.com) | Geolocation API that returns extra information about an IP address | `apiKey` | Yes | Unknown |\n| [IP Vigilante](https://www.ipvigilante.com/) | Free IP Geolocation API | No | Yes | Unknown |\n| [IPGeolocationAPI.com](https://ipgeolocationapi.com/) | Locate your visitors by IP with country details | No | Yes | Yes |\n| [ipstack](https://ipstack.com/) | Locate and identify website visitors by IP address | `apiKey` | Yes | Unknown |\n| [LocationIQ](https://locationiq.org/docs/) | Provides forward/reverse geocoding and batch geocoding | `apiKey` | Yes | Yes |\n| [Mapbox](https://www.mapbox.com/developers/) | Create/customize beautiful digital maps | `apiKey` | Yes | Unknown |\n| [Mexico](https://github.com/IcaliaLabs/sepomex) | Mexico RESTful zip codes API | No | Yes | Unknown |\n| [One Map, Singapore](https://docs.onemap.sg/) | Singapore Land Authority REST API services for Singapore addresses | `apiKey` | Yes | Unknown |\n| [OnWater](https://onwater.io/) | Determine if a lat/lon is on water or land | No | Yes | Unknown |\n| [OpenCage](https://geocoder.opencagedata.com) | Forward and reverse geocoding using open data | No | Yes | Unknown |\n| [OpenStreetMap](http://wiki.openstreetmap.org/wiki/API) | Navigation, geolocation and geographical data | `OAuth` | No | Unknown |\n| [PostcodeData.nl](http://api.postcodedata.nl/v1/postcode/?postcode=1211EP\u0026streetnumber=60\u0026ref=domeinnaam.nl\u0026type=json) | Provide geolocation data based on postcode for Dutch addresses | No | No | Unknown |\n| [Postcodes.io](https://postcodes.io) | Postcode lookup \u0026 Geolocation for the UK | No | Yes | Unknown |\n| [REST Countries](https://restcountries.eu) | Get information about countries via a RESTful API | No | Yes | Unknown |\n| [Uebermaps](https://uebermaps.com/api/v2) | Discover and share maps with friends | `apiKey` | Yes | Unknown |\n| [Utah AGRC](https://api.mapserv.utah.gov) | Utah Web API for geocoding Utah addresses | `apiKey` | Yes | Unknown |\n| [ViaCep](https://viacep.com.br) | Brazil RESTful zip codes API | No | Yes | Unknown |\n| [Zipcodeapi](https://www.zipcodeapi.com) | Find out possible zip codes for a city, distance between zip codes etc | `apiKey` | Yes | Unknown |\n| [Zippopotam](http://www.zippopotam.us) | Get information about place such as country, city, state, etc | No | No | Unknown |\n\n### Government\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [BCLaws](http://www.bclaws.ca/civix/template/complete/api/index.html) | Access to the laws of British Columbia | No | No | Unknown |\n| [BusinessUSA](https://business.usa.gov/developer) | Authoritative information on U.S. programs, events, services and more | `apiKey` | Yes | Unknown |\n| [Census.gov](https://www.census.gov/data/developers/data-sets.html) | The US Census Bureau provides various APIs and data sets on demographics and businesses | No | Yes | Unknown |\n| [Colorado Data Engine](http://codataengine.org/) | Formatted and geolocated Colorado public data | No | Yes | Unknown |\n| [Colorado Information Marketplace](https://data.colorado.gov/) | Colorado State Government Open Data | No | Yes | Unknown |\n| [Data USA](https://datausa.io/about/api/) | US Public Data | No | Yes | Unknown |\n| [Data.gov](https://api.data.gov/) | US Government Data | `apiKey` | Yes | Unknown |\n| [Data.parliament.uk](http://www.data.parliament.uk/developers/) | Contains live datasets including information about petitions, bills, MP votes, attendence and more | No | No | Unknown |\n| [EPA](https://developer.epa.gov/category/api/) | Web services and data sets from the US Environmental Protection Agency | No | Yes | Unknown |\n| [FEC](https://api.open.fec.gov/developers/) | Information on campaign donations in federal elections | `apiKey` | Yes | Unknown |\n| [Federal Register](https://www.federalregister.gov/reader-aids/developer-resources) | The Daily Journal of the United States Government | No | Yes | Unknown |\n| [Food Standards Agency](http://ratings.food.gov.uk/open-data/en-GB) | UK food hygiene rating data API | No | No | Unknown |\n| [Open Government, Australia](https://www.data.gov.au/) | Australian Government Open Data | No | Yes | Unknown |\n| [Open Government, Belgium](https://data.gov.be/) | Belgium Government Open Data | No | Yes | Unknown |\n| [Open Government, Canada](http://open.canada.ca/en) | Canadian Government Open Data | No | No | Unknown |\n| [Open Government, France](https://www.data.gouv.fr/) | French Government Open Data | `apiKey` | Yes | Unknown |\n| [Open Government, India](https://data.gov.in/) | Indian Government Open Data | `apiKey` | Yes | Unknown |\n| [Open Government, New Zealand](https://www.data.govt.nz/) | New Zealand Government Open Data | No | Yes | Unknown |\n| [Open Government, Taiwan](https://data.gov.tw/) | Taiwan Government Open Data | No | Yes | Unknown |\n| [Open Government, USA](https://www.data.gov/) | United States Government Open Data | No | Yes | Unknown |\n| [Prague Opendata](http://opendata.praha.eu/en) | Prague City Open Data | No | No | Unknown |\n| [Regulations.gov](https://regulationsgov.github.io/developers/) | Federal regulatory materials to increase understanding of the Federal rule making process | `apiKey` | Yes | Unknown |\n| [Represent by Open North](https://represent.opennorth.ca/) | Find Canadian Government Representatives | No | Yes | Unknown |\n| [USAspending.gov](https://api.usaspending.gov/) | US federal spending data | No | Yes | Unknown |\n\n### Health\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [BetterDoctor](https://developer.betterdoctor.com/) | Detailed information about doctors in your area | `apiKey` | Yes | Unknown |\n| [Diabetes](http://predictbgl.com/api/) | Logging and retrieving diabetes information | No | No | Unknown |\n| [Flutrack](http://www.flutrack.org/) | Influenza-like symptoms with geotracking | No | No | Unknown |\n| [Healthcare.gov](https://www.healthcare.gov/developers/) | Educational content about the US Health Insurance Marketplace | No | Yes | Unknown |\n| [Lexigram](https://docs.lexigram.io/v1/welcome) | NLP that extracts mentions of clinical concepts from text, gives access to clinical ontology | `apiKey` | Yes | Unknown |\n| [Makeup](http://makeup-api.herokuapp.com/) | Makeup Information | No | No | Unknown |\n| [Medicare](https://data.medicare.gov/developers) | Access to the data from the CMS - medicare.gov | No | Yes | Unknown |\n| [NPPES](https://npiregistry.cms.hhs.gov/registry/help-api) | National Plan \u0026 Provider Enumeration System, info on healthcare providers registered in US | No | Yes | Unknown |\n| [Nutritionix](https://developer.nutritionix.com/) | Worlds largest verified nutrition database | `apiKey` | Yes | Unknown |\n| [openFDA](https://open.fda.gov) | Public FDA data about drugs, devices and foods | No | Yes | Unknown |\n| [USDA Nutrients](https://ndb.nal.usda.gov/ndb/doc/index) | National Nutrient Database for Standard Reference | No | Yes | Unknown |\n\n### Jobs\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Adzuna](https://developer.adzuna.com/overview) | Job board aggregator | `apiKey` | Yes | Unknown |\n| [Authentic Jobs](https://authenticjobs.com/api/docs) | Job board for designers, hackers and creative pros | `apiKey` | Yes | Unknown |\n| [Careerjet](https://www.careerjet.com/partners/api/) | Job search engine | `apiKey` | No | Unknown |\n| [Github Jobs](https://jobs.github.com/api) | Jobs for software developers | No | Yes | Unknown |\n| [Indeed](https://www.indeed.com/publisher) | Job board aggregator | `apiKey` | Yes | Unknown |\n| [Jobs2Careers](http://api.jobs2careers.com/api/spec.pdf) | Job aggregator | `apiKey` | Yes | Unknown |\n| [Jooble](https://us.jooble.org/api/about) | Job search engine | `apiKey` | Yes | Unknown |\n| [Juju](http://www.juju.com/publisher/spec/) | Job search engine | `apiKey` | No | Unknown |\n| [Open Skills](https://github.com/workforce-data-initiative/skills-api/wiki/API-Overview) | Job titles, skills and related jobs data | No | No | Unknown |\n| [Reed](https://www.reed.co.uk/developers) | Job board aggregator | `apiKey` | Yes | Unknown |\n| [Search.gov Jobs](https://search.gov/developer/jobs.html) | Tap into a list of current jobs openings with the United States government | No | Yes | Unknown |\n| [The Muse](https://www.themuse.com/developers/api/v2) | Job board and company profiles | `apiKey` | Yes | Unknown |\n| [Upwork](https://developers.upwork.com/) | Freelance job board and management system | `OAuth` | Yes | Unknown |\n| [USAJOBS](https://developer.usajobs.gov/) | US government job board | `apiKey` | Yes | Unknown |\n| [ZipRecruiter](https://www.ziprecruiter.com/publishers) | Job search app and website | `apiKey` | Yes | Unknown |\n\n### Machine Learning\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Clarifai](https://developer.clarifai.com/) | Computer Vision | `OAuth` | Yes | Unknown |\n| [Cloudmersive](https://www.cloudmersive.com/image-recognition-and-processing-api) | Image captioning, face recognition, NSFW classification | `apiKey` | Yes | Yes |\n| [Dialogflow](https://dialogflow.com) | Natural Language Processing | `apiKey` | Yes | Unknown |\n| [Keen IO](https://keen.io/) | Data Analytics | `apiKey` | Yes | Unknown |\n| [Unplugg](https://unplu.gg/test_api.html) | Forecasting API for timeseries data | `apiKey` | Yes | Unknown |\n| [Wit.ai](https://wit.ai/) | Natural Language Processing | `OAuth` | Yes | Unknown |\n\n### Music\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [AI Mastering](https://aimastering.com/api_docs/) | Automated Music Mastering | `apiKey` | Yes | Yes |\n| [Bandsintown](https://app.swaggerhub.com/apis/Bandsintown/PublicAPI/3.0.0) | Music Events | No | Yes | Unknown |\n| [Deezer](https://developers.deezer.com/api) | Music | `OAuth` | Yes | Unknown |\n| [Discogs](https://www.discogs.com/developers/) | Music | `OAuth` | Yes | Unknown |\n| [Genius](https://docs.genius.com/) | Crowdsourced lyrics and music knowledge | `OAuth` | Yes | Unknown |\n| [Genrenator](https://binaryjazz.us/genrenator-api/) | Music genre generator | No | Yes | Unknown |\n| [iTunes Search](https://affiliate.itunes.apple.com/resources/documentation/itunes-store-web-service-search-api/) | Software products | No | Yes | Unknown |\n| [Jamendo](https://developer.jamendo.com/v3.0) | Music | `OAuth` | Yes | Unknown |\n| [LastFm](https://www.last.fm/api) | Music | `apiKey` | Yes | Unknown |\n| [Lyrics.ovh](http://docs.lyricsovh.apiary.io/) | Simple API to retrieve the lyrics of a song | No | Yes | Unknown |\n| [Mixcloud](https://www.mixcloud.com/developers/) | Music | `OAuth` | Yes | Yes |\n| [MusicBrainz](https://musicbrainz.org/doc/Development/XML_Web_Service/Version_2) | Music | No | Yes | Unknown |\n| [Musikki](https://music-api.musikki.com/reference) | Music | `apiKey` | Yes | Unknown |\n| [Musixmatch](https://developer.musixmatch.com/) | Music | `apiKey` | Yes | Unknown |\n| [Openwhyd](https://openwhyd.github.io/openwhyd/API) | Download curated playlists of streaming tracks (YouTube, SoundCloud, etc...) | `No` | Yes | No |\n| [Songkick](https://www.songkick.com/developer/) | Music Events | `OAuth` | Yes | Unknown |\n| [Songsterr](https://www.songsterr.com/a/wa/api/) | Provides guitar, bass and drums tabs and chords | No | Yes | Unknown |\n| [SoundCloud](https://developers.soundcloud.com/) | Allow users to upload and share sounds | `OAuth` | Yes | Unknown |\n| [Spotify](https://beta.developer.spotify.com/documentation/web-api/) | View Spotify music catalog, manage users' libraries, get recommendations and more | `OAuth` | Yes | Unknown |\n| [TasteDive](https://tastedive.com/read/api) | Similar artist API (also works for movies and TV shows) | `apiKey` | Yes | Unknown |\n| [TheAudioDB](http://www.theaudiodb.com) | Music | `apiKey` | No | Unknown |\n| [Vagalume](https://api.vagalume.com.br/docs/) | Crowdsourced lyrics and music knowledge | `apiKey` | Yes | Unknown |\n\n### News\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Chronicling America](http://chroniclingamerica.loc.gov/about/api/) | Provides access to millions of pages of historic US newspapers from the Library of Congress | No | No | Unknown |\n| [Currents](https://currentsapi.services/) | Latest news published in various news sources, blogs and forums | `apiKey` | Yes | Yes |\n| [Feedbin](https://github.com/feedbin/feedbin-api) | RSS reader | `OAuth` | Yes | Unknown |\n| [Feedster](https://api.feedster.me/v1/docs/) | Searchable and categorized collections of RSS feeds | `apiKey` | Yes | Unknown |\n| [New York Times](https://developer.nytimes.com/) | Provides news | `apiKey` | Yes | Unknown |\n| [News](https://newsapi.org/) | Headlines currently published on a range of news sources and blogs | `apiKey` | Yes | Unknown |\n| [NPR One](http://dev.npr.org/api/) | Personalized news listening experience from NPR | `OAuth` | Yes | Unknown |\n| [The Guardian](http://open-platform.theguardian.com/) | Access all the content the Guardian creates, categorised by tags and section | `apiKey` | Yes | Unknown |\n| [The Old Reader](https://github.com/theoldreader/api) | RSS reader | `apiKey` | Yes | Unknown |\n\n### Open Data\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [18F](http://18f.github.io/API-All-the-X/) | Unofficial US Federal Government API Development | No | No | Unknown |\n| [Abbreviation](https://market.mashape.com/daxeel/abbreviations) | Get abbreviations and meanings | `X-Mashape-Key` | Yes | Unknown |\n| [Archive.org](https://archive.readme.io/docs) | The Internet Archive | No | Yes | Unknown |\n| [Callook.info](https://callook.info) | United States ham radio callsigns | No | Yes | Unknown |\n| [CARTO](https://carto.com/) | Location Information Prediction | `apiKey` | Yes | Unknown |\n| [Celebinfo](https://market.mashape.com/daxeel/celebinfo/) | Celebrity information | `X-Mashape-Key` | Yes | Unknown |\n| [Datakick](https://www.datakick.org/api) | The open product database | `apiKey` | Yes | Unknown |\n| [Dronestream](https://dronestre.am/) | Tracks United States drone strikes | No | Yes | Unknown |\n| [Enigma Public](http://docs.enigma.com/public/public_v20_api_about) | Broadest collection of public data | `apiKey` | Yes | Yes |\n| [fonoApi](https://fonoapi.freshpixl.com/) | Mobile Device Description | No | Yes | Unknown |\n| [French Address Search](https://adresse.data.gouv.fr/api) | Address search via the French Government | No | Yes | Unknown |\n| [LinkPreview](https://www.linkpreview.net) | Get JSON formatted summary with title, description and preview image for any requested URL | `apiKey` | Yes | Yes |\n| [Marijuana Strains](http://strains.evanbusse.com/) | Marijuana strains, races, flavors and effects | `apiKey` | No | Unknown |\n| [Microlink.io](https://microlink.io) | Extract structured data from any website | No | Yes | Yes |\n| [Population.io](http://api.population.io/) | Open demographic data such as population tables, population rank and life expectancy | No | No | Unknown |\n| [Quandl](https://www.quandl.com/) | Stock Market Data | No | Yes | Unknown |\n| [Scoop.it](http://www.scoop.it/dev) | Content Curation Service | `apiKey` | No | Unknown |\n| [Teleport](https://developers.teleport.org/) | Quality of Life Data | No | Yes | Unknown |\n| [Universities List](https://github.com/Hipo/university-domains-list) | University names, countries and domains | No | Yes | Unknown |\n| [University of Oslo](https://data.uio.no/) | Courses, lecture videos, detailed information for courses etc. for the University of Oslo (Norway) | No | Yes | Unknown |\n| [UPC database](https://upcdatabase.org/api) | More than 1.5 million barcode numbers from all around the world | `apiKey` | Yes | Unknown |\n| [Wikidata](https://www.wikidata.org/w/api.php?action=help) | Collaboratively edited knowledge base operated by the Wikimedia Foundation | `OAuth` | Yes | Unknown |\n| [Wikipedia](https://www.mediawiki.org/wiki/API:Main_page) | Mediawiki Encyclopedia | No | Yes | Unknown |\n| [Yelp](https://www.yelp.com/developers/documentation/v3) | Find Local Business | `OAuth` | Yes | Unknown |\n\n### Open Source Projects\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Countly](http://resources.count.ly/docs) | Countly web analytics | No | No | Unknown |\n| [Drupal.org](https://www.drupal.org/drupalorg/docs/api) | Drupal.org | No | Yes | Unknown |\n| [Libraries.io](https://libraries.io/api) | Open source software libraries | `apiKey` | Yes | Unknown |\n\n### Patent\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [EPO](https://developers.epo.org/) | European patent search system api | `OAuth` | Yes | Unknown |\n| [TIPO](https://tiponet.tipo.gov.tw/Gazette/OpenData/OD/OD05.aspx?QryDS=API00) | Taiwan patent search system api | `apiKey` | Yes | Unknown |\n| [USPTO](https://www.uspto.gov/learning-and-resources/open-data-and-mobility) | USA patent api services | No | Yes | Unknown |\n\n### Personality\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Advice Slip](http://api.adviceslip.com/) | Generate random advice slips | No | Yes | Unknown |\n| [chucknorris.io](https://api.chucknorris.io) | JSON API for hand curated Chuck Norris jokes | No | Yes | Unknown |\n| [FavQs.com](https://favqs.com/api) | FavQs allows you to collect, discover and share your favorite quotes | `apiKey` | Yes | Unknown |\n| [Forismatic](http://forismatic.com/en/api/) | Inspirational Quotes | No | No | Unknown |\n| [icanhazdadjoke](https://icanhazdadjoke.com/api) | The largest selection of dad jokes on the internet | No | Yes | Unknown |\n| [Medium](https://github.com/Medium/medium-api-docs) | Community of readers and writers offering unique perspectives on ideas | `OAuth` | Yes | Unknown |\n| [Quotes on Design](https://quotesondesign.com/api-v4-0/) | Inspirational Quotes | No | Yes | Unknown |\n| [Traitify](https://app.traitify.com/developer) | Assess, collect and analyze Personality | No | Yes | Unknown |\n| [tronalddump.io](https://www.tronalddump.io) | Api \u0026 web archive for the things Donald Trump has said | No | Yes | Unknown |\n\n### Photography\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Flickr](https://www.flickr.com/services/api/) | Flickr Services | `OAuth` | Yes | Unknown |\n| [Getty Images](http://developers.gettyimages.com/en/) | Build applications using the world's most powerful imagery | `OAuth` | Yes | Unknown |\n| [Gfycat](https://developers.gfycat.com/api/) | Jiffier GIFs | `OAuth` | Yes | Unknown |\n| [Giphy](https://developers.giphy.com/docs/) | Get all your gifs | `apiKey` | Yes | Unknown |\n| [Gyazo](https://gyazo.com/api/docs) | Upload images | `apiKey` | Yes | Unknown |\n| [Imgur](https://apidocs.imgur.com/) | Images | `OAuth` | Yes | Unknown |\n| [Lorem Picsum](https://picsum.photos/) | Images from Unsplash | No | Yes | Unknown |\n| [Pixabay](https://pixabay.com/sk/service/about/api/) | Photography | `apiKey` | Yes | Unknown |\n| [Pixhost](https://pixhost.org/api/index.html) | Upload images, photos, galleries | No | Yes | Unknown |\n| [PlaceKitten](https://placekitten.com/) | Resizable kitten placeholder images | No | Yes | Unknown |\n| [ScreenShotLayer](https://screenshotlayer.com) | URL 2 Image | No | Yes | Unknown |\n| [Unsplash](https://unsplash.com/developers) | Photography | `OAuth` | Yes | Unknown |\n\n### Science \u0026 Math\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [arcsecond.io](https://api.arcsecond.io/) | Multiple astronomy data sources | No | Yes | Unknown |\n| [CORE](https://core.ac.uk/services#api) | Access the world's Open Access research papers | `apiKey` | Yes | Unknown |\n| [inspirehep.net](https://inspirehep.net/info/hep/api?ln=en) | High Energy Physics info. system | No | Yes | Unknown |\n| [Launch Library](https://launchlibrary.net/docs/1.3/api.html) | Upcoming Space Launches | No | Yes | Unknown |\n| [Minor Planet Center](http://www.asterank.com/mpc) | Asterank.com Information | No | No | Unknown |\n| [NASA](https://api.nasa.gov) | NASA data, including imagery | No | Yes | Unknown |\n| [Newton](https://newton.now.sh/) | Symbolic and Arithmetic Math Calculator | No | Yes | Unknown |\n| [Numbers](http://numbersapi.com) | Facts about numbers | No | No | Unknown |\n| [Open Notify](http://open-notify.org/Open-Notify-API/) | ISS astronauts, current location, etc | No | No | Unknown |\n| [Open Science Framework](https://developer.osf.io) | Repository and archive for study designs, research materials, data, manuscripts, etc | No | Yes | Unknown |\n| [SHARE](https://share.osf.io/api/v2/) | A free, open, dataset about research and scholarly activities | No | Yes | Unknown |\n| [SpaceX](https://github.com/r-spacex/SpaceX-API) | Company, vehicle, launchpad and launch data | No | Yes | Unknown |\n| [Sunrise and Sunset](https://sunrise-sunset.org/api) | Sunset and sunrise times for a given latitude and longitude | No | Yes | Unknown |\n| [USGS Earthquake Hazards Program](https://earthquake.usgs.gov/fdsnws/event/1/) | Earthquakes data real-time | No | Yes | Unknown |\n| [USGS Water Services](https://waterservices.usgs.gov/) | Water quality and level info for rivers and lakes | No | Yes | Unknown |\n| [World Bank](https://datahelpdesk.worldbank.org/knowledgebase/topics/125589) | World Data | No | No | Unknown |\n\n### Security\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [AXFR Database](http://api.axfrcheck.com) | AXFR public database | No | No | Unknown |\n| [FilterLists](https://filterlists.com/api) | Lists of filters for adblockers and firewalls | No | Yes | Unknown |\n| [HaveIBeenPwned](https://haveibeenpwned.com/API/v2) | Passwords which have previously been exposed in data breaches | No | Yes | Unknown |\n| [National Vulnerability Database](https://nvd.nist.gov/vuln/Data-Feeds/JSON-feed-changelog) | U.S. National Vulnerability Database | No | Yes | Unknown |\n| [SecurityTrails](https://securitytrails.com/corp/apidocs) | Domain and IP related information such as current and historical WHOIS and DNS records | `apiKey` | Yes | Unknown |\n| [Shodan](https://developer.shodan.io/) | Search engine for Internet connected devices | `apiKey` | Yes | Unknown |\n| [UK Police](https://data.police.uk/docs/) | UK Police data | No | Yes | Unknown |\n\n### Shopping\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Best Buy](https://bestbuyapis.github.io/api-documentation/#overview) | Products, Buying Options, Categories, Recommendations, Stores and Commerce | `apiKey` | Yes | Unknown |\n| [eBay](https://go.developer.ebay.com/) | Sell and Buy on eBay | `OAuth` | Yes | Unknown |\n| [Wal-Mart](https://developer.walmartlabs.com/docs) | Item price and availability | `apiKey` | Yes | Unknown |\n| [Wegmans](https://dev.wegmans.io) | Wegmans Food Markets | `apiKey` | Yes | Unknown |\n\n### Social\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Buffer](https://buffer.com/developers/api) | Access to pending and sent updates in Buffer | `OAuth` | Yes | Unknown |\n| [Cisco Spark](https://developer.ciscospark.com) | Team Collaboration Software | `OAuth` | Yes | Unknown |\n| [Discord](https://discordapp.com/developers/docs/intro) | Make bots for Discord, integrate Discord onto an external platform | `OAuth` | Yes | Unknown |\n| [Disqus](https://disqus.com/api/docs/auth/) | Communicate with Disqus data | `OAuth` | Yes | Unknown |\n| [Facebook](https://developers.facebook.com/) | Facebook Login, Share on FB, Social Plugins, Analytics and more | `OAuth` | Yes | Unknown |\n| [Foursquare](https://developer.foursquare.com/) | Interact with Foursquare users and places (geolocation-based checkins, photos, tips, events, etc) | `OAuth` | Yes | Unknown |\n| [Fuck Off as a Service](https://www.foaas.com) | Asks someone to fuck off | No | Yes | Unknown |\n| [Full Contact](https://www.fullcontact.com/developer/docs/) | Get Social Media profiles and contact Information | `OAuth` | Yes | Unknown |\n| [HackerNews](https://github.com/HackerNews/API) | Social news for CS and entrepreneurship | No | Yes | Unknown |\n| [Instagram](https://www.instagram.com/developer/) | Instagram Login, Share on Instagram, Social Plugins and more | `OAuth` | Yes | Unknown |\n| [LinkedIn](https://developer.linkedin.com/docs/rest-api) | The foundation of all digital integrations with LinkedIn | `OAuth` | Yes | Unknown |\n| [Meetup.com](https://www.meetup.com/meetup_api/) | Data about Meetups from Meetup.com | `apiKey` | Yes | Unknown |\n| [MySocialApp](https://mysocialapp.io) | Seamless Social Networking features, API, SDK to any app | `apiKey` | Yes | Unknown |\n| [Pinterest](https://developers.pinterest.com/) | The world's catalog of ideas | `OAuth` | Yes | Unknown |\n| [PWRTelegram bot](https://pwrtelegram.xyz) | Boosted version of the Telegram bot API | `OAuth` | Yes | Unknown |\n| [Reddit](https://www.reddit.com/dev/api) | Homepage of the internet | `OAuth` | Yes | Unknown |\n| [SharedCount](http://docs.sharedcount.com/) | Social media like and share data for any URL | `apiKey` | Yes | Unknown |\n| [Slack](https://api.slack.com/) | Team Instant Messaging | `OAuth` | Yes | Unknown |\n| [Telegram Bot](https://core.telegram.org/bots/api) | Simplified HTTP version of the MTProto API for bots | `OAuth` | Yes | Unknown |\n| [Telegram MTProto](https://core.telegram.org/api#getting-started) | Read and write Telegram data | `OAuth` | Yes | Unknown |\n| [Trash Nothing](https://trashnothing.com/developer) | A freecycling community with thousands of free items posted every day | `OAuth` | Yes | Yes |\n| [Tumblr](https://www.tumblr.com/docs/en/api/v2) | Read and write Tumblr Data | `OAuth` | Yes | Unknown |\n| [Twitch](https://dev.twitch.tv/docs) | Game Streaming API | `OAuth` | Yes | Unknown |\n| [Twitter](https://developer.twitter.com/en/docs) | Read and write Twitter data | `OAuth` | Yes | No |\n| [vk](https://vk.com/dev/sites) | Read and write vk data | `OAuth` | Yes | Unknown |\n\n### Sports \u0026 Fitness\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [BikeWise](https://www.bikewise.org/documentation/api_v2) | Bikewise is a place to learn about and report bike crashes, hazards and thefts | No | Yes | Unknown |\n| [Cartola FC](https://github.com/wgenial/cartrolandofc) | The Cartola FC API serves to check the partial points of your team | No | Yes | Unknown |\n| [City Bikes](http://api.citybik.es/v2/) | City Bikes around the world | No | No | Unknown |\n| [Cricket Live Scores](https://market.mashape.com/dev132/cricket-live-scores) | Live cricket scores | `X-Mashape-Key` | Yes | Unknown |\n| [Ergast F1](http://ergast.com/mrd/) | F1 data from the beginning of the world championships in 1950 | No | Yes | Unknown |\n| [Fitbit](https://dev.fitbit.com/) | Fitbit Information | `OAuth` | Yes | Unknown |\n| [Football-Data.org](http://api.football-data.org/index) | Football Data | No | No | Unknown |\n| [JCDecaux Bike](https://developer.jcdecaux.com/) | JCDecaux's self-service bicycles | `apiKey` | Yes | Unknown |\n| [NBA Stats](https://any-api.com/nba_com/nba_com/docs/API_Description) | Current and historical NBA Statistics | No | Yes | Unknown |\n| [NFL Arrests](http://nflarrest.com/api/) | NFL Arrest Data | No | No | Unknown |\n| [Pro Motocross](http://promotocrossapi.com) | The RESTful AMA Pro Motocross lap times for every racer on the start gate | No | No | Unknown |\n| [Strava](https://strava.github.io/api/) | Connect with athletes, activities and more | `OAuth` | Yes | Unknown |\n| [SuredBits](https://suredbits.com/api/) | Query sports data, including teams, players, games, scores and statistics | No | No | No |\n| [TheSportsDB](https://www.thesportsdb.com/api.php) | Crowd-Sourced Sports Data and Artwork | `apiKey` | Yes | Yes |\n| [UFC Data](http://ufc-data-api.ufc.com/) | Ultimate Fighting Championship information for events and fighters | No | No | No |\n| [Wger](https://wger.de/en/software/api) | Workout manager data as exercises, muscles or equipment | `apiKey` | Yes | Unknown |\n\n### Test Data\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Adorable Avatars](http://avatars.adorable.io) | Generate random cartoon avatars | No | Yes | Unknown |\n| [Bacon Ipsum](https://baconipsum.com/json-api/) | A Meatier Lorem Ipsum Generator | No | Yes | Unknown |\n| [Dicebear Avatars](https://avatars.dicebear.com/) | Generate random pixel-art avatars | No | Yes | No |\n| [FakeJSON](https://fakejson.com) | Service to generate test and fake data | `apiKey` | Yes | Yes |\n| [FHIR](http://fhirtest.uhn.ca/home) | Fast Healthcare Interoperability Resources test data | No | Yes | Unknown |\n| [Hipster Ipsum](http://hipsterjesus.com/) | Generates Hipster Ipsum text | No | No | Unknown |\n| [JSONPlaceholder](http://jsonplaceholder.typicode.com/) | Fake data for testing and prototyping | No | No | Unknown |\n| [Lorem Text](https://market.mashape.com/montanaflynn/lorem-text-generator) | Generates Lorem Ipsum text | `X-Mashape-Key` | Yes | Unknown |\n| [LoremPicsum](http://lorempicsum.com) | Generate placeholder pictures | No | No | Unknown |\n| [Loripsum](http://loripsum.net/) | The \"lorem ipsum\" generator that doesn't suck | No | No | Unknown |\n| [RandomUser](https://randomuser.me) | Generates random user data | No | Yes | Unknown |\n| [RoboHash](https://robohash.org/) | Generate random robot/alien avatars | No | Yes | Unknown |\n| [UI Names](https://github.com/thm/uinames) | Generate random fake names | No | Yes | Unknown |\n| [Yes No](https://yesno.wtf/api) | Generate yes or no randomly | No | Yes | Unknown |\n\n### Text Analysis\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Aylien Text Analysis](http://docs.aylien.com/) | A collection of information retrieval and natural language APIs | `apiKey` | Yes | Unknown |\n| [Cloudmersive Natural Language Processing](https://www.cloudmersive.com/nlp-api) | Natural language processing and text analysis | `apiKey` | Yes | Yes |\n| [Detect Language](https://detectlanguage.com/) | Detects text language | `apiKey` | Yes | Unknown |\n| [Google Cloud Natural](https://cloud.google.com/natural-language/docs/) | Natural language understanding technology, including sentiment, entity and syntax analysis | `apiKey` | Yes | Unknown |\n| [Semantira](https://semantria.readme.io/docs) | Text Analytics with sentiment analysis, categorization \u0026 named entity extraction | `OAuth` | Yes | Unknown |\n| [Watson Natural Language Understanding](https://www.ibm.com/watson/developercloud/natural-language-understanding/api/v1/) | Natural language processing for advanced text analysis | `OAuth` | Yes | Unknown |\n\n### Tracking\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Postmon](http://postmon.com.br) | An API to query Brazilian ZIP codes and orders easily, quickly and free | No | No | Unknown |\n| [Sweden](https://developer.postnord.com/docs2) | Provides information about parcels in transport | `apiKey` | No | Unknown |\n| [UPS](https://www.ups.com/upsdeveloperkit) | Shipment and Address information | `apiKey` | Yes | Unknown |\n\n### Transportation\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [ADS-B Exchange](https://www.adsbexchange.com/data/) | Access real-time and historical data of any and all airborne aircraft | No | Yes | Unknown |\n| [AIS Hub](http://www.aishub.net/api) | Real-time data of any marine and inland vessel equipped with AIS tracking system | `apiKey` | No | Unknown |\n| [AIS Web](http://www.aisweb.aer.mil.br/api/doc/index.cfm) | Aeronautical information in digital media produced by the Department of Airspace Control (DECEA) | `apiKey` | No | Unknown |\n| [Amadeus Travel Innovation Sandbox](https://sandbox.amadeus.com/) | Travel Search - Limited usage | `apiKey` | Yes | Unknown |\n| [Bay Area Rapid Transit](http://api.bart.gov) | Stations and predicted arrivals for BART | `apiKey` | No | Unknown |\n| [Community Transit](https://github.com/transitland/transitland-datastore/blob/master/README.md#api-endpoints) | Transitland API | No | Yes | Unknown |\n| [Goibibo](https://developer.goibibo.com/docs) | API for travel search | `apiKey` | Yes | Unknown |\n| [GraphHopper](https://graphhopper.com/api/1/docs/) | A-to-B routing with turn-by-turn instructions | `apiKey` | Yes | Unknown |\n| [Icelandic APIs](http://docs.apis.is/) | Open APIs that deliver services in or regarding Iceland | No | Yes | Unknown |\n| [Indian Railways](http://api.erail.in/) | Indian Railways Information | `apiKey` | No | Unknown |\n| [Izi](http://api-docs.izi.travel/) | Audio guide for travellers | `apiKey` | Yes | Unknown |\n| [Navitia](https://api.navitia.io/) | The open API for building cool stuff with transport data | `apiKey` | Yes | Unknown |\n| [REFUGE Restrooms](https://www.refugerestrooms.org/api/docs/#!/restrooms) | Provides safe restroom access for transgender, intersex and gender nonconforming individuals | No | Yes | Unknown |\n| [Schiphol Airport](https://developer.schiphol.nl/) | Schiphol | `apiKey` | Yes | Unknown |\n| [TransitLand](https://transit.land/documentation/datastore/api-endpoints.html) | Transit Aggregation | No | Yes | Unknown |\n| [Transport for Atlanta, US](http://www.itsmarta.com/app-developer-resources.aspx) | Marta | No | No | Unknown |\n| [Transport for Auckland, New Zealand](https://api.at.govt.nz/) | Auckland Transport | No | Yes | Unknown |\n| [Transport for Belgium](https://hello.irail.be/api/) | Belgian transport API | No | Yes | Unknown |\n| [Transport for Berlin, Germany](https://github.com/derhuerst/vbb-rest/blob/master/docs/index.md) | Third-party VBB API | No | Yes | Unknown |\n| [Transport for Boston, US](https://mbta.com/developers/v3-api) | MBTA API | No | No | Unknown |\n| [Transport for Budapest, Hungary](https://apiary.io/) | Budapest public transport API | No | Yes | Unknown |\n| [Transport for Chicago, US](http://www.transitchicago.com/developers/) | CTA | No | No | Unknown |\n| [Transport for Czech Republic](https://www.chaps.cz/eng/products/idos-internet) | Czech transport API | No | Yes | Unknown |\n| [Transport for Denver, US](http://www.rtd-denver.com/gtfs-developer-guide.shtml) | RTD | No | No | Unknown |\n| [Transport for Finland](https://digitransit.fi/en/developers/ ) | Finnish transport API | No | Yes | Unknown |\n| [Transport for Germany](http://data.deutschebahn.com/dataset/api-fahrplan) | Deutsche Bahn (DB) API | `apiKey` | No | Unknown |\n| [Transport for Grenoble, France](https://www.metromobilite.fr/pages/opendata/OpenDataApi.html) | Grenoble public transport | No | No | No |\n| [Transport for Honolulu, US](http://hea.thebus.org/api_info.asp) | Honolulu Transportation Information | `apiKey` | No | Unknown |\n| [Transport for India](https://data.gov.in/sector/transport) | India Public Transport API | `apiKey` | Yes | Unknown |\n| [Transport for London, England](https://api.tfl.gov.uk) | TfL API | No | Yes | Unknown |\n| [Transport for Madrid, Spain](http://opendata.emtmadrid.es/Servicios-web/BUS) | Madrid BUS transport API | `apiKey` | No | Unknown |\n| [Transport for Manchester, England](https://developer.tfgm.com/) | TfGM transport network data | `apiKey` | Yes | No |\n| [Transport for Minneapolis, US](http://svc.metrotransit.org/) | NexTrip API | `OAuth` | No | Unknown |\n| [Transport for New York City, US](http://datamine.mta.info/) | MTA | `apiKey` | No | Unknown |\n| [Transport for Norway](http://reisapi.ruter.no/help) | Norwegian transport API | No | No | Unknown |\n| [Transport for Ottawa, Canada](http://www.octranspo.com/index.php/developers) | OC Transpo next bus arrival API | No | No | Unknown |\n| [Transport for Paris, France](http://restratpws.azurewebsites.net/swagger/) | Live schedules made simple | No | No | Unknown |\n| [Transport for Paris, France](http://data.ratp.fr/api/v1/console/datasets/1.0/search/) | RATP Open Data API | No | No | Unknown |\n| [Transport for Philadelphia, US](http://www3.septa.org/hackathon/) | SEPTA APIs | No | No | Unknown |\n| [Transport for Sao Paulo, Brazil](http://www.sptrans.com.br/desenvolvedores/APIOlhoVivo/Documentacao.aspx) | SPTrans | `OAuth` | No | Unknown |\n| [Transport for Sweden](https://www.trafiklab.se/api) | Public Transport consumer | `OAuth` | Yes | Unknown |\n| [Transport for Switzerland](https://opentransportdata.swiss/en/) | Official Swiss Public Transport Open Data | `apiKey` | Yes | Unknown |\n| [Transport for Switzerland](https://transport.opendata.ch/) | Swiss public transport API | No | Yes | Unknown |\n| [Transport for The Netherlands](http://www.ns.nl/reisinformatie/ns-api) | NS, only trains | `apiKey` | No | Unknown |\n| [Transport for The Netherlands](https://github.com/skywave/KV78Turbo-OVAPI/wiki) | OVAPI, country-wide public transport | No | Yes | Unknown |\n| [Transport for Toronto, Canada](https://myttc.ca/developers) | TTC | No | Yes | Unknown |\n| [Transport for United States](http://www.nextbus.com/xmlFeedDocs/NextBusXMLFeed.pdf) | NextBus API | No | No | Unknown |\n| [Transport for Vancouver, Canada](https://developer.translink.ca/) | TransLink | `OAuth` | Yes | Unknown |\n| [Transport for Victoria, AU](https://www.ptv.vic.gov.au/about-ptv/ptv-data-and-reports/digital-products/ptv-timetable-api/) | PTV API | `apiKey` | Yes | Unknown |\n| [Transport for Washington, US](https://developer.wmata.com/) | Washington Metro transport API | `OAuth` | Yes | Unknown |\n| [Uber](https://developer.uber.com/products) | Uber ride requests and price estimation | `OAuth` | Yes | Yes |\n| [WhereIsMyTransport](https://developer.whereismytransport.com/) | Platform for public transport data in emerging cities | `OAuth` | Yes | Unknown |\n\n### URL Shorteners\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Bitly](http://dev.bitly.com/get_started.html) | URL shortener and link management | `OAuth` | Yes | Unknown |\n| [ClickMeter](https://support.clickmeter.com/hc/en-us/categories/201474986) | Monitor, compare and optimize your marketing links | `apiKey` | Yes | Unknown |\n| [Rebrandly](https://developers.rebrandly.com/v1/docs) | Custom URL shortener for sharing branded links | `apiKey` | Yes | Unknown |\n\n### Vehicle\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [Brazilian Vehicles and Prices](https://deividfortuna.github.io/fipe/) | Vehicles information from Fundação Instituto de Pesquisas Econômicas - Fipe | No | Yes | Unknown |\n| [Kelley Blue Book](http://developer.kbb.com/#!/data/1-Default) | Vehicle info, pricing, configuration, plus much more | `apiKey` | Yes | No |\n| [Mercedes-Benz](https://developer.mercedes-benz.com/apis) | Telematics data, remotely access vehicle functions, car configurator, locate service dealers | `apiKey` | Yes | No |\n| [NHTSA](https://vpic.nhtsa.dot.gov/api/) | NHTSA Product Information Catalog and Vehicle Listing | No | Yes | Unknown |\n\n### Video\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [An API of Ice And Fire](https://anapioficeandfire.com/) | Game Of Thrones API | No | Yes | Unknown |\n| [Breaking Bad Quotes](https://github.com/shevabam/breaking-bad-quotes) | Some Breaking Bad quotes | No | Yes | Unknown |\n| [Czech Television](http://www.ceskatelevize.cz/xml/tv-program/) | TV programme of Czech TV | No | No | Unknown |\n| [Dailymotion](https://developer.dailymotion.com/) | Dailymotion Developer API | `OAuth` | Yes | Unknown |\n| [Open Movie Database](http://www.omdbapi.com/) | Movie information | `apiKey` | Yes | Unknown |\n| [Ron Swanson Quotes](https://github.com/jamesseanwright/ron-swanson-quotes#ron-swanson-quotes-api) | Television | No | Yes | Unknown |\n| [STAPI](http://stapi.co) | Information on all things Star Trek | No | No | No |\n| [SWAPI](https://swapi.co) | Star Wars Information | No | Yes | Unknown |\n| [TMDb](https://www.themoviedb.org/documentation/api) | Community-based movie data | `apiKey` | Yes | Unknown |\n| [TVDB](https://api.thetvdb.com/swagger) | Television data | `apiKey` | Yes | Unknown |\n| [TVMaze](http://www.tvmaze.com/api) | TV Show Data | No | No | Unknown |\n| [Utelly](https://market.mashape.com/utelly/utelly) | Check where a tv show or movie is available | `X-Mashape-Key` | Yes | Unknown |\n| [Vimeo](https://developer.vimeo.com/) | Vimeo Developer API | `OAuth` | Yes | Unknown |\n| [YouTube](https://developers.google.com/youtube/) | Add YouTube functionality to your sites and apps | `OAuth` | Yes | Unknown |\n\n### Weather\nAPI | Description | Auth | HTTPS | CORS |\n|---|---|---|---|---|\n| [APIXU](https://www.apixu.com/doc/request.aspx) | Weather | `apiKey` | Yes | Unknown |\n| [ClimaCell Micro Weather](https://developer.climacell.co) | Historical, real-time and nowcast weather data | `apiKey` | Yes | Yes |\n| [Dark Sky](https://darksky.net/dev/) | Weather | `apiKey` | Yes | No |\n| [MetaWeather](https://www.metaweather.com/api/) | Weather | No | Yes | No |\n| [NOAA Climate Data](https://www.ncdc.noaa.gov/cdo-web/) | Weather and climate data | `apiKey` | Yes | Unknown |\n| [ODWeather](http://api.oceandrivers.com/static/docs.html) | Weather and weather webcams | No | No | Unknown |\n| [OpenUV](https://www.openuv.io) | Real-time UV Index Forecast | `apiKey` | Yes | Unknown |\n| [OpenWeatherMap](http://openweathermap.org/api) | Weather | `apiKey` | No | Unknown |\n| [Storm Glass](https://stormglass.io/) | Global marine weather from multiple sources | `apiKey` | Yes | Yes |\n| [Weatherbit](https://www.weatherbit.io/api) | Weather | `apiKey` | Yes | Unknown |\n| [Yahoo! Weather](https://developer.yahoo.com/weather/) | Weather | `apiKey` | Yes | Unknown |\n"
  },
  {
    "repo": "rg3/youtube-dl",
    "content": "[![Build Status](https://travis-ci.org/rg3/youtube-dl.svg?branch=master)](https://travis-ci.org/rg3/youtube-dl)\n\nyoutube-dl - download videos from youtube.com or other video platforms\n\n- [INSTALLATION](#installation)\n- [DESCRIPTION](#description)\n- [OPTIONS](#options)\n- [CONFIGURATION](#configuration)\n- [OUTPUT TEMPLATE](#output-template)\n- [FORMAT SELECTION](#format-selection)\n- [VIDEO SELECTION](#video-selection)\n- [FAQ](#faq)\n- [DEVELOPER INSTRUCTIONS](#developer-instructions)\n- [EMBEDDING YOUTUBE-DL](#embedding-youtube-dl)\n- [BUGS](#bugs)\n- [COPYRIGHT](#copyright)\n\n# INSTALLATION\n\nTo install it right away for all UNIX users (Linux, macOS, etc.), type:\n\n    sudo curl -L https://yt-dl.org/downloads/latest/youtube-dl -o /usr/local/bin/youtube-dl\n    sudo chmod a+rx /usr/local/bin/youtube-dl\n\nIf you do not have curl, you can alternatively use a recent wget:\n\n    sudo wget https://yt-dl.org/downloads/latest/youtube-dl -O /usr/local/bin/youtube-dl\n    sudo chmod a+rx /usr/local/bin/youtube-dl\n\nWindows users can [download an .exe file](https://yt-dl.org/latest/youtube-dl.exe) and place it in any location on their [PATH](https://en.wikipedia.org/wiki/PATH_%28variable%29) except for `%SYSTEMROOT%\\System32` (e.g. **do not** put in `C:\\Windows\\System32`).\n\nYou can also use pip:\n\n    sudo -H pip install --upgrade youtube-dl\n    \nThis command will update youtube-dl if you have already installed it. See the [pypi page](https://pypi.python.org/pypi/youtube_dl) for more information.\n\nmacOS users can install youtube-dl with [Homebrew](https://brew.sh/):\n\n    brew install youtube-dl\n\nOr with [MacPorts](https://www.macports.org/):\n\n    sudo port install youtube-dl\n\nAlternatively, refer to the [developer instructions](#developer-instructions) for how to check out and work with the git repository. For further options, including PGP signatures, see the [youtube-dl Download Page](https://rg3.github.io/youtube-dl/download.html).\n\n# DESCRIPTION\n**youtube-dl** is a command-line program to download videos from YouTube.com and a few more sites. It requires the Python interpreter, version 2.6, 2.7, or 3.2+, and it is not platform specific. It should work on your Unix box, on Windows or on macOS. It is released to the public domain, which means you can modify it, redistribute it or use it however you like.\n\n    youtube-dl [OPTIONS] URL [URL...]\n\n# OPTIONS\n    -h, --help                       Print this help text and exit\n    --version                        Print program version and exit\n    -U, --update                     Update this program to latest version. Make\n                                     sure that you have sufficient permissions\n                                     (run with sudo if needed)\n    -i, --ignore-errors              Continue on download errors, for example to\n                                     skip unavailable videos in a playlist\n    --abort-on-error                 Abort downloading of further videos (in the\n                                     playlist or the command line) if an error\n                                     occurs\n    --dump-user-agent                Display the current browser identification\n    --list-extractors                List all supported extractors\n    --extractor-descriptions         Output descriptions of all supported\n                                     extractors\n    --force-generic-extractor        Force extraction to use the generic\n                                     extractor\n    --default-search PREFIX          Use this prefix for unqualified URLs. For\n                                     example \"gvsearch2:\" downloads two videos\n                                     from google videos for youtube-dl \"large\n                                     apple\". Use the value \"auto\" to let\n                                     youtube-dl guess (\"auto_warning\" to emit a\n                                     warning when guessing). \"error\" just throws\n                                     an error. The default value \"fixup_error\"\n                                     repairs broken URLs, but emits an error if\n                                     this is not possible instead of searching.\n    --ignore-config                  Do not read configuration files. When given\n                                     in the global configuration file\n                                     /etc/youtube-dl.conf: Do not read the user\n                                     configuration in ~/.config/youtube-\n                                     dl/config (%APPDATA%/youtube-dl/config.txt\n                                     on Windows)\n    --config-location PATH           Location of the configuration file; either\n                                     the path to the config or its containing\n                                     directory.\n    --flat-playlist                  Do not extract the videos of a playlist,\n                                     only list them.\n    --mark-watched                   Mark videos watched (YouTube only)\n    --no-mark-watched                Do not mark videos watched (YouTube only)\n    --no-color                       Do not emit color codes in output\n\n## Network Options:\n    --proxy URL                      Use the specified HTTP/HTTPS/SOCKS proxy.\n                                     To enable SOCKS proxy, specify a proper\n                                     scheme. For example\n                                     socks5://127.0.0.1:1080/. Pass in an empty\n                                     string (--proxy \"\") for direct connection\n    --socket-timeout SECONDS         Time to wait before giving up, in seconds\n    --source-address IP              Client-side IP address to bind to\n    -4, --force-ipv4                 Make all connections via IPv4\n    -6, --force-ipv6                 Make all connections via IPv6\n\n## Geo Restriction:\n    --geo-verification-proxy URL     Use this proxy to verify the IP address for\n                                     some geo-restricted sites. The default\n                                     proxy specified by --proxy (or none, if the\n                                     option is not present) is used for the\n                                     actual downloading.\n    --geo-bypass                     Bypass geographic restriction via faking\n                                     X-Forwarded-For HTTP header\n    --no-geo-bypass                  Do not bypass geographic restriction via\n                                     faking X-Forwarded-For HTTP header\n    --geo-bypass-country CODE        Force bypass geographic restriction with\n                                     explicitly provided two-letter ISO 3166-2\n                                     country code\n    --geo-bypass-ip-block IP_BLOCK   Force bypass geographic restriction with\n                                     explicitly provided IP block in CIDR\n                                     notation\n\n## Video Selection:\n    --playlist-start NUMBER          Playlist video to start at (default is 1)\n    --playlist-end NUMBER            Playlist video to end at (default is last)\n    --playlist-items ITEM_SPEC       Playlist video items to download. Specify\n                                     indices of the videos in the playlist\n                                     separated by commas like: \"--playlist-items\n                                     1,2,5,8\" if you want to download videos\n                                     indexed 1, 2, 5, 8 in the playlist. You can\n                                     specify range: \"--playlist-items\n                                     1-3,7,10-13\", it will download the videos\n                                     at index 1, 2, 3, 7, 10, 11, 12 and 13.\n    --match-title REGEX              Download only matching titles (regex or\n                                     caseless sub-string)\n    --reject-title REGEX             Skip download for matching titles (regex or\n                                     caseless sub-string)\n    --max-downloads NUMBER           Abort after downloading NUMBER files\n    --min-filesize SIZE              Do not download any videos smaller than\n                                     SIZE (e.g. 50k or 44.6m)\n    --max-filesize SIZE              Do not download any videos larger than SIZE\n                                     (e.g. 50k or 44.6m)\n    --date DATE                      Download only videos uploaded in this date\n    --datebefore DATE                Download only videos uploaded on or before\n                                     this date (i.e. inclusive)\n    --dateafter DATE                 Download only videos uploaded on or after\n                                     this date (i.e. inclusive)\n    --min-views COUNT                Do not download any videos with less than\n                                     COUNT views\n    --max-views COUNT                Do not download any videos with more than\n                                     COUNT views\n    --match-filter FILTER            Generic video filter. Specify any key (see\n                                     the \"OUTPUT TEMPLATE\" for a list of\n                                     available keys) to match if the key is\n                                     present, !key to check if the key is not\n                                     present, key \u003e NUMBER (like \"comment_count\n                                     \u003e 12\", also works with \u003e=, \u003c, \u003c=, !=, =) to\n                                     compare against a number, key = 'LITERAL'\n                                     (like \"uploader = 'Mike Smith'\", also works\n                                     with !=) to match against a string literal\n                                     and \u0026 to require multiple matches. Values\n                                     which are not known are excluded unless you\n                                     put a question mark (?) after the operator.\n                                     For example, to only match videos that have\n                                     been liked more than 100 times and disliked\n                                     less than 50 times (or the dislike\n                                     functionality is not available at the given\n                                     service), but who also have a description,\n                                     use --match-filter \"like_count \u003e 100 \u0026\n                                     dislike_count \u003c? 50 \u0026 description\" .\n    --no-playlist                    Download only the video, if the URL refers\n                                     to a video and a playlist.\n    --yes-playlist                   Download the playlist, if the URL refers to\n                                     a video and a playlist.\n    --age-limit YEARS                Download only videos suitable for the given\n                                     age\n    --download-archive FILE          Download only videos not listed in the\n                                     archive file. Record the IDs of all\n                                     downloaded videos in it.\n    --include-ads                    Download advertisements as well\n                                     (experimental)\n\n## Download Options:\n    -r, --limit-rate RATE            Maximum download rate in bytes per second\n                                     (e.g. 50K or 4.2M)\n    -R, --retries RETRIES            Number of retries (default is 10), or\n                                     \"infinite\".\n    --fragment-retries RETRIES       Number of retries for a fragment (default\n                                     is 10), or \"infinite\" (DASH, hlsnative and\n                                     ISM)\n    --skip-unavailable-fragments     Skip unavailable fragments (DASH, hlsnative\n                                     and ISM)\n    --abort-on-unavailable-fragment  Abort downloading when some fragment is not\n                                     available\n    --keep-fragments                 Keep downloaded fragments on disk after\n                                     downloading is finished; fragments are\n                                     erased by default\n    --buffer-size SIZE               Size of download buffer (e.g. 1024 or 16K)\n                                     (default is 1024)\n    --no-resize-buffer               Do not automatically adjust the buffer\n                                     size. By default, the buffer size is\n                                     automatically resized from an initial value\n                                     of SIZE.\n    --http-chunk-size SIZE           Size of a chunk for chunk-based HTTP\n                                     downloading (e.g. 10485760 or 10M) (default\n                                     is disabled). May be useful for bypassing\n                                     bandwidth throttling imposed by a webserver\n                                     (experimental)\n    --playlist-reverse               Download playlist videos in reverse order\n    --playlist-random                Download playlist videos in random order\n    --xattr-set-filesize             Set file xattribute ytdl.filesize with\n                                     expected file size\n    --hls-prefer-native              Use the native HLS downloader instead of\n                                     ffmpeg\n    --hls-prefer-ffmpeg              Use ffmpeg instead of the native HLS\n                                     downloader\n    --hls-use-mpegts                 Use the mpegts container for HLS videos,\n                                     allowing to play the video while\n                                     downloading (some players may not be able\n                                     to play it)\n    --external-downloader COMMAND    Use the specified external downloader.\n                                     Currently supports\n                                     aria2c,avconv,axel,curl,ffmpeg,httpie,wget\n    --external-downloader-args ARGS  Give these arguments to the external\n                                     downloader\n\n## Filesystem Options:\n    -a, --batch-file FILE            File containing URLs to download ('-' for\n                                     stdin), one URL per line. Lines starting\n                                     with '#', ';' or ']' are considered as\n                                     comments and ignored.\n    --id                             Use only video ID in file name\n    -o, --output TEMPLATE            Output filename template, see the \"OUTPUT\n                                     TEMPLATE\" for all the info\n    --autonumber-start NUMBER        Specify the start value for %(autonumber)s\n                                     (default is 1)\n    --restrict-filenames             Restrict filenames to only ASCII\n                                     characters, and avoid \"\u0026\" and spaces in\n                                     filenames\n    -w, --no-overwrites              Do not overwrite files\n    -c, --continue                   Force resume of partially downloaded files.\n                                     By default, youtube-dl will resume\n                                     downloads if possible.\n    --no-continue                    Do not resume partially downloaded files\n                                     (restart from beginning)\n    --no-part                        Do not use .part files - write directly\n                                     into output file\n    --no-mtime                       Do not use the Last-modified header to set\n                                     the file modification time\n    --write-description              Write video description to a .description\n                                     file\n    --write-info-json                Write video metadata to a .info.json file\n    --write-annotations              Write video annotations to a\n                                     .annotations.xml file\n    --load-info-json FILE            JSON file containing the video information\n                                     (created with the \"--write-info-json\"\n                                     option)\n    --cookies FILE                   File to read cookies from and dump cookie\n                                     jar in\n    --cache-dir DIR                  Location in the filesystem where youtube-dl\n                                     can store some downloaded information\n                                     permanently. By default\n                                     $XDG_CACHE_HOME/youtube-dl or\n                                     ~/.cache/youtube-dl . At the moment, only\n                                     YouTube player files (for videos with\n                                     obfuscated signatures) are cached, but that\n                                     may change.\n    --no-cache-dir                   Disable filesystem caching\n    --rm-cache-dir                   Delete all filesystem cache files\n\n## Thumbnail images:\n    --write-thumbnail                Write thumbnail image to disk\n    --write-all-thumbnails           Write all thumbnail image formats to disk\n    --list-thumbnails                Simulate and list all available thumbnail\n                                     formats\n\n## Verbosity / Simulation Options:\n    -q, --quiet                      Activate quiet mode\n    --no-warnings                    Ignore warnings\n    -s, --simulate                   Do not download the video and do not write\n                                     anything to disk\n    --skip-download                  Do not download the video\n    -g, --get-url                    Simulate, quiet but print URL\n    -e, --get-title                  Simulate, quiet but print title\n    --get-id                         Simulate, quiet but print id\n    --get-thumbnail                  Simulate, quiet but print thumbnail URL\n    --get-description                Simulate, quiet but print video description\n    --get-duration                   Simulate, quiet but print video length\n    --get-filename                   Simulate, quiet but print output filename\n    --get-format                     Simulate, quiet but print output format\n    -j, --dump-json                  Simulate, quiet but print JSON information.\n                                     See the \"OUTPUT TEMPLATE\" for a description\n                                     of available keys.\n    -J, --dump-single-json           Simulate, quiet but print JSON information\n                                     for each command-line argument. If the URL\n                                     refers to a playlist, dump the whole\n                                     playlist information in a single line.\n    --print-json                     Be quiet and print the video information as\n                                     JSON (video is still being downloaded).\n    --newline                        Output progress bar as new lines\n    --no-progress                    Do not print progress bar\n    --console-title                  Display progress in console titlebar\n    -v, --verbose                    Print various debugging information\n    --dump-pages                     Print downloaded pages encoded using base64\n                                     to debug problems (very verbose)\n    --write-pages                    Write downloaded intermediary pages to\n                                     files in the current directory to debug\n                                     problems\n    --print-traffic                  Display sent and read HTTP traffic\n    -C, --call-home                  Contact the youtube-dl server for debugging\n    --no-call-home                   Do NOT contact the youtube-dl server for\n                                     debugging\n\n## Workarounds:\n    --encoding ENCODING              Force the specified encoding (experimental)\n    --no-check-certificate           Suppress HTTPS certificate validation\n    --prefer-insecure                Use an unencrypted connection to retrieve\n                                     information about the video. (Currently\n                                     supported only for YouTube)\n    --user-agent UA                  Specify a custom user agent\n    --referer URL                    Specify a custom referer, use if the video\n                                     access is restricted to one domain\n    --add-header FIELD:VALUE         Specify a custom HTTP header and its value,\n                                     separated by a colon ':'. You can use this\n                                     option multiple times\n    --bidi-workaround                Work around terminals that lack\n                                     bidirectional text support. Requires bidiv\n                                     or fribidi executable in PATH\n    --sleep-interval SECONDS         Number of seconds to sleep before each\n                                     download when used alone or a lower bound\n                                     of a range for randomized sleep before each\n                                     download (minimum possible number of\n                                     seconds to sleep) when used along with\n                                     --max-sleep-interval.\n    --max-sleep-interval SECONDS     Upper bound of a range for randomized sleep\n                                     before each download (maximum possible\n                                     number of seconds to sleep). Must only be\n                                     used along with --min-sleep-interval.\n\n## Video Format Options:\n    -f, --format FORMAT              Video format code, see the \"FORMAT\n                                     SELECTION\" for all the info\n    --all-formats                    Download all available video formats\n    --prefer-free-formats            Prefer free video formats unless a specific\n                                     one is requested\n    -F, --list-formats               List all available formats of requested\n                                     videos\n    --youtube-skip-dash-manifest     Do not download the DASH manifests and\n                                     related data on YouTube videos\n    --merge-output-format FORMAT     If a merge is required (e.g.\n                                     bestvideo+bestaudio), output to given\n                                     container format. One of mkv, mp4, ogg,\n                                     webm, flv. Ignored if no merge is required\n\n## Subtitle Options:\n    --write-sub                      Write subtitle file\n    --write-auto-sub                 Write automatically generated subtitle file\n                                     (YouTube only)\n    --all-subs                       Download all the available subtitles of the\n                                     video\n    --list-subs                      List all available subtitles for the video\n    --sub-format FORMAT              Subtitle format, accepts formats\n                                     preference, for example: \"srt\" or\n                                     \"ass/srt/best\"\n    --sub-lang LANGS                 Languages of the subtitles to download\n                                     (optional) separated by commas, use --list-\n                                     subs for available language tags\n\n## Authentication Options:\n    -u, --username USERNAME          Login with this account ID\n    -p, --password PASSWORD          Account password. If this option is left\n                                     out, youtube-dl will ask interactively.\n    -2, --twofactor TWOFACTOR        Two-factor authentication code\n    -n, --netrc                      Use .netrc authentication data\n    --video-password PASSWORD        Video password (vimeo, smotri, youku)\n\n## Adobe Pass Options:\n    --ap-mso MSO                     Adobe Pass multiple-system operator (TV\n                                     provider) identifier, use --ap-list-mso for\n                                     a list of available MSOs\n    --ap-username USERNAME           Multiple-system operator account login\n    --ap-password PASSWORD           Multiple-system operator account password.\n                                     If this option is left out, youtube-dl will\n                                     ask interactively.\n    --ap-list-mso                    List all supported multiple-system\n                                     operators\n\n## Post-processing Options:\n    -x, --extract-audio              Convert video files to audio-only files\n                                     (requires ffmpeg or avconv and ffprobe or\n                                     avprobe)\n    --audio-format FORMAT            Specify audio format: \"best\", \"aac\",\n                                     \"flac\", \"mp3\", \"m4a\", \"opus\", \"vorbis\", or\n                                     \"wav\"; \"best\" by default; No effect without\n                                     -x\n    --audio-quality QUALITY          Specify ffmpeg/avconv audio quality, insert\n                                     a value between 0 (better) and 9 (worse)\n                                     for VBR or a specific bitrate like 128K\n                                     (default 5)\n    --recode-video FORMAT            Encode the video to another format if\n                                     necessary (currently supported:\n                                     mp4|flv|ogg|webm|mkv|avi)\n    --postprocessor-args ARGS        Give these arguments to the postprocessor\n    -k, --keep-video                 Keep the video file on disk after the post-\n                                     processing; the video is erased by default\n    --no-post-overwrites             Do not overwrite post-processed files; the\n                                     post-processed files are overwritten by\n                                     default\n    --embed-subs                     Embed subtitles in the video (only for mp4,\n                                     webm and mkv videos)\n    --embed-thumbnail                Embed thumbnail in the audio as cover art\n    --add-metadata                   Write metadata to the video file\n    --metadata-from-title FORMAT     Parse additional metadata like song title /\n                                     artist from the video title. The format\n                                     syntax is the same as --output. Regular\n                                     expression with named capture groups may\n                                     also be used. The parsed parameters replace\n                                     existing values. Example: --metadata-from-\n                                     title \"%(artist)s - %(title)s\" matches a\n                                     title like \"Coldplay - Paradise\". Example\n                                     (regex): --metadata-from-title\n                                     \"(?P\u003cartist\u003e.+?) - (?P\u003ctitle\u003e.+)\"\n    --xattrs                         Write metadata to the video file's xattrs\n                                     (using dublin core and xdg standards)\n    --fixup POLICY                   Automatically correct known faults of the\n                                     file. One of never (do nothing), warn (only\n                                     emit a warning), detect_or_warn (the\n                                     default; fix file if we can, warn\n                                     otherwise)\n    --prefer-avconv                  Prefer avconv over ffmpeg for running the\n                                     postprocessors\n    --prefer-ffmpeg                  Prefer ffmpeg over avconv for running the\n                                     postprocessors (default)\n    --ffmpeg-location PATH           Location of the ffmpeg/avconv binary;\n                                     either the path to the binary or its\n                                     containing directory.\n    --exec CMD                       Execute a command on the file after\n                                     downloading, similar to find's -exec\n                                     syntax. Example: --exec 'adb push {}\n                                     /sdcard/Music/ \u0026\u0026 rm {}'\n    --convert-subs FORMAT            Convert the subtitles to other format\n                                     (currently supported: srt|ass|vtt|lrc)\n\n# CONFIGURATION\n\nYou can configure youtube-dl by placing any supported command line option to a configuration file. On Linux and macOS, the system wide configuration file is located at `/etc/youtube-dl.conf` and the user wide configuration file at `~/.config/youtube-dl/config`. On Windows, the user wide configuration file locations are `%APPDATA%\\youtube-dl\\config.txt` or `C:\\Users\\\u003cuser name\u003e\\youtube-dl.conf`. Note that by default configuration file may not exist so you may need to create it yourself.\n\nFor example, with the following configuration file youtube-dl will always extract the audio, not copy the mtime, use a proxy and save all videos under `Movies` directory in your home directory:\n```\n# Lines starting with # are comments\n\n# Always extract audio\n-x\n\n# Do not copy the mtime\n--no-mtime\n\n# Use this proxy\n--proxy 127.0.0.1:3128\n\n# Save all videos under Movies directory in your home directory\n-o ~/Movies/%(title)s.%(ext)s\n```\n\nNote that options in configuration file are just the same options aka switches used in regular command line calls thus there **must be no whitespace** after `-` or `--`, e.g. `-o` or `--proxy` but not `- o` or `-- proxy`.\n\nYou can use `--ignore-config` if you want to disable the configuration file for a particular youtube-dl run.\n\nYou can also use `--config-location` if you want to use custom configuration file for a particular youtube-dl run.\n\n### Authentication with `.netrc` file\n\nYou may also want to configure automatic credentials storage for extractors that support authentication (by providing login and password with `--username` and `--password`) in order not to pass credentials as command line arguments on every youtube-dl execution and prevent tracking plain text passwords in the shell command history. You can achieve this using a [`.netrc` file](https://stackoverflow.com/tags/.netrc/info) on a per extractor basis. For that you will need to create a `.netrc` file in your `$HOME` and restrict permissions to read/write by only you:\n```\ntouch $HOME/.netrc\nchmod a-rwx,u+rw $HOME/.netrc\n```\nAfter that you can add credentials for an extractor in the following format, where *extractor* is the name of the extractor in lowercase:\n```\nmachine \u003cextractor\u003e login \u003clogin\u003e password \u003cpassword\u003e\n```\nFor example:\n```\nmachine youtube login myaccount@gmail.com password my_youtube_password\nmachine twitch login my_twitch_account_name password my_twitch_password\n```\nTo activate authentication with the `.netrc` file you should pass `--netrc` to youtube-dl or place it in the [configuration file](#configuration).\n\nOn Windows you may also need to setup the `%HOME%` environment variable manually. For example:\n```\nset HOME=%USERPROFILE%\n```\n\n# OUTPUT TEMPLATE\n\nThe `-o` option allows users to indicate a template for the output file names.\n\n**tl;dr:** [navigate me to examples](#output-template-examples).\n\nThe basic usage is not to set any template arguments when downloading a single file, like in `youtube-dl -o funny_video.flv \"https://some/video\"`. However, it may contain special sequences that will be replaced when downloading each video. The special sequences may be formatted according to [python string formatting operations](https://docs.python.org/2/library/stdtypes.html#string-formatting). For example, `%(NAME)s` or `%(NAME)05d`. To clarify, that is a percent symbol followed by a name in parentheses, followed by a formatting operations. Allowed names along with sequence type are:\n\n - `id` (string): Video identifier\n - `title` (string): Video title\n - `url` (string): Video URL\n - `ext` (string): Video filename extension\n - `alt_title` (string): A secondary title of the video\n - `display_id` (string): An alternative identifier for the video\n - `uploader` (string): Full name of the video uploader\n - `license` (string): License name the video is licensed under\n - `creator` (string): The creator of the video\n - `release_date` (string): The date (YYYYMMDD) when the video was released\n - `timestamp` (numeric): UNIX timestamp of the moment the video became available\n - `upload_date` (string): Video upload date (YYYYMMDD)\n - `uploader_id` (string): Nickname or id of the video uploader\n - `channel` (string): Full name of the channel the video is uploaded on\n - `channel_id` (string): Id of the channel\n - `location` (string): Physical location where the video was filmed\n - `duration` (numeric): Length of the video in seconds\n - `view_count` (numeric): How many users have watched the video on the platform\n - `like_count` (numeric): Number of positive ratings of the video\n - `dislike_count` (numeric): Number of negative ratings of the video\n - `repost_count` (numeric): Number of reposts of the video\n - `average_rating` (numeric): Average rating give by users, the scale used depends on the webpage\n - `comment_count` (numeric): Number of comments on the video\n - `age_limit` (numeric): Age restriction for the video (years)\n - `is_live` (boolean): Whether this video is a live stream or a fixed-length video\n - `start_time` (numeric): Time in seconds where the reproduction should start, as specified in the URL\n - `end_time` (numeric): Time in seconds where the reproduction should end, as specified in the URL\n - `format` (string): A human-readable description of the format \n - `format_id` (string): Format code specified by `--format`\n - `format_note` (string): Additional info about the format\n - `width` (numeric): Width of the video\n - `height` (numeric): Height of the video\n - `resolution` (string): Textual description of width and height\n - `tbr` (numeric): Average bitrate of audio and video in KBit/s\n - `abr` (numeric): Average audio bitrate in KBit/s\n - `acodec` (string): Name of the audio codec in use\n - `asr` (numeric): Audio sampling rate in Hertz\n - `vbr` (numeric): Average video bitrate in KBit/s\n - `fps` (numeric): Frame rate\n - `vcodec` (string): Name of the video codec in use\n - `container` (string): Name of the container format\n - `filesize` (numeric): The number of bytes, if known in advance\n - `filesize_approx` (numeric): An estimate for the number of bytes\n - `protocol` (string): The protocol that will be used for the actual download\n - `extractor` (string): Name of the extractor\n - `extractor_key` (string): Key name of the extractor\n - `epoch` (numeric): Unix epoch when creating the file\n - `autonumber` (numeric): Five-digit number that will be increased with each download, starting at zero\n - `playlist` (string): Name or id of the playlist that contains the video\n - `playlist_index` (numeric): Index of the video in the playlist padded with leading zeros according to the total length of the playlist\n - `playlist_id` (string): Playlist identifier\n - `playlist_title` (string): Playlist title\n - `playlist_uploader` (string): Full name of the playlist uploader\n - `playlist_uploader_id` (string): Nickname or id of the playlist uploader\n\nAvailable for the video that belongs to some logical chapter or section:\n\n - `chapter` (string): Name or title of the chapter the video belongs to\n - `chapter_number` (numeric): Number of the chapter the video belongs to\n - `chapter_id` (string): Id of the chapter the video belongs to\n\nAvailable for the video that is an episode of some series or programme:\n\n - `series` (string): Title of the series or programme the video episode belongs to\n - `season` (string): Title of the season the video episode belongs to\n - `season_number` (numeric): Number of the season the video episode belongs to\n - `season_id` (string): Id of the season the video episode belongs to\n - `episode` (string): Title of the video episode\n - `episode_number` (numeric): Number of the video episode within a season\n - `episode_id` (string): Id of the video episode\n\nAvailable for the media that is a track or a part of a music album:\n\n - `track` (string): Title of the track\n - `track_number` (numeric): Number of the track within an album or a disc\n - `track_id` (string): Id of the track\n - `artist` (string): Artist(s) of the track\n - `genre` (string): Genre(s) of the track\n - `album` (string): Title of the album the track belongs to\n - `album_type` (string): Type of the album\n - `album_artist` (string): List of all artists appeared on the album\n - `disc_number` (numeric): Number of the disc or other physical medium the track belongs to\n - `release_year` (numeric): Year (YYYY) when the album was released\n\nEach aforementioned sequence when referenced in an output template will be replaced by the actual value corresponding to the sequence name. Note that some of the sequences are not guaranteed to be present since they depend on the metadata obtained by a particular extractor. Such sequences will be replaced with `NA`.\n\nFor example for `-o %(title)s-%(id)s.%(ext)s` and an mp4 video with title `youtube-dl test video` and id `BaW_jenozKcj`, this will result in a `youtube-dl test video-BaW_jenozKcj.mp4` file created in the current directory.\n\nFor numeric sequences you can use numeric related formatting, for example, `%(view_count)05d` will result in a string with view count padded with zeros up to 5 characters, like in `00042`.\n\nOutput templates can also contain arbitrary hierarchical path, e.g. `-o '%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s'` which will result in downloading each video in a directory corresponding to this path template. Any missing directory will be automatically created for you.\n\nTo use percent literals in an output template use `%%`. To output to stdout use `-o -`.\n\nThe current default template is `%(title)s-%(id)s.%(ext)s`.\n\nIn some cases, you don't want special characters such as 中, spaces, or \u0026, such as when transferring the downloaded filename to a Windows system or the filename through an 8bit-unsafe channel. In these cases, add the `--restrict-filenames` flag to get a shorter title:\n\n#### Output template and Windows batch files\n\nIf you are using an output template inside a Windows batch file then you must escape plain percent characters (`%`) by doubling, so that `-o \"%(title)s-%(id)s.%(ext)s\"` should become `-o \"%%(title)s-%%(id)s.%%(ext)s\"`. However you should not touch `%`'s that are not plain characters, e.g. environment variables for expansion should stay intact: `-o \"C:\\%HOMEPATH%\\Desktop\\%%(title)s.%%(ext)s\"`.\n\n#### Output template examples\n\nNote that on Windows you may need to use double quotes instead of single.\n\n```bash\n$ youtube-dl --get-filename -o '%(title)s.%(ext)s' BaW_jenozKc\nyoutube-dl test video ''_ä↭𝕐.mp4    # All kinds of weird characters\n\n$ youtube-dl --get-filename -o '%(title)s.%(ext)s' BaW_jenozKc --restrict-filenames\nyoutube-dl_test_video_.mp4          # A simple file name\n\n# Download YouTube playlist videos in separate directory indexed by video order in a playlist\n$ youtube-dl -o '%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s' https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re\n\n# Download all playlists of YouTube channel/user keeping each playlist in separate directory:\n$ youtube-dl -o '%(uploader)s/%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s' https://www.youtube.com/user/TheLinuxFoundation/playlists\n\n# Download Udemy course keeping each chapter in separate directory under MyVideos directory in your home\n$ youtube-dl -u user -p password -o '~/MyVideos/%(playlist)s/%(chapter_number)s - %(chapter)s/%(title)s.%(ext)s' https://www.udemy.com/java-tutorial/\n\n# Download entire series season keeping each series and each season in separate directory under C:/MyVideos\n$ youtube-dl -o \"C:/MyVideos/%(series)s/%(season_number)s - %(season)s/%(episode_number)s - %(episode)s.%(ext)s\" https://videomore.ru/kino_v_detalayah/5_sezon/367617\n\n# Stream the video being downloaded to stdout\n$ youtube-dl -o - BaW_jenozKc\n```\n\n# FORMAT SELECTION\n\nBy default youtube-dl tries to download the best available quality, i.e. if you want the best quality you **don't need** to pass any special options, youtube-dl will guess it for you by **default**.\n\nBut sometimes you may want to download in a different format, for example when you are on a slow or intermittent connection. The key mechanism for achieving this is so-called *format selection* based on which you can explicitly specify desired format, select formats based on some criterion or criteria, setup precedence and much more.\n\nThe general syntax for format selection is `--format FORMAT` or shorter `-f FORMAT` where `FORMAT` is a *selector expression*, i.e. an expression that describes format or formats you would like to download.\n\n**tl;dr:** [navigate me to examples](#format-selection-examples).\n\nThe simplest case is requesting a specific format, for example with `-f 22` you can download the format with format code equal to 22. You can get the list of available format codes for particular video using `--list-formats` or `-F`. Note that these format codes are extractor specific. \n\nYou can also use a file extension (currently `3gp`, `aac`, `flv`, `m4a`, `mp3`, `mp4`, `ogg`, `wav`, `webm` are supported) to download the best quality format of a particular file extension served as a single file, e.g. `-f webm` will download the best quality format with the `webm` extension served as a single file.\n\nYou can also use special names to select particular edge case formats:\n - `best`: Select the best quality format represented by a single file with video and audio.\n - `worst`: Select the worst quality format represented by a single file with video and audio.\n - `bestvideo`: Select the best quality video-only format (e.g. DASH video). May not be available.\n - `worstvideo`: Select the worst quality video-only format. May not be available.\n - `bestaudio`: Select the best quality audio only-format. May not be available.\n - `worstaudio`: Select the worst quality audio only-format. May not be available.\n\nFor example, to download the worst quality video-only format you can use `-f worstvideo`.\n\nIf you want to download multiple videos and they don't have the same formats available, you can specify the order of preference using slashes. Note that slash is left-associative, i.e. formats on the left hand side are preferred, for example `-f 22/17/18` will download format 22 if it's available, otherwise it will download format 17 if it's available, otherwise it will download format 18 if it's available, otherwise it will complain that no suitable formats are available for download.\n\nIf you want to download several formats of the same video use a comma as a separator, e.g. `-f 22,17,18` will download all these three formats, of course if they are available. Or a more sophisticated example combined with the precedence feature: `-f 136/137/mp4/bestvideo,140/m4a/bestaudio`.\n\nYou can also filter the video formats by putting a condition in brackets, as in `-f \"best[height=720]\"` (or `-f \"[filesize\u003e10M]\"`).\n\nThe following numeric meta fields can be used with comparisons `\u003c`, `\u003c=`, `\u003e`, `\u003e=`, `=` (equals), `!=` (not equals):\n - `filesize`: The number of bytes, if known in advance\n - `width`: Width of the video, if known\n - `height`: Height of the video, if known\n - `tbr`: Average bitrate of audio and video in KBit/s\n - `abr`: Average audio bitrate in KBit/s\n - `vbr`: Average video bitrate in KBit/s\n - `asr`: Audio sampling rate in Hertz\n - `fps`: Frame rate\n\nAlso filtering work for comparisons `=` (equals), `!=` (not equals), `^=` (begins with), `$=` (ends with), `*=` (contains) and following string meta fields:\n - `ext`: File extension\n - `acodec`: Name of the audio codec in use\n - `vcodec`: Name of the video codec in use\n - `container`: Name of the container format\n - `protocol`: The protocol that will be used for the actual download, lower-case (`http`, `https`, `rtsp`, `rtmp`, `rtmpe`, `mms`, `f4m`, `ism`, `http_dash_segments`, `m3u8`, or `m3u8_native`)\n - `format_id`: A short description of the format\n\nNote that none of the aforementioned meta fields are guaranteed to be present since this solely depends on the metadata obtained by particular extractor, i.e. the metadata offered by the video hoster.\n\nFormats for which the value is not known are excluded unless you put a question mark (`?`) after the operator. You can combine format filters, so `-f \"[height \u003c=? 720][tbr\u003e500]\"` selects up to 720p videos (or videos where the height is not known) with a bitrate of at least 500 KBit/s.\n\nYou can merge the video and audio of two formats into a single file using `-f \u003cvideo-format\u003e+\u003caudio-format\u003e` (requires ffmpeg or avconv installed), for example `-f bestvideo+bestaudio` will download the best video-only format, the best audio-only format and mux them together with ffmpeg/avconv.\n\nFormat selectors can also be grouped using parentheses, for example if you want to download the best mp4 and webm formats with a height lower than 480 you can use `-f '(mp4,webm)[height\u003c480]'`.\n\nSince the end of April 2015 and version 2015.04.26, youtube-dl uses `-f bestvideo+bestaudio/best` as the default format selection (see [#5447](https://github.com/rg3/youtube-dl/issues/5447), [#5456](https://github.com/rg3/youtube-dl/issues/5456)). If ffmpeg or avconv are installed this results in downloading `bestvideo` and `bestaudio` separately and muxing them together into a single file giving the best overall quality available. Otherwise it falls back to `best` and results in downloading the best available quality served as a single file. `best` is also needed for videos that don't come from YouTube because they don't provide the audio and video in two different files. If you want to only download some DASH formats (for example if you are not interested in getting videos with a resolution higher than 1080p), you can add `-f bestvideo[height\u003c=?1080]+bestaudio/best` to your configuration file. Note that if you use youtube-dl to stream to `stdout` (and most likely to pipe it to your media player then), i.e. you explicitly specify output template as `-o -`, youtube-dl still uses `-f best` format selection in order to start content delivery immediately to your player and not to wait until `bestvideo` and `bestaudio` are downloaded and muxed.\n\nIf you want to preserve the old format selection behavior (prior to youtube-dl 2015.04.26), i.e. you want to download the best available quality media served as a single file, you should explicitly specify your choice with `-f best`. You may want to add it to the [configuration file](#configuration) in order not to type it every time you run youtube-dl.\n\n#### Format selection examples\n\nNote that on Windows you may need to use double quotes instead of single.\n\n```bash\n# Download best mp4 format available or any other best if no mp4 available\n$ youtube-dl -f 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best'\n\n# Download best format available but not better that 480p\n$ youtube-dl -f 'bestvideo[height\u003c=480]+bestaudio/best[height\u003c=480]'\n\n# Download best video only format but no bigger than 50 MB\n$ youtube-dl -f 'best[filesize\u003c50M]'\n\n# Download best format available via direct link over HTTP/HTTPS protocol\n$ youtube-dl -f '(bestvideo+bestaudio/best)[protocol^=http]'\n\n# Download the best video format and the best audio format without merging them\n$ youtube-dl -f 'bestvideo,bestaudio' -o '%(title)s.f%(format_id)s.%(ext)s'\n```\nNote that in the last example, an output template is recommended as bestvideo and bestaudio may have the same file name.\n\n\n# VIDEO SELECTION\n\nVideos can be filtered by their upload date using the options `--date`, `--datebefore` or `--dateafter`. They accept dates in two formats:\n\n - Absolute dates: Dates in the format `YYYYMMDD`.\n - Relative dates: Dates in the format `(now|today)[+-][0-9](day|week|month|year)(s)?`\n \nExamples:\n\n```bash\n# Download only the videos uploaded in the last 6 months\n$ youtube-dl --dateafter now-6months\n\n# Download only the videos uploaded on January 1, 1970\n$ youtube-dl --date 19700101\n\n$ # Download only the videos uploaded in the 200x decade\n$ youtube-dl --dateafter 20000101 --datebefore 20091231\n```\n\n# FAQ\n\n### How do I update youtube-dl?\n\nIf you've followed [our manual installation instructions](https://rg3.github.io/youtube-dl/download.html), you can simply run `youtube-dl -U` (or, on Linux, `sudo youtube-dl -U`).\n\nIf you have used pip, a simple `sudo pip install -U youtube-dl` is sufficient to update.\n\nIf you have installed youtube-dl using a package manager like *apt-get* or *yum*, use the standard system update mechanism to update. Note that distribution packages are often outdated. As a rule of thumb, youtube-dl releases at least once a month, and often weekly or even daily. Simply go to https://yt-dl.org to find out the current version. Unfortunately, there is nothing we youtube-dl developers can do if your distribution serves a really outdated version. You can (and should) complain to your distribution in their bugtracker or support forum.\n\nAs a last resort, you can also uninstall the version installed by your package manager and follow our manual installation instructions. For that, remove the distribution's package, with a line like\n\n    sudo apt-get remove -y youtube-dl\n\nAfterwards, simply follow [our manual installation instructions](https://rg3.github.io/youtube-dl/download.html):\n\n```\nsudo wget https://yt-dl.org/latest/youtube-dl -O /usr/local/bin/youtube-dl\nsudo chmod a+x /usr/local/bin/youtube-dl\nhash -r\n```\n\nAgain, from then on you'll be able to update with `sudo youtube-dl -U`.\n\n### youtube-dl is extremely slow to start on Windows\n\nAdd a file exclusion for `youtube-dl.exe` in Windows Defender settings.\n\n### I'm getting an error `Unable to extract OpenGraph title` on YouTube playlists\n\nYouTube changed their playlist format in March 2014 and later on, so you'll need at least youtube-dl 2014.07.25 to download all YouTube videos.\n\nIf you have installed youtube-dl with a package manager, pip, setup.py or a tarball, please use that to update. Note that Ubuntu packages do not seem to get updated anymore. Since we are not affiliated with Ubuntu, there is little we can do. Feel free to [report bugs](https://bugs.launchpad.net/ubuntu/+source/youtube-dl/+filebug) to the [Ubuntu packaging people](mailto:ubuntu-motu@lists.ubuntu.com?subject=outdated%20version%20of%20youtube-dl) - all they have to do is update the package to a somewhat recent version. See above for a way to update.\n\n### I'm getting an error when trying to use output template: `error: using output template conflicts with using title, video ID or auto number`\n\nMake sure you are not using `-o` with any of these options `-t`, `--title`, `--id`, `-A` or `--auto-number` set in command line or in a configuration file. Remove the latter if any.\n\n### Do I always have to pass `-citw`?\n\nBy default, youtube-dl intends to have the best options (incidentally, if you have a convincing case that these should be different, [please file an issue where you explain that](https://yt-dl.org/bug)). Therefore, it is unnecessary and sometimes harmful to copy long option strings from webpages. In particular, the only option out of `-citw` that is regularly useful is `-i`.\n\n### Can you please put the `-b` option back?\n\nMost people asking this question are not aware that youtube-dl now defaults to downloading the highest available quality as reported by YouTube, which will be 1080p or 720p in some cases, so you no longer need the `-b` option. For some specific videos, maybe YouTube does not report them to be available in a specific high quality format you're interested in. In that case, simply request it with the `-f` option and youtube-dl will try to download it.\n\n### I get HTTP error 402 when trying to download a video. What's this?\n\nApparently YouTube requires you to pass a CAPTCHA test if you download too much. We're [considering to provide a way to let you solve the CAPTCHA](https://github.com/rg3/youtube-dl/issues/154), but at the moment, your best course of action is pointing a web browser to the youtube URL, solving the CAPTCHA, and restart youtube-dl.\n\n### Do I need any other programs?\n\nyoutube-dl works fine on its own on most sites. However, if you want to convert video/audio, you'll need [avconv](https://libav.org/) or [ffmpeg](https://www.ffmpeg.org/). On some sites - most notably YouTube - videos can be retrieved in a higher quality format without sound. youtube-dl will detect whether avconv/ffmpeg is present and automatically pick the best option.\n\nVideos or video formats streamed via RTMP protocol can only be downloaded when [rtmpdump](https://rtmpdump.mplayerhq.hu/) is installed. Downloading MMS and RTSP videos requires either [mplayer](https://mplayerhq.hu/) or [mpv](https://mpv.io/) to be installed.\n\n### I have downloaded a video but how can I play it?\n\nOnce the video is fully downloaded, use any video player, such as [mpv](https://mpv.io/), [vlc](https://www.videolan.org/) or [mplayer](https://www.mplayerhq.hu/).\n\n### I extracted a video URL with `-g`, but it does not play on another machine / in my web browser.\n\nIt depends a lot on the service. In many cases, requests for the video (to download/play it) must come from the same IP address and with the same cookies and/or HTTP headers. Use the `--cookies` option to write the required cookies into a file, and advise your downloader to read cookies from that file. Some sites also require a common user agent to be used, use `--dump-user-agent` to see the one in use by youtube-dl. You can also get necessary cookies and HTTP headers from JSON output obtained with `--dump-json`.\n\nIt may be beneficial to use IPv6; in some cases, the restrictions are only applied to IPv4. Some services (sometimes only for a subset of videos) do not restrict the video URL by IP address, cookie, or user-agent, but these are the exception rather than the rule.\n\nPlease bear in mind that some URL protocols are **not** supported by browsers out of the box, including RTMP. If you are using `-g`, your own downloader must support these as well.\n\nIf you want to play the video on a machine that is not running youtube-dl, you can relay the video content from the machine that runs youtube-dl. You can use `-o -` to let youtube-dl stream a video to stdout, or simply allow the player to download the files written by youtube-dl in turn.\n\n### ERROR: no fmt_url_map or conn information found in video info\n\nYouTube has switched to a new video info format in July 2011 which is not supported by old versions of youtube-dl. See [above](#how-do-i-update-youtube-dl) for how to update youtube-dl.\n\n### ERROR: unable to download video\n\nYouTube requires an additional signature since September 2012 which is not supported by old versions of youtube-dl. See [above](#how-do-i-update-youtube-dl) for how to update youtube-dl.\n\n### Video URL contains an ampersand and I'm getting some strange output `[1] 2839` or `'v' is not recognized as an internal or external command`\n\nThat's actually the output from your shell. Since ampersand is one of the special shell characters it's interpreted by the shell preventing you from passing the whole URL to youtube-dl. To disable your shell from interpreting the ampersands (or any other special characters) you have to either put the whole URL in quotes or escape them with a backslash (which approach will work depends on your shell).\n\nFor example if your URL is https://www.youtube.com/watch?t=4\u0026v=BaW_jenozKc you should end up with following command:\n\n```youtube-dl 'https://www.youtube.com/watch?t=4\u0026v=BaW_jenozKc'```\n\nor\n\n```youtube-dl https://www.youtube.com/watch?t=4\\\u0026v=BaW_jenozKc```\n\nFor Windows you have to use the double quotes:\n\n```youtube-dl \"https://www.youtube.com/watch?t=4\u0026v=BaW_jenozKc\"```\n\n### ExtractorError: Could not find JS function u'OF'\n\nIn February 2015, the new YouTube player contained a character sequence in a string that was misinterpreted by old versions of youtube-dl. See [above](#how-do-i-update-youtube-dl) for how to update youtube-dl.\n\n### HTTP Error 429: Too Many Requests or 402: Payment Required\n\nThese two error codes indicate that the service is blocking your IP address because of overuse. Contact the service and ask them to unblock your IP address, or - if you have acquired a whitelisted IP address already - use the [`--proxy` or `--source-address` options](#network-options) to select another IP address.\n\n### SyntaxError: Non-ASCII character\n\nThe error\n\n    File \"youtube-dl\", line 2\n    SyntaxError: Non-ASCII character '\\x93' ...\n\nmeans you're using an outdated version of Python. Please update to Python 2.6 or 2.7.\n\n### What is this binary file? Where has the code gone?\n\nSince June 2012 ([#342](https://github.com/rg3/youtube-dl/issues/342)) youtube-dl is packed as an executable zipfile, simply unzip it (might need renaming to `youtube-dl.zip` first on some systems) or clone the git repository, as laid out above. If you modify the code, you can run it by executing the `__main__.py` file. To recompile the executable, run `make youtube-dl`.\n\n### The exe throws an error due to missing `MSVCR100.dll`\n\nTo run the exe you need to install first the [Microsoft Visual C++ 2010 Redistributable Package (x86)](https://www.microsoft.com/en-US/download/details.aspx?id=5555).\n\n### On Windows, how should I set up ffmpeg and youtube-dl? Where should I put the exe files?\n\nIf you put youtube-dl and ffmpeg in the same directory that you're running the command from, it will work, but that's rather cumbersome.\n\nTo make a different directory work - either for ffmpeg, or for youtube-dl, or for both - simply create the directory (say, `C:\\bin`, or `C:\\Users\\\u003cUser name\u003e\\bin`), put all the executables directly in there, and then [set your PATH environment variable](https://www.java.com/en/download/help/path.xml) to include that directory.\n\nFrom then on, after restarting your shell, you will be able to access both youtube-dl and ffmpeg (and youtube-dl will be able to find ffmpeg) by simply typing `youtube-dl` or `ffmpeg`, no matter what directory you're in.\n\n### How do I put downloads into a specific folder?\n\nUse the `-o` to specify an [output template](#output-template), for example `-o \"/home/user/videos/%(title)s-%(id)s.%(ext)s\"`. If you want this for all of your downloads, put the option into your [configuration file](#configuration).\n\n### How do I download a video starting with a `-`?\n\nEither prepend `https://www.youtube.com/watch?v=` or separate the ID from the options with `--`:\n\n    youtube-dl -- -wNyEUrxzFU\n    youtube-dl \"https://www.youtube.com/watch?v=-wNyEUrxzFU\"\n\n### How do I pass cookies to youtube-dl?\n\nUse the `--cookies` option, for example `--cookies /path/to/cookies/file.txt`.\n\nIn order to extract cookies from browser use any conforming browser extension for exporting cookies. For example, [cookies.txt](https://chrome.google.com/webstore/detail/cookiestxt/njabckikapfpffapmjgojcnbfjonfjfg) (for Chrome) or [cookies.txt](https://addons.mozilla.org/en-US/firefox/addon/cookies-txt/) (for Firefox).\n\nNote that the cookies file must be in Mozilla/Netscape format and the first line of the cookies file must be either `# HTTP Cookie File` or `# Netscape HTTP Cookie File`. Make sure you have correct [newline format](https://en.wikipedia.org/wiki/Newline) in the cookies file and convert newlines if necessary to correspond with your OS, namely `CRLF` (`\\r\\n`) for Windows and `LF` (`\\n`) for Unix and Unix-like systems (Linux, macOS, etc.). `HTTP Error 400: Bad Request` when using `--cookies` is a good sign of invalid newline format.\n\nPassing cookies to youtube-dl is a good way to workaround login when a particular extractor does not implement it explicitly. Another use case is working around [CAPTCHA](https://en.wikipedia.org/wiki/CAPTCHA) some websites require you to solve in particular cases in order to get access (e.g. YouTube, CloudFlare).\n\n### How do I stream directly to media player?\n\nYou will first need to tell youtube-dl to stream media to stdout with `-o -`, and also tell your media player to read from stdin (it must be capable of this for streaming) and then pipe former to latter. For example, streaming to [vlc](https://www.videolan.org/) can be achieved with:\n\n    youtube-dl -o - \"https://www.youtube.com/watch?v=BaW_jenozKcj\" | vlc -\n\n### How do I download only new videos from a playlist?\n\nUse download-archive feature. With this feature you should initially download the complete playlist with `--download-archive /path/to/download/archive/file.txt` that will record identifiers of all the videos in a special file. Each subsequent run with the same `--download-archive` will download only new videos and skip all videos that have been downloaded before. Note that only successful downloads are recorded in the file.\n\nFor example, at first,\n\n    youtube-dl --download-archive archive.txt \"https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re\"\n\nwill download the complete `PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re` playlist and create a file `archive.txt`. Each subsequent run will only download new videos if any:\n\n    youtube-dl --download-archive archive.txt \"https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re\"\n\n### Should I add `--hls-prefer-native` into my config?\n\nWhen youtube-dl detects an HLS video, it can download it either with the built-in downloader or ffmpeg. Since many HLS streams are slightly invalid and ffmpeg/youtube-dl each handle some invalid cases better than the other, there is an option to switch the downloader if needed.\n\nWhen youtube-dl knows that one particular downloader works better for a given website, that downloader will be picked. Otherwise, youtube-dl will pick the best downloader for general compatibility, which at the moment happens to be ffmpeg. This choice may change in future versions of youtube-dl, with improvements of the built-in downloader and/or ffmpeg.\n\nIn particular, the generic extractor (used when your website is not in the [list of supported sites by youtube-dl](https://rg3.github.io/youtube-dl/supportedsites.html) cannot mandate one specific downloader.\n\nIf you put either `--hls-prefer-native` or `--hls-prefer-ffmpeg` into your configuration, a different subset of videos will fail to download correctly. Instead, it is much better to [file an issue](https://yt-dl.org/bug) or a pull request which details why the native or the ffmpeg HLS downloader is a better choice for your use case.\n\n### Can you add support for this anime video site, or site which shows current movies for free?\n\nAs a matter of policy (as well as legality), youtube-dl does not include support for services that specialize in infringing copyright. As a rule of thumb, if you cannot easily find a video that the service is quite obviously allowed to distribute (i.e. that has been uploaded by the creator, the creator's distributor, or is published under a free license), the service is probably unfit for inclusion to youtube-dl.\n\nA note on the service that they don't host the infringing content, but just link to those who do, is evidence that the service should **not** be included into youtube-dl. The same goes for any DMCA note when the whole front page of the service is filled with videos they are not allowed to distribute. A \"fair use\" note is equally unconvincing if the service shows copyright-protected videos in full without authorization.\n\nSupport requests for services that **do** purchase the rights to distribute their content are perfectly fine though. If in doubt, you can simply include a source that mentions the legitimate purchase of content.\n\n### How can I speed up work on my issue?\n\n(Also known as: Help, my important issue not being solved!) The youtube-dl core developer team is quite small. While we do our best to solve as many issues as possible, sometimes that can take quite a while. To speed up your issue, here's what you can do:\n\nFirst of all, please do report the issue [at our issue tracker](https://yt-dl.org/bugs). That allows us to coordinate all efforts by users and developers, and serves as a unified point. Unfortunately, the youtube-dl project has grown too large to use personal email as an effective communication channel.\n\nPlease read the [bug reporting instructions](#bugs) below. A lot of bugs lack all the necessary information. If you can, offer proxy, VPN, or shell access to the youtube-dl developers. If you are able to, test the issue from multiple computers in multiple countries to exclude local censorship or misconfiguration issues.\n\nIf nobody is interested in solving your issue, you are welcome to take matters into your own hands and submit a pull request (or coerce/pay somebody else to do so).\n\nFeel free to bump the issue from time to time by writing a small comment (\"Issue is still present in youtube-dl version ...from France, but fixed from Belgium\"), but please not more than once a month. Please do not declare your issue as `important` or `urgent`.\n\n### How can I detect whether a given URL is supported by youtube-dl?\n\nFor one, have a look at the [list of supported sites](docs/supportedsites.md). Note that it can sometimes happen that the site changes its URL scheme (say, from https://example.com/video/1234567 to https://example.com/v/1234567 ) and youtube-dl reports an URL of a service in that list as unsupported. In that case, simply report a bug.\n\nIt is *not* possible to detect whether a URL is supported or not. That's because youtube-dl contains a generic extractor which matches **all** URLs. You may be tempted to disable, exclude, or remove the generic extractor, but the generic extractor not only allows users to extract videos from lots of websites that embed a video from another service, but may also be used to extract video from a service that it's hosting itself. Therefore, we neither recommend nor support disabling, excluding, or removing the generic extractor.\n\nIf you want to find out whether a given URL is supported, simply call youtube-dl with it. If you get no videos back, chances are the URL is either not referring to a video or unsupported. You can find out which by examining the output (if you run youtube-dl on the console) or catching an `UnsupportedError` exception if you run it from a Python program.\n\n# Why do I need to go through that much red tape when filing bugs?\n\nBefore we had the issue template, despite our extensive [bug reporting instructions](#bugs), about 80% of the issue reports we got were useless, for instance because people used ancient versions hundreds of releases old, because of simple syntactic errors (not in youtube-dl but in general shell usage), because the problem was already reported multiple times before, because people did not actually read an error message, even if it said \"please install ffmpeg\", because people did not mention the URL they were trying to download and many more simple, easy-to-avoid problems, many of whom were totally unrelated to youtube-dl.\n\nyoutube-dl is an open-source project manned by too few volunteers, so we'd rather spend time fixing bugs where we are certain none of those simple problems apply, and where we can be reasonably confident to be able to reproduce the issue without asking the reporter repeatedly. As such, the output of `youtube-dl -v YOUR_URL_HERE` is really all that's required to file an issue. The issue template also guides you through some basic steps you can do, such as checking that your version of youtube-dl is current.\n\n# DEVELOPER INSTRUCTIONS\n\nMost users do not need to build youtube-dl and can [download the builds](https://rg3.github.io/youtube-dl/download.html) or get them from their distribution.\n\nTo run youtube-dl as a developer, you don't need to build anything either. Simply execute\n\n    python -m youtube_dl\n\nTo run the test, simply invoke your favorite test runner, or execute a test file directly; any of the following work:\n\n    python -m unittest discover\n    python test/test_download.py\n    nosetests\n\nSee item 6 of [new extractor tutorial](#adding-support-for-a-new-site) for how to run extractor specific test cases.\n\nIf you want to create a build of youtube-dl yourself, you'll need\n\n* python\n* make (only GNU make is supported)\n* pandoc\n* zip\n* nosetests\n\n### Adding support for a new site\n\nIf you want to add support for a new site, first of all **make sure** this site is **not dedicated to [copyright infringement](README.md#can-you-add-support-for-this-anime-video-site-or-site-which-shows-current-movies-for-free)**. youtube-dl does **not support** such sites thus pull requests adding support for them **will be rejected**.\n\nAfter you have ensured this site is distributing its content legally, you can follow this quick list (assuming your service is called `yourextractor`):\n\n1. [Fork this repository](https://github.com/rg3/youtube-dl/fork)\n2. Check out the source code with:\n\n        git clone git@github.com:YOUR_GITHUB_USERNAME/youtube-dl.git\n\n3. Start a new git branch with\n\n        cd youtube-dl\n        git checkout -b yourextractor\n\n4. Start with this simple template and save it to `youtube_dl/extractor/yourextractor.py`:\n\n    ```python\n    # coding: utf-8\n    from __future__ import unicode_literals\n\n    from .common import InfoExtractor\n\n\n    class YourExtractorIE(InfoExtractor):\n        _VALID_URL = r'https?://(?:www\\.)?yourextractor\\.com/watch/(?P\u003cid\u003e[0-9]+)'\n        _TEST = {\n            'url': 'https://yourextractor.com/watch/42',\n            'md5': 'TODO: md5 sum of the first 10241 bytes of the video file (use --test)',\n            'info_dict': {\n                'id': '42',\n                'ext': 'mp4',\n                'title': 'Video title goes here',\n                'thumbnail': r're:^https?://.*\\.jpg$',\n                # TODO more properties, either as:\n                # * A value\n                # * MD5 checksum; start the string with md5:\n                # * A regular expression; start the string with re:\n                # * Any Python type (for example int or float)\n            }\n        }\n\n        def _real_extract(self, url):\n            video_id = self._match_id(url)\n            webpage = self._download_webpage(url, video_id)\n\n            # TODO more code goes here, for example ...\n            title = self._html_search_regex(r'\u003ch1\u003e(.+?)\u003c/h1\u003e', webpage, 'title')\n\n            return {\n                'id': video_id,\n                'title': title,\n                'description': self._og_search_description(webpage),\n                'uploader': self._search_regex(r'\u003cdiv[^\u003e]+id=\"uploader\"[^\u003e]*\u003e([^\u003c]+)\u003c', webpage, 'uploader', fatal=False),\n                # TODO more properties (see youtube_dl/extractor/common.py)\n            }\n    ```\n5. Add an import in [`youtube_dl/extractor/extractors.py`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/extractors.py).\n6. Run `python test/test_download.py TestDownload.test_YourExtractor`. This *should fail* at first, but you can continually re-run it until you're done. If you decide to add more than one test, then rename ``_TEST`` to ``_TESTS`` and make it into a list of dictionaries. The tests will then be named `TestDownload.test_YourExtractor`, `TestDownload.test_YourExtractor_1`, `TestDownload.test_YourExtractor_2`, etc. Note that tests with `only_matching` key in test's dict are not counted in.\n7. Have a look at [`youtube_dl/extractor/common.py`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/extractor/common.py) for possible helper methods and a [detailed description of what your extractor should and may return](https://github.com/rg3/youtube-dl/blob/7f41a598b3fba1bcab2817de64a08941200aa3c8/youtube_dl/extractor/common.py#L94-L303). Add tests and code for as many as you want.\n8. Make sure your code follows [youtube-dl coding conventions](#youtube-dl-coding-conventions) and check the code with [flake8](https://pypi.python.org/pypi/flake8). Also make sure your code works under all [Python](https://www.python.org/) versions claimed supported by youtube-dl, namely 2.6, 2.7, and 3.2+.\n9. When the tests pass, [add](https://git-scm.com/docs/git-add) the new files and [commit](https://git-scm.com/docs/git-commit) them and [push](https://git-scm.com/docs/git-push) the result, like this:\n\n        $ git add youtube_dl/extractor/extractors.py\n        $ git add youtube_dl/extractor/yourextractor.py\n        $ git commit -m '[yourextractor] Add new extractor'\n        $ git push origin yourextractor\n\n10. Finally, [create a pull request](https://help.github.com/articles/creating-a-pull-request). We'll then review and merge it.\n\nIn any case, thank you very much for your contributions!\n\n## youtube-dl coding conventions\n\nThis section introduces a guide lines for writing idiomatic, robust and future-proof extractor code.\n\nExtractors are very fragile by nature since they depend on the layout of the source data provided by 3rd party media hosters out of your control and this layout tends to change. As an extractor implementer your task is not only to write code that will extract media links and metadata correctly but also to minimize dependency on the source's layout and even to make the code foresee potential future changes and be ready for that. This is important because it will allow the extractor not to break on minor layout changes thus keeping old youtube-dl versions working. Even though this breakage issue is easily fixed by emitting a new version of youtube-dl with a fix incorporated, all the previous versions become broken in all repositories and distros' packages that may not be so prompt in fetching the update from us. Needless to say, some non rolling release distros may never receive an update at all.\n\n### Mandatory and optional metafields\n\nFor extraction to work youtube-dl relies on metadata your extractor extracts and provides to youtube-dl expressed by an [information dictionary](https://github.com/rg3/youtube-dl/blob/7f41a598b3fba1bcab2817de64a08941200aa3c8/youtube_dl/extractor/common.py#L94-L303) or simply *info dict*. Only the following meta fields in the *info dict* are considered mandatory for a successful extraction process by youtube-dl:\n\n - `id` (media identifier)\n - `title` (media title)\n - `url` (media download URL) or `formats`\n\nIn fact only the last option is technically mandatory (i.e. if you can't figure out the download location of the media the extraction does not make any sense). But by convention youtube-dl also treats `id` and `title` as mandatory. Thus the aforementioned metafields are the critical data that the extraction does not make any sense without and if any of them fail to be extracted then the extractor is considered completely broken.\n\n[Any field](https://github.com/rg3/youtube-dl/blob/7f41a598b3fba1bcab2817de64a08941200aa3c8/youtube_dl/extractor/common.py#L188-L303) apart from the aforementioned ones are considered **optional**. That means that extraction should be **tolerant** to situations when sources for these fields can potentially be unavailable (even if they are always available at the moment) and **future-proof** in order not to break the extraction of general purpose mandatory fields.\n\n#### Example\n\nSay you have some source dictionary `meta` that you've fetched as JSON with HTTP request and it has a key `summary`:\n\n```python\nmeta = self._download_json(url, video_id)\n```\n    \nAssume at this point `meta`'s layout is:\n\n```python\n{\n    ...\n    \"summary\": \"some fancy summary text\",\n    ...\n}\n```\n\nAssume you want to extract `summary` and put it into the resulting info dict as `description`. Since `description` is an optional meta field you should be ready that this key may be missing from the `meta` dict, so that you should extract it like:\n\n```python\ndescription = meta.get('summary')  # correct\n```\n\nand not like:\n\n```python\ndescription = meta['summary']  # incorrect\n```\n\nThe latter will break extraction process with `KeyError` if `summary` disappears from `meta` at some later time but with the former approach extraction will just go ahead with `description` set to `None` which is perfectly fine (remember `None` is equivalent to the absence of data).\n\nSimilarly, you should pass `fatal=False` when extracting optional data from a webpage with `_search_regex`, `_html_search_regex` or similar methods, for instance:\n\n```python\ndescription = self._search_regex(\n    r'\u003cspan[^\u003e]+id=\"title\"[^\u003e]*\u003e([^\u003c]+)\u003c',\n    webpage, 'description', fatal=False)\n```\n\nWith `fatal` set to `False` if `_search_regex` fails to extract `description` it will emit a warning and continue extraction.\n\nYou can also pass `default=\u003csome fallback value\u003e`, for example:\n\n```python\ndescription = self._search_regex(\n    r'\u003cspan[^\u003e]+id=\"title\"[^\u003e]*\u003e([^\u003c]+)\u003c',\n    webpage, 'description', default=None)\n```\n\nOn failure this code will silently continue the extraction with `description` set to `None`. That is useful for metafields that may or may not be present.\n \n### Provide fallbacks\n\nWhen extracting metadata try to do so from multiple sources. For example if `title` is present in several places, try extracting from at least some of them. This makes it more future-proof in case some of the sources become unavailable.\n\n#### Example\n\nSay `meta` from the previous example has a `title` and you are about to extract it. Since `title` is a mandatory meta field you should end up with something like:\n\n```python\ntitle = meta['title']\n```\n\nIf `title` disappears from `meta` in future due to some changes on the hoster's side the extraction would fail since `title` is mandatory. That's expected.\n\nAssume that you have some another source you can extract `title` from, for example `og:title` HTML meta of a `webpage`. In this case you can provide a fallback scenario:\n\n```python\ntitle = meta.get('title') or self._og_search_title(webpage)\n```\n\nThis code will try to extract from `meta` first and if it fails it will try extracting `og:title` from a `webpage`.\n\n### Make regular expressions flexible\n\nWhen using regular expressions try to write them fuzzy and flexible.\n \n#### Example\n\nSay you need to extract `title` from the following HTML code:\n\n```html\n\u003cspan style=\"position: absolute; left: 910px; width: 90px; float: right; z-index: 9999;\" class=\"title\"\u003esome fancy title\u003c/span\u003e\n```\n\nThe code for that task should look similar to:\n\n```python\ntitle = self._search_regex(\n    r'\u003cspan[^\u003e]+class=\"title\"[^\u003e]*\u003e([^\u003c]+)', webpage, 'title')\n```\n\nOr even better:\n\n```python\ntitle = self._search_regex(\n    r'\u003cspan[^\u003e]+class=([\"\\'])title\\1[^\u003e]*\u003e(?P\u003ctitle\u003e[^\u003c]+)',\n    webpage, 'title', group='title')\n```\n\nNote how you tolerate potential changes in the `style` attribute's value or switch from using double quotes to single for `class` attribute: \n\nThe code definitely should not look like:\n\n```python\ntitle = self._search_regex(\n    r'\u003cspan style=\"position: absolute; left: 910px; width: 90px; float: right; z-index: 9999;\" class=\"title\"\u003e(.*?)\u003c/span\u003e',\n    webpage, 'title', group='title')\n```\n\n### Use safe conversion functions\n\nWrap all extracted numeric data into safe functions from [`youtube_dl/utils.py`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/utils.py): `int_or_none`, `float_or_none`. Use them for string to number conversions as well.\n\nUse `url_or_none` for safe URL processing.\n\nUse `try_get` for safe metadata extraction from parsed JSON.\n\nExplore [`youtube_dl/utils.py`](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/utils.py) for more useful convenience functions.\n\n#### More examples\n\n##### Safely extract optional description from parsed JSON\n```python\ndescription = try_get(response, lambda x: x['result']['video'][0]['summary'], compat_str)\n```\n\n##### Safely extract more optional metadata\n```python\nvideo = try_get(response, lambda x: x['result']['video'][0], dict) or {}\ndescription = video.get('summary')\nduration = float_or_none(video.get('durationMs'), scale=1000)\nview_count = int_or_none(video.get('views'))\n```\n\n# EMBEDDING YOUTUBE-DL\n\nyoutube-dl makes the best effort to be a good command-line program, and thus should be callable from any programming language. If you encounter any problems parsing its output, feel free to [create a report](https://github.com/rg3/youtube-dl/issues/new).\n\nFrom a Python program, you can embed youtube-dl in a more powerful fashion, like this:\n\n```python\nfrom __future__ import unicode_literals\nimport youtube_dl\n\nydl_opts = {}\nwith youtube_dl.YoutubeDL(ydl_opts) as ydl:\n    ydl.download(['https://www.youtube.com/watch?v=BaW_jenozKc'])\n```\n\nMost likely, you'll want to use various options. For a list of options available, have a look at [`youtube_dl/YoutubeDL.py`](https://github.com/rg3/youtube-dl/blob/3e4cedf9e8cd3157df2457df7274d0c842421945/youtube_dl/YoutubeDL.py#L137-L312). For a start, if you want to intercept youtube-dl's output, set a `logger` object.\n\nHere's a more complete example of a program that outputs only errors (and a short message after the download is finished), and downloads/converts the video to an mp3 file:\n\n```python\nfrom __future__ import unicode_literals\nimport youtube_dl\n\n\nclass MyLogger(object):\n    def debug(self, msg):\n        pass\n\n    def warning(self, msg):\n        pass\n\n    def error(self, msg):\n        print(msg)\n\n\ndef my_hook(d):\n    if d['status'] == 'finished':\n        print('Done downloading, now converting ...')\n\n\nydl_opts = {\n    'format': 'bestaudio/best',\n    'postprocessors': [{\n        'key': 'FFmpegExtractAudio',\n        'preferredcodec': 'mp3',\n        'preferredquality': '192',\n    }],\n    'logger': MyLogger(),\n    'progress_hooks': [my_hook],\n}\nwith youtube_dl.YoutubeDL(ydl_opts) as ydl:\n    ydl.download(['https://www.youtube.com/watch?v=BaW_jenozKc'])\n```\n\n# BUGS\n\nBugs and suggestions should be reported at: \u003chttps://github.com/rg3/youtube-dl/issues\u003e. Unless you were prompted to or there is another pertinent reason (e.g. GitHub fails to accept the bug report), please do not send bug reports via personal email. For discussions, join us in the IRC channel [#youtube-dl](irc://chat.freenode.net/#youtube-dl) on freenode ([webchat](https://webchat.freenode.net/?randomnick=1\u0026channels=youtube-dl)).\n\n**Please include the full output of youtube-dl when run with `-v`**, i.e. **add** `-v` flag to **your command line**, copy the **whole** output and post it in the issue body wrapped in \\`\\`\\` for better formatting. It should look similar to this:\n```\n$ youtube-dl -v \u003cyour command line\u003e\n[debug] System config: []\n[debug] User config: []\n[debug] Command-line args: [u'-v', u'https://www.youtube.com/watch?v=BaW_jenozKcj']\n[debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251\n[debug] youtube-dl version 2015.12.06\n[debug] Git HEAD: 135392e\n[debug] Python version 2.6.6 - Windows-2003Server-5.2.3790-SP2\n[debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4\n[debug] Proxy map: {}\n...\n```\n**Do not post screenshots of verbose logs; only plain text is acceptable.**\n\nThe output (including the first lines) contains important debugging information. Issues without the full output are often not reproducible and therefore do not get solved in short order, if ever.\n\nPlease re-read your issue once again to avoid a couple of common mistakes (you can and should use this as a checklist):\n\n### Is the description of the issue itself sufficient?\n\nWe often get issue reports that we cannot really decipher. While in most cases we eventually get the required information after asking back multiple times, this poses an unnecessary drain on our resources. Many contributors, including myself, are also not native speakers, so we may misread some parts.\n\nSo please elaborate on what feature you are requesting, or what bug you want to be fixed. Make sure that it's obvious\n\n- What the problem is\n- How it could be fixed\n- How your proposed solution would look like\n\nIf your report is shorter than two lines, it is almost certainly missing some of these, which makes it hard for us to respond to it. We're often too polite to close the issue outright, but the missing info makes misinterpretation likely. As a committer myself, I often get frustrated by these issues, since the only possible way for me to move forward on them is to ask for clarification over and over.\n\nFor bug reports, this means that your report should contain the *complete* output of youtube-dl when called with the `-v` flag. The error message you get for (most) bugs even says so, but you would not believe how many of our bug reports do not contain this information.\n\nIf your server has multiple IPs or you suspect censorship, adding `--call-home` may be a good idea to get more diagnostics. If the error is `ERROR: Unable to extract ...` and you cannot reproduce it from multiple countries, add `--dump-pages` (warning: this will yield a rather large output, redirect it to the file `log.txt` by adding `\u003elog.txt 2\u003e\u00261` to your command-line) or upload the `.dump` files you get when you add `--write-pages` [somewhere](https://gist.github.com/).\n\n**Site support requests must contain an example URL**. An example URL is a URL you might want to download, like `https://www.youtube.com/watch?v=BaW_jenozKc`. There should be an obvious video present. Except under very special circumstances, the main page of a video service (e.g. `https://www.youtube.com/`) is *not* an example URL.\n\n###  Are you using the latest version?\n\nBefore reporting any issue, type `youtube-dl -U`. This should report that you're up-to-date. About 20% of the reports we receive are already fixed, but people are using outdated versions. This goes for feature requests as well.\n\n###  Is the issue already documented?\n\nMake sure that someone has not already opened the issue you're trying to open. Search at the top of the window or browse the [GitHub Issues](https://github.com/rg3/youtube-dl/search?type=Issues) of this repository. If there is an issue, feel free to write something along the lines of \"This affects me as well, with version 2015.01.01. Here is some more information on the issue: ...\". While some issues may be old, a new post into them often spurs rapid activity.\n\n###  Why are existing options not enough?\n\nBefore requesting a new feature, please have a quick peek at [the list of supported options](https://github.com/rg3/youtube-dl/blob/master/README.md#options). Many feature requests are for features that actually exist already! Please, absolutely do show off your work in the issue report and detail how the existing similar options do *not* solve your problem.\n\n###  Is there enough context in your bug report?\n\nPeople want to solve problems, and often think they do us a favor by breaking down their larger problems (e.g. wanting to skip already downloaded files) to a specific request (e.g. requesting us to look whether the file exists before downloading the info page). However, what often happens is that they break down the problem into two steps: One simple, and one impossible (or extremely complicated one).\n\nWe are then presented with a very complicated request when the original problem could be solved far easier, e.g. by recording the downloaded video IDs in a separate file. To avoid this, you must include the greater context where it is non-obvious. In particular, every feature request that does not consist of adding support for a new site should contain a use case scenario that explains in what situation the missing feature would be useful.\n\n###  Does the issue involve one problem, and one problem only?\n\nSome of our users seem to think there is a limit of issues they can or should open. There is no limit of issues they can or should open. While it may seem appealing to be able to dump all your issues into one ticket, that means that someone who solves one of your issues cannot mark the issue as closed. Typically, reporting a bunch of issues leads to the ticket lingering since nobody wants to attack that behemoth, until someone mercifully splits the issue into multiple ones.\n\nIn particular, every site support request issue should only pertain to services at one site (generally under a common domain, but always using the same backend technology). Do not request support for vimeo user videos, White house podcasts, and Google Plus pages in the same issue. Also, make sure that you don't post bug reports alongside feature requests. As a rule of thumb, a feature request does not include outputs of youtube-dl that are not immediately related to the feature at hand. Do not post reports of a network error alongside the request for a new video service.\n\n###  Is anyone going to need the feature?\n\nOnly post features that you (or an incapacitated friend you can personally talk to) require. Do not post features because they seem like a good idea. If they are really useful, they will be requested by someone who requires them.\n\n###  Is your question about youtube-dl?\n\nIt may sound strange, but some bug reports we receive are completely unrelated to youtube-dl and relate to a different, or even the reporter's own, application. Please make sure that you are actually using youtube-dl. If you are using a UI for youtube-dl, report the bug to the maintainer of the actual application providing the UI. On the other hand, if your UI for youtube-dl fails in some way you believe is related to youtube-dl, by all means, go ahead and report the bug.\n\n# COPYRIGHT\n\nyoutube-dl is released into the public domain by the copyright holders.\n\nThis README file was originally written by [Daniel Bolton](https://github.com/dbbolton) and is likewise released into the public domain.\n"
  },
  {
    "repo": "pallets/flask",
    "content": "Flask\n=====\n\nFlask is a lightweight `WSGI`_ web application framework. It is designed\nto make getting started quick and easy, with the ability to scale up to\ncomplex applications. It began as a simple wrapper around `Werkzeug`_\nand `Jinja`_ and has become one of the most popular Python web\napplication frameworks.\n\nFlask offers suggestions, but doesn't enforce any dependencies or\nproject layout. It is up to the developer to choose the tools and\nlibraries they want to use. There are many extensions provided by the\ncommunity that make adding new functionality easy.\n\n\nInstalling\n----------\n\nInstall and update using `pip`_:\n\n.. code-block:: text\n\n    pip install -U Flask\n\n\nA Simple Example\n----------------\n\n.. code-block:: python\n\n    from flask import Flask\n\n    app = Flask(__name__)\n\n    @app.route('/')\n    def hello():\n        return 'Hello, World!'\n\n.. code-block:: text\n\n    $ env FLASK_APP=hello.py flask run\n     * Serving Flask app \"hello\"\n     * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n\n\nContributing\n------------\n\nFor guidance on setting up a development environment and how to make a\ncontribution to Flask, see the `contributing guidelines`_.\n\n.. _contributing guidelines: https://github.com/pallets/flask/blob/master/CONTRIBUTING.rst\n\n\nDonate\n------\n\nThe Pallets organization develops and supports Flask and the libraries\nit uses. In order to grow the community of contributors and users, and\nallow the maintainers to devote more time to the projects, `please\ndonate today`_.\n\n.. _please donate today: https://psfmember.org/civicrm/contribute/transact?reset=1\u0026id=20\n\n\nLinks\n-----\n\n* Website: https://www.palletsprojects.com/p/flask/\n* Documentation: http://flask.pocoo.org/docs/\n* License: `BSD \u003chttps://github.com/pallets/flask/blob/master/LICENSE\u003e`_\n* Releases: https://pypi.org/project/Flask/\n* Code: https://github.com/pallets/flask\n* Issue tracker: https://github.com/pallets/flask/issues\n* Test status:\n\n  * Linux, Mac: https://travis-ci.org/pallets/flask\n  * Windows: https://ci.appveyor.com/project/pallets/flask\n\n* Test coverage: https://codecov.io/gh/pallets/flask\n\n.. _WSGI: https://wsgi.readthedocs.io\n.. _Werkzeug: https://www.palletsprojects.com/p/werkzeug/\n.. _Jinja: https://www.palletsprojects.com/p/jinja/\n.. _pip: https://pip.pypa.io/en/stable/quickstart/\n"
  },
  {
    "repo": "nvbn/thefuck",
    "content": "# The Fuck [![Version][version-badge]][version-link] [![Build Status][travis-badge]][travis-link] [![Windows Build Status][appveyor-badge]][appveyor-link] [![Coverage][coverage-badge]][coverage-link] [![MIT License][license-badge]](LICENSE.md)\n\n*The Fuck* is a magnificent app, inspired by a [@liamosaur](https://twitter.com/liamosaur/)\n[tweet](https://twitter.com/liamosaur/status/506975850596536320),\nthat corrects errors in previous console commands.\n\n\nIs *The Fuck* too slow? [Try the experimental instant mode!](#experimental-instant-mode)\n\n[![gif with examples][examples-link]][examples-link]\n\nMore examples:\n\n```bash\n➜ apt-get install vim\nE: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)\nE: Unable to lock the administration directory (/var/lib/dpkg/), are you root?\n\n➜ fuck\nsudo apt-get install vim [enter/↑/↓/ctrl+c]\n[sudo] password for nvbn:\nReading package lists... Done\n...\n```\n\n```bash\n➜ git push\nfatal: The current branch master has no upstream branch.\nTo push the current branch and set the remote as upstream, use\n\n    git push --set-upstream origin master\n\n\n➜ fuck\ngit push --set-upstream origin master [enter/↑/↓/ctrl+c]\nCounting objects: 9, done.\n...\n```\n\n```bash\n➜ puthon\nNo command 'puthon' found, did you mean:\n Command 'python' from package 'python-minimal' (main)\n Command 'python' from package 'python3' (main)\nzsh: command not found: puthon\n\n➜ fuck\npython [enter/↑/↓/ctrl+c]\nPython 3.4.2 (default, Oct  8 2014, 13:08:17)\n...\n```\n\n```bash\n➜ git brnch\ngit: 'brnch' is not a git command. See 'git --help'.\n\nDid you mean this?\n    branch\n\n➜ fuck\ngit branch [enter/↑/↓/ctrl+c]\n* master\n```\n\n```bash\n➜ lein rpl\n'rpl' is not a task. See 'lein help'.\n\nDid you mean this?\n         repl\n\n➜ fuck\nlein repl [enter/↑/↓/ctrl+c]\nnREPL server started on port 54848 on host 127.0.0.1 - nrepl://127.0.0.1:54848\nREPL-y 0.3.1\n...\n```\n\nIf you're not afraid of blindly running corrected commands, the\n`require_confirmation` [settings](#settings) option can be disabled:\n\n```bash\n➜ apt-get install vim\nE: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)\nE: Unable to lock the administration directory (/var/lib/dpkg/), are you root?\n\n➜ fuck\nsudo apt-get install vim\n[sudo] password for nvbn:\nReading package lists... Done\n...\n```\n\n## Requirements\n\n- python (3.4+)\n- pip\n- python-dev\n\n## Installation\n\nOn OS X, you can install *The Fuck* via [Homebrew][homebrew]:\n\n```bash\nbrew install thefuck\n```\n\nOn Ubuntu / Mint, install *The Fuck* with the following commands:\n```bash\nsudo apt update\nsudo apt install python3-dev python3-pip python3-setuptools\nsudo pip3 install thefuck\n```\n\nOn FreeBSD, install *The Fuck* with the following commands:\n```bash\nsudo portsnap fetch update\ncd /usr/ports/misc/thefuck \u0026\u0026 sudo make install clean\n```\n\nOn ChromeOS, install *The Fuck* using [chromebrew](https://github.com/skycocker/chromebrew) with the following command:\n```bash\ncrew install thefuck\n```\n\nOn other systems, install *The Fuck*  by using `pip`:\n\n```bash\npip install thefuck\n```\n\n[Alternatively, you may use an OS package manager (OS X, Ubuntu, Arch).](https://github.com/nvbn/thefuck/wiki/Installation)\n\n\u003ca href='#manual-installation' name='manual-installation'\u003e#\u003c/a\u003e\nIt is recommended that you place this command in your `.bash_profile`,\n`.bashrc`, `.zshrc` or other startup script:\n\n```bash\neval $(thefuck --alias)\n# You can use whatever you want as an alias, like for Mondays:\neval $(thefuck --alias FUCK)\n```\n\n[Or in your shell config (Bash, Zsh, Fish, Powershell, tcsh).](https://github.com/nvbn/thefuck/wiki/Shell-aliases)\n\nChanges are only available in a new shell session. To make changes immediately\navailable, run `source ~/.bashrc` (or your shell config file like `.zshrc`).\n\nTo run fixed commands without confirmation, use the `--yeah` option (or just `-y` for short):\n\n```bash\nfuck --yeah\n```\n\nTo fix commands recursively until succeeding, use the `-r` option:\n\n```bash\nfuck -r\n```\n\n## Updating\n\n```bash\npip3 install thefuck --upgrade\n```\n\n**Note: Alias functionality was changed in v1.34 of *The Fuck***\n\n## How it works\n\n*The Fuck* attempts to match the previous command with a rule. If a match is\nfound, a new command is created using the matched rule and executed. The\nfollowing rules are enabled by default:\n\n* `adb_unknown_command` \u0026ndash; fixes misspelled commands like `adb logcta`;\n* `ag_literal` \u0026ndash; adds `-Q` to `ag` when suggested;\n* `aws_cli` \u0026ndash; fixes misspelled commands like `aws dynamdb scan`;\n* `az_cli` \u0026ndash; fixes misspelled commands like `az providers`;\n* `cargo` \u0026ndash; runs `cargo build` instead of `cargo`;\n* `cargo_no_command` \u0026ndash; fixes wrongs commands like `cargo buid`;\n* `cat_dir` \u0026ndash; replaces `cat` with `ls` when you try to `cat` a directory;\n* `cd_correction` \u0026ndash; spellchecks and correct failed cd commands;\n* `cd_mkdir` \u0026ndash; creates directories before cd'ing into them;\n* `cd_parent` \u0026ndash; changes `cd..` to `cd ..`;\n* `chmod_x` \u0026ndash; add execution bit;\n* `composer_not_command` \u0026ndash; fixes composer command name;\n* `cp_omitting_directory` \u0026ndash; adds `-a` when you `cp` directory;\n* `cpp11` \u0026ndash; adds missing `-std=c++11` to `g++` or `clang++`;\n* `dirty_untar` \u0026ndash; fixes `tar x` command that untarred in the current directory;\n* `dirty_unzip` \u0026ndash; fixes `unzip` command that unzipped in the current directory;\n* `django_south_ghost` \u0026ndash; adds `--delete-ghost-migrations` to failed because ghosts django south migration;\n* `django_south_merge` \u0026ndash; adds `--merge` to inconsistent django south migration;\n* `docker_not_command` \u0026ndash; fixes wrong docker commands like `docker tags`;\n* `dry` \u0026ndash; fixes repetitions like `git git push`;\n* `fab_command_not_found` \u0026ndash; fix misspelled fabric commands;\n* `fix_alt_space` \u0026ndash; replaces Alt+Space with Space character;\n* `fix_file` \u0026ndash; opens a file with an error in your `$EDITOR`;\n* `gem_unknown_command` \u0026ndash; fixes wrong `gem` commands;\n* `git_add` \u0026ndash; fixes *\"pathspec 'foo' did not match any file(s) known to git.\"*;\n* `git_add_force` \u0026ndash; adds `--force` to `git add \u003cpathspec\u003e...` when paths are .gitignore'd;\n* `git_bisect_usage` \u0026ndash; fixes `git bisect strt`, `git bisect goood`, `git bisect rset`, etc. when bisecting;\n* `git_branch_delete` \u0026ndash; changes `git branch -d` to `git branch -D`;\n* `git_branch_exists` \u0026ndash; offers `git branch -d foo`, `git branch -D foo` or `git checkout foo` when creating a branch that already exists;\n* `git_branch_list` \u0026ndash; catches `git branch list` in place of `git branch` and removes created branch;\n* `git_checkout` \u0026ndash; fixes branch name or creates new branch;\n* `git_commit_amend` \u0026ndash; offers `git commit --amend` after previous commit;\n* `git_diff_no_index` \u0026ndash; adds `--no-index` to previous `git diff` on untracked files;\n* `git_diff_staged` \u0026ndash; adds `--staged` to previous `git diff` with unexpected output;\n* `git_fix_stash` \u0026ndash; fixes `git stash` commands (misspelled subcommand and missing `save`);\n* `git_flag_after_filename` \u0026ndash; fixes `fatal: bad flag '...' after filename`\n* `git_help_aliased` \u0026ndash; fixes `git help \u003calias\u003e` commands replacing \u003calias\u003e with the aliased command;\n* `git_merge` \u0026ndash; adds remote to branch names;\n* `git_merge_unrelated` \u0026ndash; adds `--allow-unrelated-histories` when required\n* `git_not_command` \u0026ndash; fixes wrong git commands like `git brnch`;\n* `git_pull` \u0026ndash; sets upstream before executing previous `git pull`;\n* `git_pull_clone` \u0026ndash; clones instead of pulling when the repo does not exist;\n* `git_pull_uncommitted_changes` \u0026ndash; stashes changes before pulling and pops them afterwards;\n* `git_push` \u0026ndash; adds `--set-upstream origin $branch` to previous failed `git push`;\n* `git_push_different_branch_names` \u0026ndash; fixes pushes when local brach name does not match remote branch name;\n* `git_push_pull` \u0026ndash; runs `git pull` when `push` was rejected;\n* `git_push_without_commits` \u0026ndash; Creates an initial commit if you forget and only `git add .`, when setting up a new project;\n* `git_rebase_no_changes` \u0026ndash; runs `git rebase --skip` instead of `git rebase --continue` when there are no changes;\n* `git_remote_delete` \u0026ndash; replaces `git remote delete remote_name` with `git remote remove remote_name`;\n* `git_rm_local_modifications` \u0026ndash;  adds `-f` or `--cached` when you try to `rm` a locally modified file;\n* `git_rm_recursive` \u0026ndash; adds `-r` when you try to `rm` a directory;\n* `git_rm_staged` \u0026ndash;  adds `-f` or `--cached` when you try to `rm` a file with staged changes\n* `git_rebase_merge_dir` \u0026ndash; offers `git rebase (--continue | --abort | --skip)` or removing the `.git/rebase-merge` dir when a rebase is in progress;\n* `git_remote_seturl_add` \u0026ndash; runs `git remote add` when `git remote set_url` on nonexistant remote;\n* `git_stash` \u0026ndash; stashes your local modifications before rebasing or switching branch;\n* `git_stash_pop` \u0026ndash; adds your local modifications before popping stash, then resets;\n* `git_tag_force` \u0026ndash; adds `--force` to `git tag \u003ctagname\u003e` when the tag already exists;\n* `git_two_dashes` \u0026ndash; adds a missing dash to commands like `git commit -amend` or `git rebase -continue`;\n* `go_run` \u0026ndash; appends `.go` extension when compiling/running Go programs;\n* `gradle_no_task` \u0026ndash; fixes not found or ambiguous `gradle` task;\n* `gradle_wrapper` \u0026ndash; replaces `gradle` with `./gradlew`;\n* `grep_arguments_order` \u0026ndash; fixes `grep` arguments order for situations like `grep -lir . test`;\n* `grep_recursive` \u0026ndash; adds `-r` when you try to `grep` directory;\n* `grunt_task_not_found` \u0026ndash; fixes misspelled `grunt` commands;\n* `gulp_not_task` \u0026ndash; fixes misspelled `gulp` tasks;\n* `has_exists_script` \u0026ndash; prepends `./` when script/binary exists;\n* `heroku_multiple_apps` \u0026ndash; add `--app \u003capp\u003e` to `heroku` commands like `heroku pg`;\n* `heroku_not_command` \u0026ndash; fixes wrong `heroku` commands like `heroku log`;\n* `history` \u0026ndash; tries to replace command with most similar command from history;\n* `hostscli` \u0026ndash; tries to fix `hostscli` usage;\n* `ifconfig_device_not_found` \u0026ndash; fixes wrong device names like `wlan0` to `wlp2s0`;\n* `java` \u0026ndash; removes `.java` extension when running Java programs;\n* `javac` \u0026ndash; appends missing `.java` when compiling Java files;\n* `lein_not_task` \u0026ndash; fixes wrong `lein` tasks like `lein rpl`;\n* `long_form_help` \u0026ndash; changes `-h` to `--help` when the short form version is not supported\n* `ln_no_hard_link` \u0026ndash; catches hard link creation on directories, suggest symbolic link;\n* `ln_s_order` \u0026ndash; fixes `ln -s` arguments order;\n* `ls_all` \u0026ndash; adds `-A` to `ls` when output is empty;\n* `ls_lah` \u0026ndash; adds `-lah` to `ls`;\n* `man` \u0026ndash; changes manual section;\n* `man_no_space` \u0026ndash; fixes man commands without spaces, for example `mandiff`;\n* `mercurial` \u0026ndash; fixes wrong `hg` commands;\n* `missing_space_before_subcommand` \u0026ndash; fixes command with missing space like `npminstall`;\n* `mkdir_p` \u0026ndash; adds `-p` when you try to create a directory without parent;\n* `mvn_no_command` \u0026ndash; adds `clean package` to `mvn`;\n* `mvn_unknown_lifecycle_phase` \u0026ndash; fixes misspelled lifecycle phases with `mvn`;\n* `npm_missing_script` \u0026ndash; fixes `npm` custom script name in `npm run-script \u003cscript\u003e`;\n* `npm_run_script` \u0026ndash; adds missing `run-script` for custom `npm` scripts;\n* `npm_wrong_command` \u0026ndash; fixes wrong npm commands like `npm urgrade`;\n* `no_command` \u0026ndash; fixes wrong console commands, for example `vom/vim`;\n* `no_such_file` \u0026ndash; creates missing directories with `mv` and `cp` commands;\n* `open` \u0026ndash; either prepends `http://` to address passed to `open` or create a new file or directory and passes it to `open`;\n* `pip_unknown_command` \u0026ndash; fixes wrong `pip` commands, for example `pip instatl/pip install`;\n* `php_s` \u0026ndash; replaces `-s` by `-S` when trying to run a local php server;\n* `port_already_in_use` \u0026ndash; kills process that bound port;\n* `prove_recursively` \u0026ndash; adds `-r` when called with directory;\n* `python_command` \u0026ndash; prepends `python` when you try to run non-executable/without `./` python script;\n* `python_execute` \u0026ndash; appends missing `.py` when executing Python files;\n* `quotation_marks` \u0026ndash; fixes uneven usage of `'` and `\"` when containing args';\n* `path_from_history` \u0026ndash; replaces not found path with similar absolute path from history;\n* `react_native_command_unrecognized` \u0026ndash; fixes unrecognized `react-native` commands;\n* `remove_trailing_cedilla` \u0026ndash; remove trailling cedillas `ç`, a common typo for european keyboard layouts;\n* `rm_dir` \u0026ndash; adds `-rf` when you try to remove a directory;\n* `scm_correction` \u0026ndash; corrects wrong scm like `hg log` to `git log`;\n* `sed_unterminated_s` \u0026ndash; adds missing '/' to `sed`'s `s` commands;\n* `sl_ls` \u0026ndash; changes `sl` to `ls`;\n* `ssh_known_hosts` \u0026ndash; removes host from `known_hosts` on warning;\n* `sudo` \u0026ndash; prepends `sudo` to previous command if it failed because of permissions;\n* `sudo_command_from_user_path` \u0026ndash; runs commands from users `$PATH` with `sudo`;\n* `switch_lang` \u0026ndash; switches command from your local layout to en;\n* `systemctl` \u0026ndash; correctly orders parameters of confusing `systemctl`;\n* `test.py` \u0026ndash; runs `py.test` instead of `test.py`;\n* `touch` \u0026ndash; creates missing directories before \"touching\";\n* `tsuru_login` \u0026ndash; runs `tsuru login` if not authenticated or session expired;\n* `tsuru_not_command` \u0026ndash; fixes wrong `tsuru` commands like `tsuru shell`;\n* `tmux` \u0026ndash; fixes `tmux` commands;\n* `unknown_command` \u0026ndash; fixes hadoop hdfs-style \"unknown command\", for example adds missing '-' to the command on `hdfs dfs ls`;\n* `unsudo` \u0026ndash; removes `sudo` from previous command if a process refuses to run on super user privilege.\n* `vagrant_up` \u0026ndash; starts up the vagrant instance;\n* `whois` \u0026ndash; fixes `whois` command;\n* `workon_doesnt_exists` \u0026ndash; fixes `virtualenvwrapper` env name os suggests to create new.\n* `yarn_alias` \u0026ndash; fixes aliased `yarn` commands like `yarn ls`;\n* `yarn_command_not_found` \u0026ndash; fixes misspelled `yarn` commands;\n* `yarn_command_replaced` \u0026ndash; fixes replaced `yarn` commands;\n* `yarn_help` \u0026ndash; makes it easier to open `yarn` documentation;\n\nThe following rules are enabled by default on specific platforms only:\n\n* `apt_get` \u0026ndash; installs app from apt if it not installed (requires `python-commandnotfound` / `python3-commandnotfound`);\n* `apt_get_search` \u0026ndash; changes trying to search using `apt-get` with searching using `apt-cache`;\n* `apt_invalid_operation` \u0026ndash; fixes invalid `apt` and `apt-get` calls, like `apt-get isntall vim`;\n* `apt_list_upgradable` \u0026ndash; helps you run `apt list --upgradable` after `apt update`;\n* `apt_upgrade` \u0026ndash; helps you run `apt upgrade` after `apt list --upgradable`;\n* `brew_cask_dependency` \u0026ndash; installs cask dependencies;\n* `brew_install` \u0026ndash; fixes formula name for `brew install`;\n* `brew_reinstall` \u0026ndash; turns `brew install \u003cformula\u003e` into `brew reinstall \u003cformula\u003e`;\n* `brew_link` \u0026ndash; adds `--overwrite --dry-run` if linking fails;\n* `brew_uninstall` \u0026ndash; adds `--force` to `brew uninstall` if multiple versions were installed;\n* `brew_unknown_command` \u0026ndash; fixes wrong brew commands, for example `brew docto/brew doctor`;\n* `brew_update_formula` \u0026ndash; turns `brew update \u003cformula\u003e` into `brew upgrade \u003cformula\u003e`;\n* `dnf_no_such_command` \u0026ndash; fixes mistyped DNF commands;\n* `pacman` \u0026ndash; installs app with `pacman` if it is not installed (uses `yaourt` if available);\n* `pacman_not_found` \u0026ndash; fixes package name with `pacman` or `yaourt`.\n\nThe following commands are bundled with *The Fuck*, but are not enabled by\ndefault:\n\n* `git_push_force` \u0026ndash; adds `--force-with-lease` to a `git push` (may conflict with `git_push_pull`);\n* `rm_root` \u0026ndash; adds `--no-preserve-root` to `rm -rf /` command.\n\n## Creating your own rules\n\nTo add your own rule, create a file named `your-rule-name.py`\nin `~/.config/thefuck/rules`. The rule file must contain two functions:\n\n```python\nmatch(command: Command) -\u003e bool\nget_new_command(command: Command) -\u003e str | list[str]\n```\n\nAdditionally, rules can contain optional functions:\n\n```python\nside_effect(old_command: Command, fixed_command: str) -\u003e None\n```\nRules can also contain the optional variables `enabled_by_default`, `requires_output` and `priority`.\n\n`Command` has three attributes: `script`, `output` and `script_parts`.\nYour rule should not change `Command`.\n\n\n**Rules api changed in 3.0:** To access a rule's settings, import it with\n `from thefuck.conf import settings`\n  \n`settings` is a special object assembled from `~/.config/thefuck/settings.py`, \nand values from env ([see more below](#settings)).\n\nA simple example rule for running a script with `sudo`:\n\n```python\ndef match(command):\n    return ('permission denied' in command.output.lower()\n            or 'EACCES' in command.output)\n\n\ndef get_new_command(command):\n    return 'sudo {}'.format(command.script)\n\n# Optional:\nenabled_by_default = True\n\ndef side_effect(command, fixed_command):\n    subprocess.call('chmod 777 .', shell=True)\n\npriority = 1000  # Lower first, default is 1000\n\nrequires_output = True\n```\n\n[More examples of rules](https://github.com/nvbn/thefuck/tree/master/thefuck/rules),\n[utility functions for rules](https://github.com/nvbn/thefuck/tree/master/thefuck/utils.py),\n[app/os-specific helpers](https://github.com/nvbn/thefuck/tree/master/thefuck/specific/).\n\n## Settings\n\nSeveral *The Fuck* parameters can be changed in the file `$XDG_CONFIG_HOME/thefuck/settings.py`\n(`$XDG_CONFIG_HOME` defaults to `~/.config`):\n\n* `rules` \u0026ndash; list of enabled rules, by default `thefuck.conf.DEFAULT_RULES`;\n* `exclude_rules` \u0026ndash; list of disabled rules, by default `[]`;\n* `require_confirmation` \u0026ndash; requires confirmation before running new command, by default `True`;\n* `wait_command` \u0026ndash; max amount of time in seconds for getting previous command output;\n* `no_colors` \u0026ndash; disable colored output;\n* `priority` \u0026ndash; dict with rules priorities, rule with lower `priority` will be matched first;\n* `debug` \u0026ndash; enables debug output, by default `False`;\n* `history_limit` \u0026ndash; numeric value of how many history commands will be scanned, like `2000`;\n* `alter_history` \u0026ndash; push fixed command to history, by default `True`;\n* `wait_slow_command` \u0026ndash; max amount of time in seconds for getting previous command output if it in `slow_commands` list;\n* `slow_commands` \u0026ndash; list of slow commands;\n* `num_close_matches` \u0026ndash; maximum number of close matches to suggest, by default `3`.\n\nAn example of `settings.py`:\n\n```python\nrules = ['sudo', 'no_command']\nexclude_rules = ['git_push']\nrequire_confirmation = True\nwait_command = 10\nno_colors = False\npriority = {'sudo': 100, 'no_command': 9999}\ndebug = False\nhistory_limit = 9999\nwait_slow_command = 20\nslow_commands = ['react-native', 'gradle']\nnum_close_matches = 5\n```\n\nOr via environment variables:\n\n* `THEFUCK_RULES` \u0026ndash; list of enabled rules, like `DEFAULT_RULES:rm_root` or `sudo:no_command`;\n* `THEFUCK_EXCLUDE_RULES` \u0026ndash; list of disabled rules, like `git_pull:git_push`;\n* `THEFUCK_REQUIRE_CONFIRMATION` \u0026ndash; require confirmation before running new command, `true/false`;\n* `THEFUCK_WAIT_COMMAND` \u0026ndash; max amount of time in seconds for getting previous command output;\n* `THEFUCK_NO_COLORS` \u0026ndash; disable colored output, `true/false`;\n* `THEFUCK_PRIORITY` \u0026ndash; priority of the rules, like `no_command=9999:apt_get=100`,\nrule with lower `priority` will be matched first;\n* `THEFUCK_DEBUG` \u0026ndash; enables debug output, `true/false`;\n* `THEFUCK_HISTORY_LIMIT` \u0026ndash; how many history commands will be scanned, like `2000`;\n* `THEFUCK_ALTER_HISTORY` \u0026ndash; push fixed command to history `true/false`;\n* `THEFUCK_WAIT_SLOW_COMMAND` \u0026ndash; max amount of time in seconds for getting previous command output if it in `slow_commands` list;\n* `THEFUCK_SLOW_COMMANDS` \u0026ndash; list of slow commands, like `lein:gradle`;\n* `THEFUCK_NUM_CLOSE_MATCHES` \u0026ndash; maximum number of close matches to suggest, like `5`.\n\nFor example:\n\n```bash\nexport THEFUCK_RULES='sudo:no_command'\nexport THEFUCK_EXCLUDE_RULES='git_pull:git_push'\nexport THEFUCK_REQUIRE_CONFIRMATION='true'\nexport THEFUCK_WAIT_COMMAND=10\nexport THEFUCK_NO_COLORS='false'\nexport THEFUCK_PRIORITY='no_command=9999:apt_get=100'\nexport THEFUCK_HISTORY_LIMIT='2000'\nexport THEFUCK_NUM_CLOSE_MATCHES='5'\n```\n\n## Third-party packages with rules\n\nIf you'd like to make a specific set of non-public rules, but would still like\nto share them with others, create a package named `thefuck_contrib_*` with\nthe following structure:\n\n```\nthefuck_contrib_foo\n  thefuck_contrib_foo\n    rules\n      __init__.py\n      *third-party rules*\n    __init__.py\n    *third-party-utils*\n  setup.py\n```\n\n*The Fuck* will find rules located in the `rules` module.\n\n## Experimental instant mode\n\nThe default behavior of *The Fuck* requires time to re-run previous commands.\nWhen in instant mode, *The Fuck* saves time by logging output with [script](https://en.wikipedia.org/wiki/Script_(Unix)),\nthen reading the log.\n\n[![gif with instant mode][instant-mode-gif-link]][instant-mode-gif-link]\n\nCurrently, instant mode only supports Python 3 with bash or zsh. zsh's autocorrect function also needs to be disabled in order for thefuck to work properly.\n\nTo enable instant mode, add `--enable-experimental-instant-mode`\nto the alias initialization in `.bashrc`, `.bash_profile` or `.zshrc`.\n\nFor example:\n\n```bash\neval $(thefuck --alias --enable-experimental-instant-mode)\n```\n\n## Developing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md)\n\n## License MIT\nProject License can be found [here](LICENSE.md).\n\n\n[version-badge]:   https://img.shields.io/pypi/v/thefuck.svg?label=version\n[version-link]:    https://pypi.python.org/pypi/thefuck/\n[travis-badge]:    https://travis-ci.org/nvbn/thefuck.svg?branch=master\n[travis-link]:     https://travis-ci.org/nvbn/thefuck\n[appveyor-badge]:  https://ci.appveyor.com/api/projects/status/1sskj4imj02um0gu/branch/master?svg=true\n[appveyor-link]:   https://ci.appveyor.com/project/nvbn/thefuck\n[coverage-badge]:  https://img.shields.io/coveralls/nvbn/thefuck.svg\n[coverage-link]:   https://coveralls.io/github/nvbn/thefuck\n[license-badge]:   https://img.shields.io/badge/license-MIT-007EC7.svg\n[examples-link]:   https://raw.githubusercontent.com/nvbn/thefuck/master/example.gif\n[instant-mode-gif-link]:   https://raw.githubusercontent.com/nvbn/thefuck/master/example_instant_mode.gif\n[homebrew]:        http://brew.sh/\n"
  },
  {
    "repo": "jakubroztocil/httpie",
    "content": "HTTPie: a CLI, cURL-like tool for humans\n########################################\n\nHTTPie (pronounced *aitch-tee-tee-pie*) is a command line HTTP client.\nIts goal is to make CLI interaction with web services as human-friendly\nas possible. It provides a simple ``http`` command that allows for sending\narbitrary HTTP requests using a simple and natural syntax, and displays\ncolorized output. HTTPie can be used for testing, debugging, and\ngenerally interacting with HTTP servers.\n\n\n.. class:: no-web\n\n    .. image:: https://raw.githubusercontent.com/jakubroztocil/httpie/master/httpie.png\n        :alt: HTTPie compared to cURL\n        :width: 100%\n        :align: center\n\n\n.. class:: no-web no-pdf\n\n|pypi| |unix_build| |coverage| |gitter|\n\n\n\n.. contents::\n\n.. section-numbering::\n\n\n\nMain features\n=============\n\n* Expressive and intuitive syntax\n* Formatted and colorized terminal output\n* Built-in JSON support\n* Forms and file uploads\n* HTTPS, proxies, and authentication\n* Arbitrary request data\n* Custom headers\n* Persistent sessions\n* Wget-like downloads\n* Python 2.7 and 3.x support\n* Linux, macOS and Windows support\n* Plugins\n* Documentation\n* Test coverage\n\n\nInstallation\n============\n\n\nmacOS\n-----\n\n\nOn macOS, HTTPie can be installed via `Homebrew \u003chttp://brew.sh/\u003e`_\n(recommended):\n\n.. code-block:: bash\n\n    $ brew install httpie\n\n\nA MacPorts *port* is also available:\n\n.. code-block:: bash\n\n    $ port install httpie\n\nLinux\n-----\n\nMost Linux distributions provide a package that can be installed using the\nsystem package manager, for example:\n\n.. code-block:: bash\n\n    # Debian, Ubuntu, etc.\n    $ apt-get install httpie\n\n.. code-block:: bash\n\n    # Fedora\n    $ dnf install httpie\n\n.. code-block:: bash\n\n    # CentOS, RHEL, ...\n    $ yum install httpie\n\n.. code-block:: bash\n\n    # Arch Linux\n    $ pacman -S httpie\n\n\nWindows, etc.\n-------------\n\nA universal installation method (that works on Windows, Mac OS X, Linux, …,\nand always provides the latest version) is to use `pip`_:\n\n\n.. code-block:: bash\n\n    # Make sure we have an up-to-date version of pip and setuptools:\n    $ pip install --upgrade pip setuptools\n\n    $ pip install --upgrade httpie\n\n\n(If ``pip`` installation fails for some reason, you can try\n``easy_install httpie`` as a fallback.)\n\n\nPython version\n--------------\n\nAlthough Python 2.7 is supported as well, it is strongly recommended to\ninstall HTTPie against the latest Python 3.x whenever possible. That will\nensure that some of the newer HTTP features, such as\n`SNI (Server Name Indication)`_, work out of the box.\nPython 3 is the default for Homebrew installations starting with version 0.9.4.\nTo see which version HTTPie uses, run ``http --debug``.\n\n\nUnstable version\n----------------\n\nYou can also install the latest unreleased development version directly from\nthe ``master`` branch on GitHub.  It is a work-in-progress of a future stable\nrelease so the experience might be not as smooth.\n\n\n.. class:: no-pdf\n\n|unix_build|\n\n\nOn macOS you can install it with Homebrew:\n\n.. code-block:: bash\n\n    $ brew install httpie --HEAD\n\n\nOtherwise with ``pip``:\n\n.. code-block:: bash\n\n    $ pip install --upgrade https://github.com/jakubroztocil/httpie/archive/master.tar.gz\n\n\nVerify that now we have the\n`current development version identifier \u003chttps://github.com/jakubroztocil/httpie/blob/0af6ae1be444588bbc4747124e073423151178a0/httpie/__init__.py#L5\u003e`_\nwith the ``-dev`` suffix, for example:\n\n.. code-block:: bash\n\n    $ http --version\n    1.0.0-dev\n\n\nUsage\n=====\n\n\nHello World:\n\n\n.. code-block:: bash\n\n    $ http httpie.org\n\n\nSynopsis:\n\n.. code-block:: bash\n\n    $ http [flags] [METHOD] URL [ITEM [ITEM]]\n\n\nSee also ``http --help``.\n\n\nExamples\n--------\n\nCustom `HTTP method`_, `HTTP headers`_ and `JSON`_ data:\n\n.. code-block:: bash\n\n    $ http PUT example.org X-API-Token:123 name=John\n\n\nSubmitting `forms`_:\n\n.. code-block:: bash\n\n    $ http -f POST example.org hello=World\n\n\nSee the request that is being sent using one of the `output options`_:\n\n.. code-block:: bash\n\n    $ http -v example.org\n\n\nUse `Github API`_ to post a comment on an\n`issue \u003chttps://github.com/jakubroztocil/httpie/issues/83\u003e`_\nwith `authentication`_:\n\n.. code-block:: bash\n\n    $ http -a USERNAME POST https://api.github.com/repos/jakubroztocil/httpie/issues/83/comments body='HTTPie is awesome! :heart:'\n\n\nUpload a file using `redirected input`_:\n\n.. code-block:: bash\n\n    $ http example.org \u003c file.json\n\n\nDownload a file and save it via `redirected output`_:\n\n.. code-block:: bash\n\n    $ http example.org/file \u003e file\n\n\nDownload a file ``wget`` style:\n\n.. code-block:: bash\n\n    $ http --download example.org/file\n\nUse named `sessions`_ to make certain aspects or the communication persistent\nbetween requests to the same host:\n\n.. code-block:: bash\n\n    $ http --session=logged-in -a username:password httpbin.org/get API-Key:123\n\n    $ http --session=logged-in httpbin.org/headers\n\n\nSet a custom ``Host`` header to work around missing DNS records:\n\n.. code-block:: bash\n\n    $ http localhost:8000 Host:example.com\n\n..\n\n\nHTTP method\n===========\n\nThe name of the HTTP method comes right before the URL argument:\n\n.. code-block:: bash\n\n    $ http DELETE example.org/todos/7\n\n\nWhich looks similar to the actual ``Request-Line`` that is sent:\n\n.. code-block:: http\n\n    DELETE /todos/7 HTTP/1.1\n\n\nWhen the ``METHOD`` argument is omitted from the command, HTTPie defaults to\neither ``GET`` (with no request data) or ``POST`` (with request data).\n\n\nRequest URL\n===========\n\nThe only information HTTPie needs to perform a request is a URL.\nThe default scheme is, somewhat unsurprisingly, ``http://``,\nand can be omitted from the argument – ``http example.org`` works just fine.\n\n\nQuerystring parameters\n----------------------\n\nIf you find yourself manually constructing URLs with querystring parameters\non the terminal, you may appreciate the ``param==value`` syntax for appending\nURL parameters. With that, you don't have to worry about escaping the ``\u0026``\nseparators for your shell. Also, special characters in parameter values,\nwill also automatically escaped (HTTPie otherwise expects the URL to be\nalready escaped). To search for ``HTTPie logo`` on Google Images you could use\nthis command:\n\n.. code-block:: bash\n\n    $ http www.google.com search=='HTTPie logo' tbm==isch\n\n\n.. code-block:: http\n\n    GET /?search=HTTPie+logo\u0026tbm=isch HTTP/1.1\n\n\n\nURL shortcuts for ``localhost``\n-------------------------------\n\nAdditionally, curl-like shorthand for localhost is supported.\nThis means that, for example ``:3000`` would expand to ``http://localhost:3000``\nIf the port is omitted, then port 80 is assumed.\n\n.. code-block:: bash\n\n    $ http :/foo\n\n\n.. code-block:: http\n\n    GET /foo HTTP/1.1\n    Host: localhost\n\n\n.. code-block:: bash\n\n    $ http :3000/bar\n\n\n.. code-block:: http\n\n    GET /bar HTTP/1.1\n    Host: localhost:3000\n\n\n.. code-block:: bash\n\n    $ http :\n\n\n.. code-block:: http\n\n    GET / HTTP/1.1\n    Host: localhost\n\n\nCustom default scheme\n---------------------\n\nYou can use the ``--default-scheme \u003cURL_SCHEME\u003e`` option to create\nshortcuts for other protocols than HTTP:\n\n.. code-block:: bash\n\n    $ alias https='http --default-scheme=https'\n\n\nRequest items\n=============\n\nThere are a few different *request item* types that provide a\nconvenient mechanism for specifying HTTP headers, simple JSON and\nform data, files, and URL parameters.\n\nThey are key/value pairs specified after the URL. All have in\ncommon that they become part of the actual request that is sent and that\ntheir type is distinguished only by the separator used:\n``:``, ``=``, ``:=``, ``==``, ``@``, ``=@``, and ``:=@``. The ones with an\n``@`` expect a file path as value.\n\n+-----------------------+-----------------------------------------------------+\n| Item Type             | Description                                         |\n+=======================+=====================================================+\n| HTTP Headers          | Arbitrary HTTP header, e.g. ``X-API-Token:123``.    |\n| ``Name:Value``        |                                                     |\n+-----------------------+-----------------------------------------------------+\n| URL parameters        | Appends the given name/value pair as a query        |\n| ``name==value``       | string parameter to the URL.                        |\n|                       | The ``==`` separator is used.                       |\n+-----------------------+-----------------------------------------------------+\n| Data Fields           | Request data fields to be serialized as a JSON      |\n| ``field=value``,      | object (default), or to be form-encoded             |\n| ``field=@file.txt``   | (``--form, -f``).                                   |\n+-----------------------+-----------------------------------------------------+\n| Raw JSON fields       | Useful when sending JSON and one or                 |\n| ``field:=json``,      | more fields need to be a ``Boolean``, ``Number``,   |\n| ``field:=@file.json`` | nested ``Object``, or an ``Array``,  e.g.,          |\n|                       | ``meals:='[\"ham\",\"spam\"]'`` or ``pies:=[1,2,3]``    |\n|                       | (note the quotes).                                  |\n+-----------------------+-----------------------------------------------------+\n| Form File Fields      | Only available with ``--form, -f``.                 |\n| ``field@/dir/file``   | For example ``screenshot@~/Pictures/img.png``.      |\n|                       | The presence of a file field results                |\n|                       | in a ``multipart/form-data`` request.               |\n+-----------------------+-----------------------------------------------------+\n\n\nNote that data fields aren't the only way to specify request data:\n`Redirected input`_ is a mechanism for passing arbitrary request data.\n\n\nEscaping rules\n--------------\n\nYou can use ``\\`` to escape characters that shouldn't be used as separators\n(or parts thereof). For instance, ``foo\\==bar`` will become a data key/value\npair (``foo=`` and ``bar``) instead of a URL parameter.\n\nOften it is necessary to quote the values, e.g. ``foo='bar baz'``.\n\nIf any of the field names or headers starts with a minus\n(e.g., ``-fieldname``), you need to place all such items after the special\ntoken ``--`` to prevent confusion with ``--arguments``:\n\n.. code-block:: bash\n\n    $ http httpbin.org/post  --  -name-starting-with-dash=foo -Unusual-Header:bar\n\n.. code-block:: http\n\n    POST /post HTTP/1.1\n    -Unusual-Header: bar\n    Content-Type: application/json\n\n    {\n        \"-name-starting-with-dash\": \"foo\"\n    }\n\n\n\nJSON\n====\n\nJSON is the *lingua franca* of modern web services and it is also the\n**implicit content type** HTTPie uses by default.\n\n\nSimple example:\n\n.. code-block:: bash\n\n    $ http PUT example.org name=John email=john@example.org\n\n.. code-block:: http\n\n    PUT / HTTP/1.1\n    Accept: application/json, */*\n    Accept-Encoding: gzip, deflate\n    Content-Type: application/json\n    Host: example.org\n\n    {\n        \"name\": \"John\",\n        \"email\": \"john@example.org\"\n    }\n\n\nDefault behaviour\n-----------------\n\n\nIf your command includes some data `request items`_, they are serialized as a JSON\nobject by default. HTTPie also automatically sets the following headers,\nboth of which can be overwritten:\n\n================    =======================================\n``Content-Type``    ``application/json``\n``Accept``          ``application/json, */*``\n================    =======================================\n\n\nExplicit JSON\n-------------\n\nYou can use ``--json, -j`` to explicitly set ``Accept``\nto ``application/json`` regardless of whether you are sending data\n(it's a shortcut for setting the header via the usual header notation:\n``http url Accept:'application/json, */*'``). Additionally,\nHTTPie will try to detect JSON responses even when the\n``Content-Type`` is incorrectly ``text/plain`` or unknown.\n\n\n\nNon-string JSON fields\n----------------------\n\nNon-string fields use the ``:=`` separator, which allows you to embed raw JSON\ninto the resulting object. Text and raw JSON files can also be embedded into\nfields using ``=@`` and ``:=@``:\n\n.. code-block:: bash\n\n    $ http PUT api.example.com/person/1 \\\n        name=John \\\n        age:=29 married:=false hobbies:='[\"http\", \"pies\"]' \\  # Raw JSON\n        description=@about-john.txt \\   # Embed text file\n        bookmarks:=@bookmarks.json      # Embed JSON file\n\n\n.. code-block:: http\n\n    PUT /person/1 HTTP/1.1\n    Accept: application/json, */*\n    Content-Type: application/json\n    Host: api.example.com\n\n    {\n        \"age\": 29,\n        \"hobbies\": [\n            \"http\",\n            \"pies\"\n        ],\n        \"description\": \"John is a nice guy who likes pies.\",\n        \"married\": false,\n        \"name\": \"John\",\n        \"bookmarks\": {\n            \"HTTPie\": \"http://httpie.org\",\n        }\n    }\n\n\nPlease note that with this syntax the command gets unwieldy when sending\ncomplex data. In that case it's always better to use `redirected input`_:\n\n.. code-block:: bash\n\n    $ http POST api.example.com/person/1 \u003c person.json\n\n\nForms\n=====\n\nSubmitting forms is very similar to sending `JSON`_ requests. Often the only\ndifference is in adding the ``--form, -f`` option, which ensures that\ndata fields are serialized as, and ``Content-Type`` is set to,\n``application/x-www-form-urlencoded; charset=utf-8``. It is possible to make\nform data the implicit content type instead of JSON\nvia the `config`_ file.\n\n\nRegular forms\n-------------\n\n.. code-block:: bash\n\n    $ http --form POST api.example.org/person/1 name='John Smith'\n\n\n.. code-block:: http\n\n    POST /person/1 HTTP/1.1\n    Content-Type: application/x-www-form-urlencoded; charset=utf-8\n\n    name=John+Smith\n\n\nFile upload forms\n-----------------\n\nIf one or more file fields is present, the serialization and content type is\n``multipart/form-data``:\n\n.. code-block:: bash\n\n    $ http -f POST example.com/jobs name='John Smith' cv@~/Documents/cv.pdf\n\n\nThe request above is the same as if the following HTML form were\nsubmitted:\n\n.. code-block:: html\n\n    \u003cform enctype=\"multipart/form-data\" method=\"post\" action=\"http://example.com/jobs\"\u003e\n        \u003cinput type=\"text\" name=\"name\" /\u003e\n        \u003cinput type=\"file\" name=\"cv\" /\u003e\n    \u003c/form\u003e\n\nNote that ``@`` is used to simulate a file upload form field, whereas\n``=@`` just embeds the file content as a regular text field value.\n\n\nHTTP headers\n============\n\nTo set custom headers you can use the ``Header:Value`` notation:\n\n.. code-block:: bash\n\n    $ http example.org  User-Agent:Bacon/1.0  'Cookie:valued-visitor=yes;foo=bar'  \\\n        X-Foo:Bar  Referer:http://httpie.org/\n\n\n.. code-block:: http\n\n    GET / HTTP/1.1\n    Accept: */*\n    Accept-Encoding: gzip, deflate\n    Cookie: valued-visitor=yes;foo=bar\n    Host: example.org\n    Referer: http://httpie.org/\n    User-Agent: Bacon/1.0\n    X-Foo: Bar\n\n\nDefault request headers\n-----------------------\n\nThere are a couple of default headers that HTTPie sets:\n\n.. code-block:: http\n\n    GET / HTTP/1.1\n    Accept: */*\n    Accept-Encoding: gzip, deflate\n    User-Agent: HTTPie/\u003cversion\u003e\n    Host: \u003ctaken-from-URL\u003e\n\n\n\nAny of these except ``Host`` can be overwritten and some of them unset.\n\n\n\nEmpty headers and header un-setting\n-----------------------------------\n\nTo unset a previously specified header\n(such a one of the default headers), use ``Header:``:\n\n\n.. code-block:: bash\n\n    $ http httpbin.org/headers Accept: User-Agent:\n\n\nTo send a header with an empty value, use ``Header;``:\n\n\n.. code-block:: bash\n\n    $ http httpbin.org/headers 'Header;'\n\n\nCookies\n=======\n\nHTTP clients send cookies to the server as regular `HTTP headers`_. That means,\nHTTPie does not offer any special syntax for specifying cookies — the usual\n``Header:Value`` notation is used:\n\n\nSend a single cookie:\n\n.. code-block:: bash\n\n    $ http example.org Cookie:sessionid=foo\n\n.. code-block:: http\n\n    GET / HTTP/1.1\n    Accept: */*\n    Accept-Encoding: gzip, deflate\n    Connection: keep-alive\n    Cookie: sessionid=foo\n    Host: example.org\n    User-Agent: HTTPie/0.9.9\n\n\nSend multiple cookies\n(note the header is quoted to prevent the shell from interpreting the ``;``):\n\n.. code-block:: bash\n\n    $ http example.org 'Cookie:sessionid=foo;another-cookie=bar'\n\n.. code-block:: http\n\n    GET / HTTP/1.1\n    Accept: */*\n    Accept-Encoding: gzip, deflate\n    Connection: keep-alive\n    Cookie: sessionid=foo;another-cookie=bar\n    Host: example.org\n    User-Agent: HTTPie/0.9.9\n\n\nIf you often deal with cookies in your requests, then chances are you'd appreciate\nthe `sessions`_ feature.\n\n\nAuthentication\n==============\n\nThe currently supported authentication schemes are Basic and Digest\n(see `auth plugins`_ for more). There are two flags that control authentication:\n\n===================     ======================================================\n``--auth, -a``          Pass a ``username:password`` pair as\n                        the argument. Or, if you only specify a username\n                        (``-a username``), you'll be prompted for\n                        the password before the request is sent.\n                        To send an empty password, pass ``username:``.\n                        The ``username:password@hostname`` URL syntax is\n                        supported as well (but credentials passed via ``-a``\n                        have higher priority).\n\n``--auth-type, -A``     Specify the auth mechanism. Possible values are\n                        ``basic`` and ``digest``. The default value is\n                        ``basic`` so it can often be omitted.\n===================     ======================================================\n\n\n\nBasic auth\n----------\n\n\n.. code-block:: bash\n\n    $ http -a username:password example.org\n\n\nDigest auth\n-----------\n\n\n.. code-block:: bash\n\n    $ http -A digest -a username:password example.org\n\n\nPassword prompt\n---------------\n\n.. code-block:: bash\n\n    $ http -a username example.org\n\n\n``.netrc``\n----------\n\nAuthentication information from your ``~/.netrc`` file is honored as well:\n\n.. code-block:: bash\n\n    $ cat ~/.netrc\n    machine httpbin.org\n    login httpie\n    password test\n\n    $ http httpbin.org/basic-auth/httpie/test\n    HTTP/1.1 200 OK\n    [...]\n\n\nAuth plugins\n------------\n\nAdditional authentication mechanism can be installed as plugins.\nThey can be found on the `Python Package Index \u003chttps://pypi.python.org/pypi?%3Aaction=search\u0026term=httpie\u0026submit=search\u003e`_.\nHere's a few picks:\n\n* `httpie-api-auth \u003chttps://github.com/pd/httpie-api-auth\u003e`_: ApiAuth\n* `httpie-aws-auth \u003chttps://github.com/httpie/httpie-aws-auth\u003e`_: AWS / Amazon S3\n* `httpie-edgegrid \u003chttps://github.com/akamai-open/httpie-edgegrid\u003e`_: EdgeGrid\n* `httpie-hmac-auth \u003chttps://github.com/guardian/httpie-hmac-auth\u003e`_: HMAC\n* `httpie-jwt-auth \u003chttps://github.com/teracyhq/httpie-jwt-auth\u003e`_: JWTAuth (JSON Web Tokens)\n* `httpie-negotiate \u003chttps://github.com/ndzou/httpie-negotiate\u003e`_: SPNEGO (GSS Negotiate)\n* `httpie-ntlm \u003chttps://github.com/httpie/httpie-ntlm\u003e`_: NTLM (NT LAN Manager)\n* `httpie-oauth \u003chttps://github.com/httpie/httpie-oauth\u003e`_: OAuth\n* `requests-hawk \u003chttps://github.com/mozilla-services/requests-hawk\u003e`_: Hawk\n\n\n\n\nHTTP redirects\n==============\n\nBy default, HTTP redirects are not followed and only the first\nresponse is shown:\n\n\n.. code-block:: bash\n\n    $ http httpbin.org/redirect/3\n\n\nFollow ``Location``\n-------------------\n\nTo instruct HTTPie to follow the ``Location`` header of ``30x`` responses\nand show the final response instead, use the ``--follow, -F`` option:\n\n\n.. code-block:: bash\n\n    $ http --follow httpbin.org/redirect/3\n\n\nShowing intermediary redirect responses\n---------------------------------------\n\nIf you additionally wish to see the intermediary requests/responses,\nthen use the ``--all`` option as well:\n\n\n.. code-block:: bash\n\n    $ http --follow --all httpbin.org/redirect/3\n\n\n\nLimiting maximum redirects followed\n-----------------------------------\n\nTo change the default limit of maximum ``30`` redirects, use the\n``--max-redirects=\u003climit\u003e`` option:\n\n\n.. code-block:: bash\n\n    $ http --follow --all --max-redirects=5 httpbin.org/redirect/3\n\n\nProxies\n=======\n\nYou can specify proxies to be used through the ``--proxy`` argument for each\nprotocol (which is included in the value in case of redirects across protocols):\n\n.. code-block:: bash\n\n    $ http --proxy=http:http://10.10.1.10:3128 --proxy=https:https://10.10.1.10:1080 example.org\n\n\nWith Basic authentication:\n\n.. code-block:: bash\n\n    $ http --proxy=http:http://user:pass@10.10.1.10:3128 example.org\n\n\nEnvironment variables\n---------------------\n\nYou can also configure proxies by environment variables ``HTTP_PROXY`` and\n``HTTPS_PROXY``, and the underlying Requests library will pick them up as well.\nIf you want to disable proxies configured through the environment variables for\ncertain hosts, you can specify them in ``NO_PROXY``.\n\nIn your ``~/.bash_profile``:\n\n.. code-block:: bash\n\n export HTTP_PROXY=http://10.10.1.10:3128\n export HTTPS_PROXY=https://10.10.1.10:1080\n export NO_PROXY=localhost,example.com\n\n\nSOCKS\n-----\n\nHomebrew-installed HTTPie comes with SOCKS proxy support out of the box. To enable SOCKS proxy support for non-Homebrew  installations, you'll need to install ``requests[socks]`` manually using ``pip``:\n\n\n.. code-block:: bash\n\n    $ pip install -U requests[socks]\n\nUsage is the same as for other types of `proxies`_:\n\n.. code-block:: bash\n\n    $ http --proxy=http:socks5://user:pass@host:port --proxy=https:socks5://user:pass@host:port example.org\n\n\nHTTPS\n=====\n\n\nServer SSL certificate verification\n-----------------------------------\n\nTo skip the host's SSL certificate verification, you can pass ``--verify=no``\n(default is ``yes``):\n\n.. code-block:: bash\n\n    $ http --verify=no https://example.org\n\n\nCustom CA bundle\n----------------\n\nYou can also use ``--verify=\u003cCA_BUNDLE_PATH\u003e`` to set a custom CA bundle path:\n\n.. code-block:: bash\n\n    $ http --verify=/ssl/custom_ca_bundle https://example.org\n\n\n\nClient side SSL certificate\n---------------------------\nTo use a client side certificate for the SSL communication, you can pass\nthe path of the cert file with ``--cert``:\n\n.. code-block:: bash\n\n    $ http --cert=client.pem https://example.org\n\n\nIf the private key is not contained in the cert file you may pass the\npath of the key file with ``--cert-key``:\n\n.. code-block:: bash\n\n    $ http --cert=client.crt --cert-key=client.key https://example.org\n\n\nSSL version\n-----------\n\nUse the ``--ssl=\u003cPROTOCOL\u003e`` to specify the desired protocol version to use.\nThis will default to SSL v2.3 which will negotiate the highest protocol that both\nthe server and your installation of OpenSSL support. The available protocols\nare ``ssl2.3``, ``ssl3``, ``tls1``, ``tls1.1``, ``tls1.2``, ``tls1.3``. (The actually\navailable set of protocols may vary depending on your OpenSSL installation.)\n\n.. code-block:: bash\n\n    # Specify the vulnerable SSL v3 protocol to talk to an outdated server:\n    $ http --ssl=ssl3 https://vulnerable.example.org\n\n\nSNI (Server Name Indication)\n----------------------------\n\nIf you use HTTPie with `Python version`_ lower than 2.7.9\n(can be verified with ``http --debug``) and need to talk to servers that\nuse SNI (Server Name Indication) you need to install some additional\ndependencies:\n\n.. code-block:: bash\n\n    $ pip install --upgrade requests[security]\n\n\nYou can use the following command to test SNI support:\n\n.. code-block:: bash\n\n    $ http https://sni.velox.ch\n\n\nOutput options\n==============\n\nBy default, HTTPie only outputs the final response and the whole response\nmessage is printed (headers as well as the body). You can control what should\nbe printed via several options:\n\n=================   =====================================================\n``--headers, -h``   Only the response headers are printed.\n``--body, -b``      Only the response body is printed.\n``--verbose, -v``   Print the whole HTTP exchange (request and response).\n                    This option also enables ``--all`` (see below).\n``--print, -p``     Selects parts of the HTTP exchange.\n=================   =====================================================\n\n``--verbose`` can often be useful for debugging the request and generating\ndocumentation examples:\n\n.. code-block:: bash\n\n    $ http --verbose PUT httpbin.org/put hello=world\n    PUT /put HTTP/1.1\n    Accept: application/json, */*\n    Accept-Encoding: gzip, deflate\n    Content-Type: application/json\n    Host: httpbin.org\n    User-Agent: HTTPie/0.2.7dev\n\n    {\n        \"hello\": \"world\"\n    }\n\n\n    HTTP/1.1 200 OK\n    Connection: keep-alive\n    Content-Length: 477\n    Content-Type: application/json\n    Date: Sun, 05 Aug 2012 00:25:23 GMT\n    Server: gunicorn/0.13.4\n\n    {\n        […]\n    }\n\n\nWhat parts of the HTTP exchange should be printed\n-------------------------------------------------\n\nAll the other `output options`_ are under the hood just shortcuts for\nthe more powerful ``--print, -p``. It accepts a string of characters each\nof which represents a specific part of the HTTP exchange:\n\n==========  ==================\nCharacter   Stands for\n==========  ==================\n``H``       request headers\n``B``       request body\n``h``       response headers\n``b``       response body\n==========  ==================\n\nPrint request and response headers:\n\n.. code-block:: bash\n\n    $ http --print=Hh PUT httpbin.org/put hello=world\n\n\nViewing intermediary requests/responses\n---------------------------------------\n\nTo see all the HTTP communication, i.e. the final request/response as\nwell as any possible  intermediary requests/responses, use the ``--all``\noption. The intermediary HTTP communication include followed redirects\n(with ``--follow``), the first unauthorized request when HTTP digest\nauthentication is used (``--auth=digest``), etc.\n\n.. code-block:: bash\n\n    # Include all responses that lead to the final one:\n    $ http --all --follow httpbin.org/redirect/3\n\n\nThe intermediary requests/response are by default formatted according to\n``--print, -p`` (and its shortcuts described above). If you'd like to change\nthat, use the ``--history-print, -P`` option. It takes the same\narguments as ``--print, -p`` but applies to the intermediary requests only.\n\n\n.. code-block:: bash\n\n    # Print the intermediary requests/responses differently than the final one:\n    $ http -A digest -a foo:bar --all -p Hh -P H httpbin.org/digest-auth/auth/foo/bar\n\n\nConditional body download\n-------------------------\n\nAs an optimization, the response body is downloaded from the server\nonly if it's part of the output. This is similar to performing a ``HEAD``\nrequest, except that it applies to any HTTP method you use.\n\nLet's say that there is an API that returns the whole resource when it is\nupdated, but you are only interested in the response headers to see the\nstatus code after an update:\n\n.. code-block:: bash\n\n    $ http --headers PATCH example.org/Really-Huge-Resource name='New Name'\n\n\nSince we are only printing the HTTP headers here, the connection to the server\nis closed as soon as all the response headers have been received.\nTherefore, bandwidth and time isn't wasted downloading the body\nwhich you don't care about. The response headers are downloaded always,\neven if they are not part of the output\n\n\nRedirected Input\n================\n\nThe universal method for passing request data is through redirected ``stdin``\n(standard input)—piping. Such data is buffered and then with no further\nprocessing used as the request body. There are multiple useful ways to use\npiping:\n\nRedirect from a file:\n\n.. code-block:: bash\n\n    $ http PUT example.com/person/1 X-API-Token:123 \u003c person.json\n\n\nOr the output of another program:\n\n.. code-block:: bash\n\n    $ grep '401 Unauthorized' /var/log/httpd/error_log | http POST example.org/intruders\n\n\nYou can use ``echo`` for simple data:\n\n.. code-block:: bash\n\n    $ echo '{\"name\": \"John\"}' | http PATCH example.com/person/1 X-API-Token:123\n\n\nYou can even pipe web services together using HTTPie:\n\n.. code-block:: bash\n\n    $ http GET https://api.github.com/repos/jakubroztocil/httpie | http POST httpbin.org/post\n\n\nYou can use ``cat`` to enter multiline data on the terminal:\n\n.. code-block:: bash\n\n    $ cat | http POST example.com\n    \u003cpaste\u003e\n    ^D\n\n\n.. code-block:: bash\n\n    $ cat | http POST example.com/todos Content-Type:text/plain\n    - buy milk\n    - call parents\n    ^D\n\n\nOn OS X, you can send the contents of the clipboard with ``pbpaste``:\n\n.. code-block:: bash\n\n    $ pbpaste | http PUT example.com\n\n\nPassing data through ``stdin`` cannot be combined with data fields specified\non the command line:\n\n\n.. code-block:: bash\n\n    $ echo 'data' | http POST example.org more=data   # This is invalid\n\n\nTo prevent HTTPie from reading ``stdin`` data you can use the\n``--ignore-stdin`` option.\n\n\nRequest data from a filename\n----------------------------\n\nAn alternative to redirected ``stdin`` is specifying a filename (as\n``@/path/to/file``) whose content is used as if it came from ``stdin``.\n\nIt has the advantage that the ``Content-Type``\nheader is automatically set to the appropriate value based on the\nfilename extension. For example, the following request sends the\nverbatim contents of that XML file with ``Content-Type: application/xml``:\n\n.. code-block:: bash\n\n    $ http PUT httpbin.org/put @/data/file.xml\n\n\nTerminal output\n===============\n\nHTTPie does several things by default in order to make its terminal output\neasy to read.\n\n\nColors and formatting\n---------------------\n\nSyntax highlighting is applied to HTTP headers and bodies (where it makes\nsense). You can choose your preferred color scheme via the ``--style`` option\nif you don't like the default one (see ``$ http --help`` for the possible\nvalues).\n\nAlso, the following formatting is applied:\n\n* HTTP headers are sorted by name.\n* JSON data is indented, sorted by keys, and unicode escapes are converted\n  to the characters they represent.\n\nOne of these options can be used to control output processing:\n\n====================   ========================================================\n``--pretty=all``       Apply both colors and formatting.\n                       Default for terminal output.\n``--pretty=colors``    Apply colors.\n``--pretty=format``    Apply formatting.\n``--pretty=none``      Disables output processing.\n                       Default for redirected output.\n====================   ========================================================\n\nBinary data\n-----------\n\nBinary data is suppressed for terminal output, which makes it safe to perform\nrequests to URLs that send back binary data. Binary data is suppressed also in\nredirected, but prettified output. The connection is closed as soon as we know\nthat the response body is binary,\n\n.. code-block:: bash\n\n    $ http example.org/Movie.mov\n\n\nYou will nearly instantly see something like this:\n\n.. code-block:: http\n\n    HTTP/1.1 200 OK\n    Accept-Ranges: bytes\n    Content-Encoding: gzip\n    Content-Type: video/quicktime\n    Transfer-Encoding: chunked\n\n    +-----------------------------------------+\n    | NOTE: binary data not shown in terminal |\n    +-----------------------------------------+\n\n\nRedirected output\n=================\n\nHTTPie uses a different set of defaults for redirected output than for\n`terminal output`_. The differences being:\n\n* Formatting and colors aren't applied (unless ``--pretty`` is specified).\n* Only the response body is printed (unless one of the `output options`_ is set).\n* Also, binary data isn't suppressed.\n\nThe reason is to make piping HTTPie's output to another programs and\ndownloading files work with no extra flags. Most of the time, only the raw\nresponse body is of an interest when the output is redirected.\n\nDownload a file:\n\n.. code-block:: bash\n\n    $ http example.org/Movie.mov \u003e Movie.mov\n\n\nDownload an image of Octocat, resize it using ImageMagick, upload it elsewhere:\n\n.. code-block:: bash\n\n    $ http octodex.github.com/images/original.jpg | convert - -resize 25% -  | http example.org/Octocats\n\n\nForce colorizing and formatting, and show both the request and the response in\n``less`` pager:\n\n.. code-block:: bash\n\n    $ http --pretty=all --verbose example.org | less -R\n\n\nThe ``-R`` flag tells ``less`` to interpret color escape sequences included\nHTTPie`s output.\n\nYou can create a shortcut for invoking HTTPie with colorized and paged output\nby adding the following to your ``~/.bash_profile``:\n\n.. code-block:: bash\n\n    function httpless {\n        # `httpless example.org'\n        http --pretty=all --print=hb \"$@\" | less -R;\n    }\n\n\nDownload mode\n=============\n\nHTTPie features a download mode in which it acts similarly to ``wget``.\n\nWhen enabled using the ``--download, -d`` flag, response headers are printed to\nthe terminal (``stderr``), and a progress bar is shown while the response body\nis being saved to a file.\n\n.. code-block:: bash\n\n    $ http --download https://github.com/jakubroztocil/httpie/archive/master.tar.gz\n\n.. code-block:: http\n\n    HTTP/1.1 200 OK\n    Content-Disposition: attachment; filename=httpie-master.tar.gz\n    Content-Length: 257336\n    Content-Type: application/x-gzip\n\n    Downloading 251.30 kB to \"httpie-master.tar.gz\"\n    Done. 251.30 kB in 2.73862s (91.76 kB/s)\n\n\nDownloaded file name\n--------------------\n\nIf not provided via ``--output, -o``, the output filename will be determined\nfrom ``Content-Disposition`` (if available), or from the URL and\n``Content-Type``. If the guessed filename already exists, HTTPie adds a unique\nsuffix to it.\n\n\nPiping while downloading\n------------------------\n\nYou can also redirect the response body to another program while the response\nheaders and progress are still shown in the terminal:\n\n.. code-block:: bash\n\n    $ http -d https://github.com/jakubroztocil/httpie/archive/master.tar.gz |  tar zxf -\n\n\n\nResuming downloads\n------------------\n\nIf ``--output, -o`` is specified, you can resume a partial download using the\n``--continue, -c`` option. This only works with servers that support\n``Range`` requests and ``206 Partial Content`` responses. If the server doesn't\nsupport that, the whole file will simply be downloaded:\n\n.. code-block:: bash\n\n    $ http -dco file.zip example.org/file\n\nOther notes\n-----------\n\n* The ``--download`` option only changes how the response body is treated.\n* You can still set custom headers, use sessions, ``--verbose, -v``, etc.\n* ``--download`` always implies ``--follow`` (redirects are followed).\n* HTTPie exits with status code ``1`` (error) if the body hasn't been fully\n  downloaded.\n* ``Accept-Encoding`` cannot be set with ``--download``.\n\n\nStreamed responses\n==================\n\nResponses are downloaded and printed in chunks which allows for streaming\nand large file downloads without using too much memory. However, when\n`colors and formatting`_ is applied, the whole response is buffered and only\nthen processed at once.\n\n\nDisabling buffering\n-------------------\n\nYou can use the ``--stream, -S`` flag to make two things happen:\n\n1. The output is flushed in much smaller chunks without any buffering,\n   which makes HTTPie behave kind of like ``tail -f`` for URLs.\n\n2. Streaming becomes enabled even when the output is prettified: It will be\n   applied to each line of the response and flushed immediately. This makes\n   it possible to have a nice output for long-lived requests, such as one\n   to the Twitter streaming API.\n\n\nExamples use cases\n------------------\n\nPrettified streamed response:\n\n.. code-block:: bash\n\n    $ http --stream -f -a YOUR-TWITTER-NAME https://stream.twitter.com/1/statuses/filter.json track='Justin Bieber'\n\n\nStreamed output by small chunks alá ``tail -f``:\n\n.. code-block:: bash\n\n    # Send each new tweet (JSON object) mentioning \"Apple\" to another\n    # server as soon as it arrives from the Twitter streaming API:\n    $ http --stream -f -a YOUR-TWITTER-NAME https://stream.twitter.com/1/statuses/filter.json track=Apple \\\n    | while read tweet; do echo \"$tweet\" | http POST example.org/tweets ; done\n\nSessions\n========\n\nBy default, every request HTTPie makes is completely independent of any\nprevious ones to the same host.\n\n\nHowever, HTTPie also supports persistent\nsessions via the ``--session=SESSION_NAME_OR_PATH`` option. In a session,\ncustom `HTTP headers`_ (except for the ones starting with ``Content-`` or ``If-``),\n`authentication`_, and `cookies`_\n(manually specified or sent by the server) persist between requests\nto the same host.\n\n\n.. code-block:: bash\n\n    # Create a new session\n    $ http --session=/tmp/session.json example.org API-Token:123\n\n    # Re-use an existing session — API-Token will be set:\n    $ http --session=/tmp/session.json example.org\n\n\nAll session data, including credentials, cookie data,\nand custom headers are stored in plain text.\nThat means session files can also be created and edited manually in a text\neditor—they are regular JSON. It also means that they can be read by anyone\nwho has access to the session file.\n\n\nNamed sessions\n--------------\n\n\nYou can create one or more named session per host. For example, this is how\nyou can create a new session named ``user1`` for ``example.org``:\n\n.. code-block:: bash\n\n    $ http --session=user1 -a user1:password example.org X-Foo:Bar\n\nFrom now on, you can refer to the session by its name. When you choose to\nuse the session again, any previously specified authentication or HTTP headers\nwill automatically be set:\n\n.. code-block:: bash\n\n    $ http --session=user1 example.org\n\nTo create or reuse a different session, simple specify a different name:\n\n.. code-block:: bash\n\n    $ http --session=user2 -a user2:password example.org X-Bar:Foo\n\nNamed sessions' data is stored in JSON files in the directory\n``~/.httpie/sessions/\u003chost\u003e/\u003cname\u003e.json``\n(``%APPDATA%\\httpie\\sessions\\\u003chost\u003e\\\u003cname\u003e.json`` on Windows).\n\n\nAnonymous sessions\n------------------\n\nInstead of a name, you can also directly specify a path to a session file. This\nallows for sessions to be re-used across multiple hosts:\n\n.. code-block:: bash\n\n    $ http --session=/tmp/session.json example.org\n    $ http --session=/tmp/session.json admin.example.org\n    $ http --session=~/.httpie/sessions/another.example.org/test.json example.org\n    $ http --session-read-only=/tmp/session.json example.org\n\n\nReadonly session\n----------------\n\nTo use an existing session file without updating it from the request/response\nexchange once it is created, specify the session name via\n``--session-read-only=SESSION_NAME_OR_PATH`` instead.\n\n\nConfig\n======\n\nHTTPie uses a simple JSON config file.\n\n\n\nConfig file location\n--------------------\n\n\nThe default location of the configuration file is ``~/.httpie/config.json``\n(or ``%APPDATA%\\httpie\\config.json`` on Windows). The config directory\nlocation can be changed by setting the ``HTTPIE_CONFIG_DIR``\nenvironment variable. To view the exact location run ``http --debug``.\n\nConfigurable options\n--------------------\n\nThe JSON file contains an object with the following keys:\n\n\n``default_options``\n~~~~~~~~~~~~~~~~~~~\n\n\nAn ``Array`` (by default empty) of default options that should be applied to\nevery invocation of HTTPie.\n\nFor instance, you can use this option to change the default style and output\noptions: ``\"default_options\": [\"--style=fruity\", \"--body\"]`` Another useful\ndefault option could be ``\"--session=default\"`` to make HTTPie always\nuse `sessions`_ (one named ``default`` will automatically be used).\nOr you could change the implicit request content type from JSON to form by\nadding ``--form`` to the list.\n\n\n``__meta__``\n~~~~~~~~~~~~\n\nHTTPie automatically stores some of its metadata here. Please do not change.\n\n\n\nUn-setting previously specified options\n---------------------------------------\n\nDefault options from the config file, or specified any other way,\ncan be unset for a particular invocation via ``--no-OPTION`` arguments passed\non the command line (e.g., ``--no-style`` or ``--no-session``).\n\n\n\nScripting\n=========\n\nWhen using HTTPie from shell scripts, it can be handy to set the\n``--check-status`` flag. It instructs HTTPie to exit with an error if the\nHTTP status is one of ``3xx``, ``4xx``, or ``5xx``. The exit status will\nbe ``3`` (unless ``--follow`` is set), ``4``, or ``5``,\nrespectively.\n\n.. code-block:: bash\n\n    #!/bin/bash\n\n    if http --check-status --ignore-stdin --timeout=2.5 HEAD example.org/health \u0026\u003e /dev/null; then\n        echo 'OK!'\n    else\n        case $? in\n            2) echo 'Request timed out!' ;;\n            3) echo 'Unexpected HTTP 3xx Redirection!' ;;\n            4) echo 'HTTP 4xx Client Error!' ;;\n            5) echo 'HTTP 5xx Server Error!' ;;\n            6) echo 'Exceeded --max-redirects=\u003cn\u003e redirects!' ;;\n            *) echo 'Other Error!' ;;\n        esac\n    fi\n\n\nBest practices\n--------------\n\nThe default behaviour of automatically reading ``stdin`` is typically not\ndesirable during non-interactive invocations. You most likely want\nuse the ``--ignore-stdin`` option to disable it.\n\nIt is a common gotcha that without this option HTTPie seemingly hangs.\nWhat happens is that when HTTPie is invoked for example from a cron job,\n``stdin`` is not connected to a terminal.\nTherefore, rules for `redirected input`_ apply, i.e., HTTPie starts to read it\nexpecting that the request body will be passed through.\nAnd since there's no data nor ``EOF``, it will be stuck. So unless you're\npiping some data to HTTPie, this flag should be used in scripts.\n\nAlso, it might be good to override the default ``30`` second ``--timeout`` to\nsomething that suits you.\n\n\n\nMeta\n====\n\nInterface design\n----------------\n\nThe syntax of the command arguments closely corresponds to the actual HTTP\nrequests sent over the wire. It has the advantage  that it's easy to remember\nand read. It is often possible to translate an HTTP request to an HTTPie\nargument list just by inlining the request elements. For example, compare this\nHTTP request:\n\n.. code-block:: http\n\n    POST /collection HTTP/1.1\n    X-API-Key: 123\n    User-Agent: Bacon/1.0\n    Content-Type: application/x-www-form-urlencoded\n\n    name=value\u0026name2=value2\n\n\nwith the HTTPie command that sends it:\n\n.. code-block:: bash\n\n    $ http -f POST example.org/collection \\\n      X-API-Key:123 \\\n      User-Agent:Bacon/1.0 \\\n      name=value \\\n      name2=value2\n\n\nNotice that both the order of elements and the syntax is very similar,\nand that only a small portion of the command is used to control HTTPie and\ndoesn't directly correspond to any part of the request (here it's only ``-f``\nasking HTTPie to send a form request).\n\nThe two modes, ``--pretty=all`` (default for terminal) and ``--pretty=none``\n(default for redirected output), allow for both user-friendly interactive use\nand usage from scripts, where HTTPie serves as a generic HTTP client.\n\nAs HTTPie is still under heavy development, the existing command line\nsyntax and some of the ``--OPTIONS`` may change slightly before\nHTTPie reaches its final version ``1.0``. All changes are recorded in the\n`change log`_.\n\n\n\nUser support\n------------\n\nPlease use the following support channels:\n\n* `GitHub issues \u003chttps://github.com/jkbr/httpie/issues\u003e`_\n  for bug reports and feature requests.\n* `Our Gitter chat room \u003chttps://gitter.im/jkbrzt/httpie\u003e`_\n  to ask questions, discuss features, and for general discussion.\n* `StackOverflow \u003chttps://stackoverflow.com\u003e`_\n  to ask questions (please make sure to use the\n  `httpie \u003chttp://stackoverflow.com/questions/tagged/httpie\u003e`_ tag).\n* Tweet directly to `@clihttp \u003chttps://twitter.com/clihttp\u003e`_.\n* You can also tweet directly to `@jakubroztocil`_.\n\n\nRelated projects\n----------------\n\nDependencies\n~~~~~~~~~~~~\n\nUnder the hood, HTTPie uses these two amazing libraries:\n\n* `Requests \u003chttp://python-requests.org\u003e`_\n  — Python HTTP library for humans\n* `Pygments \u003chttp://pygments.org/\u003e`_\n  — Python syntax highlighter\n\n\nHTTPie friends\n~~~~~~~~~~~~~~\n\nHTTPie plays exceptionally well with the following tools:\n\n* `jq \u003chttps://stedolan.github.io/jq/\u003e`_\n  — CLI JSON processor that\n  works great in conjunction with HTTPie\n* `http-prompt \u003chttps://github.com/eliangcs/http-prompt\u003e`_\n  —  interactive shell for HTTPie featuring autocomplete\n  and command syntax highlighting\n\n\nAlternatives\n~~~~~~~~~~~~\n\n* `httpcat \u003chttps://github.com/jakubroztocil/httpcat\u003e`_ — a lower-level sister utility\n  of HTTPie for constructing raw HTTP requests on the command line.\n* `curl \u003chttps://curl.haxx.se\u003e`_ — a \"Swiss knife\" command line tool and\n  an exceptional library for transferring data with URLs.\n\n\nContributing\n------------\n\nSee `CONTRIBUTING.rst \u003chttps://github.com/jakubroztocil/httpie/blob/master/CONTRIBUTING.rst\u003e`_.\n\n\nChange log\n----------\n\nSee `CHANGELOG \u003chttps://github.com/jakubroztocil/httpie/blob/master/CHANGELOG.rst\u003e`_.\n\n\nArtwork\n-------\n\nSee `claudiatd/httpie-artwork`_\n\n\nLicence\n-------\n\nBSD-3-Clause: `LICENSE \u003chttps://github.com/jakubroztocil/httpie/blob/master/LICENSE\u003e`_.\n\n\n\nAuthors\n-------\n\n`Jakub Roztocil`_  (`@jakubroztocil`_) created HTTPie and `these fine people`_\nhave contributed.\n\n\n.. _pip: https://pip.pypa.io/en/stable/installing/\n.. _Github API: http://developer.github.com/v3/issues/comments/#create-a-comment\n.. _these fine people: https://github.com/jakubroztocil/httpie/contributors\n.. _Jakub Roztocil: https://roztocil.co\n.. _@jakubroztocil: https://twitter.com/jakubroztocil\n.. _claudiatd/httpie-artwork: https://github.com/claudiatd/httpie-artwork\n\n\n.. |pypi| image:: https://img.shields.io/pypi/v/httpie.svg?style=flat-square\u0026label=latest%20stable%20version\n    :target: https://pypi.python.org/pypi/httpie\n    :alt: Latest version released on PyPi\n\n.. |coverage| image:: https://img.shields.io/coveralls/jakubroztocil/httpie/master.svg?style=flat-square\u0026label=coverage\n    :target: https://coveralls.io/r/jakubroztocil/httpie?branch=master\n    :alt: Test coverage\n\n.. |unix_build| image:: https://img.shields.io/travis/jakubroztocil/httpie/master.svg?style=flat-square\u0026label=unix%20build\n    :target: http://travis-ci.org/jakubroztocil/httpie\n    :alt: Build status of the master branch on Mac/Linux\n\n.. |gitter| image:: https://img.shields.io/gitter/room/jkbrzt/httpie.svg?style=flat-square\n    :target: https://gitter.im/jkbrzt/httpie\n    :alt: Chat on Gitter\n"
  },
  {
    "repo": "django/django",
    "content": "Django is a high-level Python Web framework that encourages rapid development\nand clean, pragmatic design. Thanks for checking it out.\n\nAll documentation is in the \"``docs``\" directory and online at\nhttps://docs.djangoproject.com/en/stable/. If you're just getting started,\nhere's how we recommend you read the docs:\n\n* First, read ``docs/intro/install.txt`` for instructions on installing Django.\n\n* Next, work through the tutorials in order (``docs/intro/tutorial01.txt``,\n  ``docs/intro/tutorial02.txt``, etc.).\n\n* If you want to set up an actual deployment server, read\n  ``docs/howto/deployment/index.txt`` for instructions.\n\n* You'll probably want to read through the topical guides (in ``docs/topics``)\n  next; from there you can jump to the HOWTOs (in ``docs/howto``) for specific\n  problems, and check out the reference (``docs/ref``) for gory details.\n\n* See ``docs/README`` for instructions on building an HTML version of the docs.\n\nDocs are updated rigorously. If you find any problems in the docs, or think\nthey should be clarified in any way, please take 30 seconds to fill out a\nticket here: https://code.djangoproject.com/newticket\n\nTo get more help:\n\n* Join the ``#django`` channel on irc.freenode.net. Lots of helpful people hang\n  out there. See https://en.wikipedia.org/wiki/Wikipedia:IRC/Tutorial if you're\n  new to IRC.\n\n* Join the django-users mailing list, or read the archives, at\n  https://groups.google.com/group/django-users.\n\nTo contribute to Django:\n\n* Check out https://docs.djangoproject.com/en/dev/internals/contributing/ for\n  information about getting involved.\n\nTo run Django's test suite:\n\n* Follow the instructions in the \"Unit tests\" section of\n  ``docs/internals/contributing/writing-code/unit-tests.txt``, published online at\n  https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/unit-tests/#running-the-unit-tests\n"
  },
  {
    "repo": "josephmisiti/awesome-machine-learning",
    "content": "# Awesome Machine Learning [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n\nA curated list of awesome machine learning frameworks, libraries and software (by language). Inspired by `awesome-php`.\n\nIf you want to contribute to this list (please do), send me a pull request or contact me [@josephmisiti](https://twitter.com/josephmisiti).\nAlso, a listed repository should be deprecated if:\n\n* Repository's owner explicitly say that \"this library is not maintained\".\n* Not committed for long time (2~3 years).\n\nFurther resources:\n\n* For a list of free machine learning books available for download, go [here](https://github.com/josephmisiti/awesome-machine-learning/blob/master/books.md).\n\n* For a list of (mostly) free machine learning courses available online, go [here](https://github.com/josephmisiti/awesome-machine-learning/blob/master/courses.md).\n\n* For a list of blogs on data science and  machine learning, go [here](https://github.com/josephmisiti/awesome-machine-learning/blob/master/blogs.md).\n\n* For a list of free-to-attend meetups and local events, go [here](https://github.com/josephmisiti/awesome-machine-learning/blob/master/meetups.md).\n\n## Table of Contents\n\n### Frameworks and Libraries\n\u003c!-- MarkdownTOC depth=4 --\u003e\n\n- [APL](#apl)\n    - [General-Purpose Machine Learning](#apl-general-purpose)\n- [C](#c)\n    - [General-Purpose Machine Learning](#c-general-purpose)\n    - [Computer Vision](#c-cv)\n- [C++](#cpp)\n    - [Computer Vision](#cpp-cv)\n    - [General-Purpose Machine Learning](#cpp-general-purpose)\n    - [Natural Language Processing](#cpp-nlp)\n    - [Speech Recognition](#speech-recognition-1)\n    - [Sequence Analysis](#cpp-sequence)\n    - [Gesture Recognition](#cpp-gestures)\n- [Common Lisp](#common-lisp)\n    - [General-Purpose Machine Learning](#common-lisp-general-purpose)\n- [Clojure](#clojure)\n    - [Natural Language Processing](#clojure-nlp)\n    - [General-Purpose Machine Learning](#clojure-general-purpose)\n    - [Data Analysis / Data Visualization](#clojure-data-analysis)\n- [Crystal](#crystal)\n    - [General-Purpose Machine Learning](#crystal-general-purpose)\n- [Elixir](#elixir)\n    - [General-Purpose Machine Learning](#elixir-general-purpose)\n    - [Natural Language Processing](#elixir-nlp)\n- [Erlang](#erlang)\n    - [General-Purpose Machine Learning](#erlang-general-purpose)\n- [Go](#go)\n    - [Natural Language Processing](#go-nlp)\n    - [General-Purpose Machine Learning](#go-general-purpose)\n    - [Data Analysis / Data Visualization](#go-data-analysis)\n    - [Facial Detection and Recognition](#go-facial-recognition)\n    - [Image Classification](#go-image-classification)\n- [Haskell](#haskell)\n    - [General-Purpose Machine Learning](#haskell-general-purpose)\n- [Java](#java)\n    - [Natural Language Processing](#java-nlp)\n    - [General-Purpose Machine Learning](#java-general-purpose)\n    - [Speech Recognition](#java-speech-recognition)\n    - [Data Analysis / Data Visualization](#java-data-analysis)\n    - [Deep Learning](#java-deep-learning)\n- [Javascript](#javascript)\n    - [Natural Language Processing](#javascript-nlp)\n    - [Data Analysis / Data Visualization](#javascript-data-analysis)\n    - [General-Purpose Machine Learning](#javascript-general-purpose)\n    - [Misc](#javascript-misc)\n    - [Demos and Scripts](#javascript-demos)\n- [Julia](#julia)\n    - [General-Purpose Machine Learning](#julia-general-purpose)\n    - [Natural Language Processing](#julia-nlp)\n    - [Data Analysis / Data Visualization](#julia-data-analysis)\n    - [Misc Stuff / Presentations](#julia-misc)\n- [Lua](#lua)\n    - [General-Purpose Machine Learning](#lua-general-purpose)\n    - [Demos and Scripts](#lua-demos)\n- [Matlab](#matlab)\n    - [Computer Vision](#matlab-cv)\n    - [Natural Language Processing](#matlab-nlp)\n    - [General-Purpose Machine Learning](#matlab-general-purpose)\n    - [Data Analysis / Data Visualization](#matlab-data-analysis)\n- [.NET](#net)\n    - [Computer Vision](#net-cv)\n    - [Natural Language Processing](#net-nlp)\n    - [General-Purpose Machine Learning](#net-general-purpose)\n    - [Data Analysis / Data Visualization](#net-data-analysis)\n- [Objective C](#objectivec)\n    - [General-Purpose Machine Learning](#objectivec-general-purpose)\n- [OCaml](#ocaml)\n    - [General-Purpose Machine Learning](#ocaml-general-purpose)\n- [Perl](#perl)\n    - [Data Analysis / Data Visualization](#perl-data)\n\t- [General-Purpose Machine Learning](#perl-ml)\n- [Perl 6](#perl6)\n- [PHP](#php)\n    - [Natural Language Processing](#php-nlp)\n    - [General-Purpose Machine Learning](#php-general-purpose)\n- [Python](#python)\n    - [Computer Vision](#python-cv)\n    - [Natural Language Processing](#python-nlp)\n    - [General-Purpose Machine Learning](#python-general-purpose)\n    - [Data Analysis / Data Visualization](#python-data-analysis)\n    - [Misc Scripts / iPython Notebooks / Codebases](#python-misc)\n    - [Kaggle Competition Source Code](#python-kaggle)\n    - [Neural Networks](#python-neural-networks)\n    - [Reinforcement Learning](#python-reinforcement-learning)\n- [Ruby](#ruby)\n    - [Natural Language Processing](#ruby-nlp)\n    - [General-Purpose Machine Learning](#ruby-general-purpose)\n    - [Data Analysis / Data Visualization](#ruby-data-analysis)\n    - [Misc](#ruby-misc)\n- [Rust](#rust)\n    - [General-Purpose Machine Learning](#rust-general-purpose)\n- [R](#r)\n    - [General-Purpose Machine Learning](#r-general-purpose)\n    - [Data Analysis / Data Visualization](#r-data-analysis)\n- [SAS](#sas)\n    - [General-Purpose Machine Learning](#sas-general-purpose)\n    - [Data Analysis / Data Visualization](#sas-data-analysis)\n    - [High Performance Machine Learning (MPP)](#sas-mpp)\n    - [Natural Language Processing](#sas-nlp)\n写个脚本把它们爬下来    - [Demos and Scripts](#sas-demos)\n- [Scala](#scala)\n    - [Natural Language Processing](#scala-nlp)\n    - [Data Analysis / Data Visualization](#scala-data-analysis)\n    - [General-Purpose Machine Learning](#scala-general-purpose)\n- [Swift](#swift)\n    - [General-Purpose Machine Learning](#swift-general-purpose)\n- [TensorFlow](#tensor)\n    - [General-Purpose Machine Learning](#tensor-general-purpose)\n\n### Tools\n\n- [Misc](#tools-misc)\n\n\n[Credits](#credits)\n\n\u003c!-- /MarkdownTOC --\u003e\n\n\u003ca name=\"apl\"\u003e\u003c/a\u003e\n## APL\n\n\u003ca name=\"apl-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n* [naive-apl](https://github.com/mattcunningham/naive-apl) - Naive Bayesian Classifier implementation in APL.\n\n\u003ca name=\"c\"\u003e\u003c/a\u003e\n## C\n\n\u003ca name=\"c-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n* [Darknet](https://github.com/pjreddie/darknet) - Darknet is an open source neural network framework written in C and CUDA. It is fast, easy to install, and supports CPU and GPU computation.\n* [Recommender](https://github.com/GHamrouni/Recommender) - A C library for product recommendations/suggestions using collaborative filtering (CF).\n* [Hybrid Recommender System](https://github.com/SeniorSA/hybrid-rs-trainner) - A hybrid recommender system based upon scikit-learn algorithms.\n* [neonrvm](https://github.com/siavashserver/neonrvm) - neonrvm is an open source machine learning library based on RVM technique. It's written in C programming language and comes with Python programming language bindings.\n\n\u003ca name=\"c-cv\"\u003e\u003c/a\u003e\n#### Computer Vision\n\n* [CCV](https://github.com/liuliu/ccv) - C-based/Cached/Core Computer Vision Library, A Modern Computer Vision Library.\n* [VLFeat](http://www.vlfeat.org/) - VLFeat is an open and portable library of computer vision algorithms, which has Matlab toolbox.\n\n### Speech Recognition\n* [HTK](http://htk.eng.cam.ac.uk/) -The Hidden Markov Model Toolkit (HTK) is a portable toolkit for building and manipulating hidden Markov models.\n\n\u003ca name=\"cpp\"\u003e\u003c/a\u003e\n## C++\n\n\u003ca name=\"cpp-cv\"\u003e\u003c/a\u003e\n#### Computer Vision\n\n* [DLib](http://dlib.net/imaging.html) - DLib has C++ and Python interfaces for face detection and training general object detectors.\n* [EBLearn](http://eblearn.sourceforge.net/) - Eblearn is an object-oriented C++ library that implements various machine learning models\n* [OpenCV](http://opencv.org) - OpenCV has C++, C, Python, Java and MATLAB interfaces and supports Windows, Linux, Android and Mac OS.\n* [VIGRA](https://github.com/ukoethe/vigra) - VIGRA is a generic cross-platform C++ computer vision and machine learning library for volumes of arbitrary dimensionality with Python bindings.\n\n\u003ca name=\"cpp-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n\n* [BanditLib](https://github.com/jkomiyama/banditlib) - A simple Multi-armed Bandit library.\n* [Caffe](http://caffe.berkeleyvision.org)  - A deep learning framework developed with cleanliness, readability, and speed in mind. [DEEP LEARNING]\n* [CatBoost](https://github.com/catboost/catboost) - General purpose gradient boosting on decision trees library with categorical features support out of the box. It is easy to install, contains fast inference implementation and supports CPU and GPU (even multi-GPU) computation.\n* [CNTK](https://github.com/Microsoft/CNTK) - The Computational Network Toolkit (CNTK) by Microsoft Research, is a unified deep-learning toolkit that describes neural networks as a series of computational steps via a directed graph.\n* [CUDA](https://code.google.com/p/cuda-convnet/) - This is a fast C++/CUDA implementation of convolutional [DEEP LEARNING]\n* [CXXNET](https://github.com/antinucleon/cxxnet) - Yet another deep learning framework with less than 1000 lines core code [DEEP LEARNING]\n* [DeepDetect](https://github.com/beniz/deepdetect) - A machine learning API and server written in C++11. It makes state of the art machine learning easy to work with and integrate into existing applications.\n* [Distributed Machine learning Tool Kit (DMTK)](http://www.dmtk.io/) - A distributed machine learning (parameter server) framework by Microsoft. Enables training models on large data sets across multiple machines. Current tools bundled with it include: LightLDA and Distributed (Multisense) Word Embedding.\n* [DLib](http://dlib.net/ml.html) - A suite of ML tools designed to be easy to imbed in other applications.\n* [DSSTNE](https://github.com/amznlabs/amazon-dsstne) - A software library created by Amazon for training and deploying deep neural networks using GPUs which emphasizes speed and scale over experimental flexibility.\n* [DyNet](https://github.com/clab/dynet) - A dynamic neural network library working well with networks that have dynamic structures that change for every training instance. Written in C++ with bindings in Python.\n* [encog-cpp](https://code.google.com/archive/p/encog-cpp)\n* [Fido](https://github.com/FidoProject/Fido) - A highly-modular C++ machine learning library for embedded electronics and robotics.\n* [igraph](http://igraph.org/c/) - General purpose graph library.\n* [Intel(R) DAAL](https://github.com/01org/daal) - A high performance software library developed by Intel and optimized for Intel's architectures. Library provides algorithmic building blocks for all stages of data analytics and allows to process data in batch, online and distributed modes.\n* [LightGBM](https://github.com/Microsoft/LightGBM) - Microsoft's fast, distributed, high performance gradient boosting (GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks.\n* [libfm](http://www.libfm.org/) - A generic approach that allows to mimic most factorization models by feature engineering.\n* [MLDB](https://mldb.ai) - The Machine Learning Database is a database designed for machine learning. Send it commands over a RESTful API to store data, explore it using SQL, then train machine learning models and expose them as APIs.\n* [mlpack](http://www.mlpack.org/) - A scalable C++ machine learning library.\n* [proNet-core](https://github.com/cnclabs/proNet-core) - A general-purpose network embedding framework: pair-wise representations optimization Network Edit.\n* [PyCUDA](https://mathema.tician.de/software/pycuda/) - Python interface to CUDA\n* [ROOT](https://root.cern.ch) - A modular scientific software framework. It provides all the functionalities needed to deal with big data processing, statistical analysis, visualization and storage.\n* [shark](http://image.diku.dk/shark/sphinx_pages/build/html/index.html) - A fast, modular, feature-rich open-source C++ machine learning library.\n* [Shogun](https://github.com/shogun-toolbox/shogun) - The Shogun Machine Learning Toolbox.\n* [sofia-ml](https://code.google.com/archive/p/sofia-ml) - Suite of fast incremental algorithms.\n* [Stan](http://mc-stan.org/) - A probabilistic programming language implementing full Bayesian statistical inference with Hamiltonian Monte Carlo sampling.\n* [Timbl](https://languagemachines.github.io/timbl/) - A software package/C++ library implementing several memory-based learning algorithms, among which IB1-IG, an implementation of k-nearest neighbor classification, and IGTree, a decision-tree approximation of IB1-IG. Commonly used for NLP.\n* [Vowpal Wabbit (VW)](https://github.com/JohnLangford/vowpal_wabbit/wiki) - A fast out-of-core learning system.\n* [Warp-CTC](https://github.com/baidu-research/warp-ctc) - A fast parallel implementation of Connectionist Temporal Classification (CTC), on both CPU and GPU.\n* [XGBoost](https://github.com/dmlc/xgboost) - A parallelized optimized general purpose gradient boosting library.\n* [LKYDeepNN](https://github.com/mosdeo/LKYDeepNN) -  A header-only C++11 Neural Network library. Low dependency, native traditional chinese document.\n* [xLearn](https://github.com/aksnzhy/xlearn) - A high performance, easy-to-use, and scalable machine learning package, which can be used to solve large-scale machine learning problems. xLearn is especially useful for solving machine learning problems on large-scale sparse data, which is very common in Internet services such as online advertisement and recommender systems.\n* [Featuretools](https://github.com/featuretools/featuretools) - A library for automated feature engineering. It excels at transforming transactional and relational datasets into feature matrices for machine learning using reusable feature engineering \"primitives\". \n\n\u003ca name=\"cpp-nlp\"\u003e\u003c/a\u003e\n#### Natural Language Processing\n\n* [BLLIP Parser](https://github.com/BLLIP/bllip-parser) - BLLIP Natural Language Parser (also known as the Charniak-Johnson parser).\n* [colibri-core](https://github.com/proycon/colibri-core) - C++ library, command line tools, and Python binding for extracting and working with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.\n* [CRF++](https://taku910.github.io/crfpp/) - Open source implementation of Conditional Random Fields (CRFs) for segmenting/labeling sequential data \u0026 other Natural Language Processing tasks.\n* [CRFsuite](http://www.chokkan.org/software/crfsuite/) - CRFsuite is an implementation of Conditional Random Fields (CRFs) for labeling sequential data.\n* [frog](https://github.com/LanguageMachines/frog) - Memory-based NLP suite developed for Dutch: PoS tagger, lemmatiser, dependency parser, NER, shallow parser, morphological analyzer.\n* [libfolia](https://github.com/LanguageMachines/libfolia) - C++ library for the [FoLiA format](http://proycon.github.io/folia/)\n* [MeTA](https://github.com/meta-toolkit/meta) - [MeTA : ModErn Text Analysis](https://meta-toolkit.org/) is a C++ Data Sciences Toolkit that facilitates mining big text data.\n* [MIT Information Extraction Toolkit](https://github.com/mit-nlp/MITIE) - C, C++, and Python tools for named entity recognition and relation extraction\n* [ucto](https://github.com/LanguageMachines/ucto) - Unicode-aware regular-expression based tokenizer for various languages. Tool and C++ library. Supports FoLiA format.\n\n#### Speech Recognition\n* [Kaldi](https://github.com/kaldi-asr/kaldi) - Kaldi is a toolkit for speech recognition written in C++ and licensed under the Apache License v2.0. Kaldi is intended for use by speech recognition researchers.\n\n\u003ca name=\"cpp-sequence\"\u003e\u003c/a\u003e\n#### Sequence Analysis\n* [ToPS](https://github.com/ayoshiaki/tops) - This is an objected-oriented framework that facilitates the integration of probabilistic models for sequences over a user defined alphabet.\n\n\u003ca name=\"cpp-gestures\"\u003e\u003c/a\u003e\n#### Gesture Detection\n* [grt](https://github.com/nickgillian/grt) - The Gesture Recognition Toolkit (GRT) is a cross-platform, open-source, C++ machine learning library designed for real-time gesture recognition.\n\n\u003ca name=\"common-lisp\"\u003e\u003c/a\u003e\n## Common Lisp\n\n\u003ca name=\"common-lisp-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n\n* [mgl](https://github.com/melisgl/mgl/) - Neural networks  (boltzmann machines, feed-forward and recurrent nets), Gaussian Processes.\n* [mgl-gpr](https://github.com/melisgl/mgl-gpr/) - Evolutionary algorithms.\n* [cl-libsvm](https://github.com/melisgl/cl-libsvm/) - Wrapper for the libsvm support vector machine library.\n* [cl-online-learning](https://github.com/masatoi/cl-online-learning) - Online learning algorithms (Perceptron, AROW, SCW, Logistic Regression).\n* [cl-random-forest](https://github.com/masatoi/cl-random-forest) - Implementation of Random Forest in Common Lisp.\n\n\u003ca name=\"clojure\"\u003e\u003c/a\u003e\n## Clojure\n\n\u003ca name=\"clojure-nlp\"\u003e\u003c/a\u003e\n#### Natural Language Processing\n\n* [Clojure-openNLP](https://github.com/dakrone/clojure-opennlp) - Natural Language Processing in Clojure (opennlp).\n* [Infections-clj](https://github.com/r0man/inflections-clj) - Rails-like inflection library for Clojure and ClojureScript.\n\n\u003ca name=\"clojure-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n\n* [Touchstone](https://github.com/ptaoussanis/touchstone) - Clojure A/B testing library.\n* [Clojush](https://github.com/lspector/Clojush) -  The Push programming language and the PushGP genetic programming system implemented in Clojure.\n* [Infer](https://github.com/aria42/infer) - Inference and machine learning in Clojure.\n* [Clj-ML](https://github.com/antoniogarrote/clj-ml) - A machine learning library for Clojure built on top of Weka and friends.\n* [DL4CLJ](https://github.com/engagor/dl4clj/) - Clojure wrapper for Deeplearning4j.\n* [Encog](https://github.com/jimpil/enclog) - Clojure wrapper for Encog (v3) (Machine-Learning framework that specializes in neural-nets).\n* [Fungp](https://github.com/vollmerm/fungp) - A genetic programming library for Clojure.\n* [Statistiker](https://github.com/clojurewerkz/statistiker) - Basic Machine Learning algorithms in Clojure.\n* [clortex](https://github.com/htm-community/clortex) - General Machine Learning library using Numenta’s Cortical Learning Algorithm.\n* [comportex](https://github.com/htm-community/comportex) - Functionally composable Machine Learning library using Numenta’s Cortical Learning Algorithm.\n* [cortex](https://github.com/thinktopic/cortex) - Neural networks, regression and feature learning in Clojure.\n* [lambda-ml](https://github.com/cloudkj/lambda-ml) - Simple, concise implementations of machine learning techniques and utilities in Clojure.\n\n\u003ca name=\"clojure-data-analysis\"\u003e\u003c/a\u003e\n#### Data Analysis / Data Visualization\n\n* [Incanter](http://incanter.org/) - Incanter is a Clojure-based, R-like platform for statistical computing and graphics.\n* [PigPen](https://github.com/Netflix/PigPen) - Map-Reduce for Clojure.\n* [Envision](https://github.com/clojurewerkz/envision) - Clojure Data Visualisation library, based on Statistiker and D3.\n\n\u003ca name=\"crystal\"\u003e\u003c/a\u003e\n## Crystal\n\n\u003ca name=\"crystal-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n\n* [machine](https://github.com/mathieulaporte/machine) - Simple machine learning algorithm.\n* [crystal-fann](https://github.com/bararchy/crystal-fann) - FANN (Fast Artificial Neural Network) binding.\n\n\u003ca name=\"elixir\"\u003e\u003c/a\u003e\n## Elixir\n\n\u003ca name=\"elixir-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n\n* [Simple Bayes](https://github.com/fredwu/simple_bayes) - A Simple Bayes / Naive Bayes implementation in Elixir.\n* [emel](https://github.com/mrdimosthenis/emel) - A simple and functional machine learning library written in Elixir.\n\n\u003ca name=\"elixir-nlp\"\u003e\u003c/a\u003e\n#### Natural Language Processing\n\n* [Stemmer](https://github.com/fredwu/stemmer) - An English (Porter2) stemming implementation in Elixir.\n\n\u003ca name=\"erlang\"\u003e\u003c/a\u003e\n## Erlang\n\n\u003ca name=\"erlang-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n\n* [Disco](https://github.com/discoproject/disco/) - Map Reduce in Erlang.\n* [Yanni](https://bitbucket.org/nato/yanni/overview) - ANN neural networks using Erlangs leightweight processes.\n\n\u003ca name=\"go\"\u003e\u003c/a\u003e\n## Go\n\n\u003ca name=\"go-nlp\"\u003e\u003c/a\u003e\n#### Natural Language Processing\n\n* [go-porterstemmer](https://github.com/reiver/go-porterstemmer) - A native Go clean room implementation of the Porter Stemming algorithm.\n* [paicehusk](https://github.com/Rookii/paicehusk) - Golang implementation of the Paice/Husk Stemming Algorithm.\n* [snowball](https://github.com/tebeka/snowball) - Snowball Stemmer for Go.\n* [Textbox](https://godoc.org/github.com/machinebox/sdk-go/textbox) - Natural language processing SDK from Machine Box\n* [go-ngram](https://github.com/Lazin/go-ngram) - In-memory n-gram index with compression.\n* [word-embedding](https://github.com/ynqa/word-embedding) - Word Embeddings: the full implementation of word2vec, GloVe in Go.\n* [sentences](https://github.com/neurosnap/sentences) - Golang implementation of Punkt sentence tokenizer.\n\n\u003ca name=\"go-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n\n* [eaopt](https://github.com/MaxHalford/eaopt) - An evolutionary optimization library.\n* [Go Learn](https://github.com/sjwhitworth/golearn) - Machine Learning for Go.\n* [go-pr](https://github.com/daviddengcn/go-pr) - Pattern recognition package in Go lang.\n* [go-ml](https://github.com/alonsovidales/go_ml) - Linear / Logistic regression, Neural Networks, Collaborative Filtering and Gaussian Multivariate Distribution.\n* [bayesian](https://github.com/jbrukh/bayesian) - Naive Bayesian Classification for Golang.\n* [go-galib](https://github.com/thoj/go-galib) - Genetic Algorithms library written in Go / Golang.\n* [Cloudforest](https://github.com/ryanbressler/CloudForest) - Ensembles of decision trees in Go/Golang.\n* [gobrain](https://github.com/goml/gobrain) - Neural Networks written in Go.\n* [GoNN](https://github.com/fxsjy/gonn) - GoNN is an implementation of Neural Network in Go Language, which includes BPNN, RBF, PCN.\n* [MXNet](https://github.com/dmlc/mxnet) - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.\n* [go-mxnet-predictor](https://github.com/songtianyi/go-mxnet-predictor) - Go binding for MXNet c_predict_api to do inference with pre-trained model.\n* [neat](https://github.com/jinyeom/neat) - Plug-and-play, parallel Go framework for NeuroEvolution of Augmenting Topologies (NEAT).\n* [go-ml-transpiler](https://github.com/znly/go-ml-transpiler) - An open source Go transpiler for machine learning models.\n\n\u003ca name=\"go-data-analysis\"\u003e\u003c/a\u003e\n#### Data Analysis / Data Visualization\n\n* [go-graph](https://github.com/StepLg/go-graph) - Graph library for Go/Golang language.\n* [SVGo](http://www.svgopen.org/2011/papers/34-SVGo_a_Go_Library_for_SVG_generation/) - The Go Language library for SVG generation.\n* [RF](https://github.com/fxsjy/RF.go) - Random forests implementation in Go.\n* [Glot](https://github.com/arafatk/glot) - Glot is a plotting library for Golang built on top of gnuplot. \n\n\u003ca name=\"go-facial-recognition\"\u003e\u003c/a\u003e\n#### Facial Detection and Recognition\n\n* [Facebox](https://godoc.org/github.com/machinebox/sdk-go/facebox) - Facial detection and recognition SDK with one-shot teaching from Machine Box\n\n\u003ca name=\"go-image-classification\"\u003e\u003c/a\u003e\n#### Image Classification\n\n* [Tagbox](https://godoc.org/github.com/machinebox/sdk-go/tagbox) - Image classification SDK with one-shot teaching from Machine Box\n* [Nudebox](https://godoc.org/github.com/machinebox/sdk-go/nudebox) - Nudity detection from Machine Box\n\n\u003ca name=\"haskell\"\u003e\u003c/a\u003e\n## Haskell\n\n\u003ca name=\"haskell-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n* [haskell-ml](https://github.com/ajtulloch/haskell-ml) - Haskell implementations of various ML algorithms.\n* [HLearn](https://github.com/mikeizbicki/HLearn) - a suite of libraries for interpreting machine learning models according to their algebraic structure.\n* [hnn](https://wiki.haskell.org/HNN) - Haskell Neural Network library.\n* [hopfield-networks](https://github.com/ajtulloch/hopfield-networks) - Hopfield Networks for unsupervised learning in Haskell.\n* [caffegraph](https://github.com/ajtulloch/dnngraph) - A DSL for deep neural networks.\n* [LambdaNet](https://github.com/jbarrow/LambdaNet) - Configurable Neural Networks in Haskell.\n\n\u003ca name=\"java\"\u003e\u003c/a\u003e\n## Java\n\n\u003ca name=\"java-nlp\"\u003e\u003c/a\u003e\n#### Natural Language Processing\n* [Cortical.io](http://www.cortical.io/) - Retina: an API performing complex NLP operations (disambiguation, classification, streaming text filtering, etc...) as quickly and intuitively as the brain.\n* [IRIS](https://github.com/cortical-io/Public/tree/master/iris) - [Cortical.io's](http://cortical.io) FREE NLP, Retina API Analysis Tool (written in JavaFX!) - [See the Tutorial Video](https://www.youtube.com/watch?v=CsF4pd7fGF0).\n* [CoreNLP](http://nlp.stanford.edu/software/corenlp.shtml) - Stanford CoreNLP provides a set of natural language analysis tools which can take raw English language text input and give the base forms of words.\n* [Stanford Parser](http://nlp.stanford.edu/software/lex-parser.shtml) - A natural language parser is a program that works out the grammatical structure of sentences.\n* [Stanford POS Tagger](http://nlp.stanford.edu/software/tagger.shtml) - A Part-Of-Speech Tagger (POS Tagger).\n* [Stanford Name Entity Recognizer](http://nlp.stanford.edu/software/CRF-NER.shtml) - Stanford NER is a Java implementation of a Named Entity Recognizer.\n* [Stanford Word Segmenter](http://nlp.stanford.edu/software/segmenter.shtml) - Tokenization of raw text is a standard pre-processing step for many NLP tasks.\n* [Tregex, Tsurgeon and Semgrex](http://nlp.stanford.edu/software/tregex.shtml) - Tregex is a utility for matching patterns in trees, based on tree relationships and regular expression matches on nodes (the name is short for \"tree regular expressions\").\n* [Stanford Phrasal: A Phrase-Based Translation System](http://nlp.stanford.edu/phrasal/)\n* [Stanford English Tokenizer](http://nlp.stanford.edu/software/tokenizer.shtml) - Stanford Phrasal is a state-of-the-art statistical phrase-based machine translation system, written in Java.\n* [Stanford Tokens Regex](http://nlp.stanford.edu/software/tokensregex.shtml) - A tokenizer divides text into a sequence of tokens, which roughly correspond to \"words\".\n* [Stanford Temporal Tagger](http://nlp.stanford.edu/software/sutime.shtml) - SUTime is a library for recognizing and normalizing time expressions.\n* [Stanford SPIED](http://nlp.stanford.edu/software/patternslearning.shtml) - Learning entities from unlabeled text starting with seed sets using patterns in an iterative fashion.\n* [Stanford Topic Modeling Toolbox](http://nlp.stanford.edu/software/tmt/tmt-0.4/) - Topic modeling tools to social scientists and others who wish to perform analysis on datasets.\n* [Twitter Text Java](https://github.com/twitter/twitter-text-java) - A Java implementation of Twitter's text processing library.\n* [MALLET](http://mallet.cs.umass.edu/) - A Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text.\n* [OpenNLP](https://opennlp.apache.org/) - a machine learning based toolkit for the processing of natural language text.\n* [LingPipe](http://alias-i.com/lingpipe/index.html) - A tool kit for processing text using computational linguistics.\n* [ClearTK](https://code.google.com/archive/p/cleartk) - ClearTK provides a framework for developing statistical natural language processing (NLP) components in Java and is built on top of Apache UIMA.\n* [Apache cTAKES](http://ctakes.apache.org/) - Apache clinical Text Analysis and Knowledge Extraction System (cTAKES) is an open-source natural language processing system for information extraction from electronic medical record clinical free-text.\n* [ClearNLP](https://github.com/clir/clearnlp) - The ClearNLP project provides software and resources for natural language processing. The project started at the Center for Computational Language and EducAtion Research, and is currently developed by the Center for Language and Information Research at Emory University. This project is under the Apache 2 license.\n* [CogcompNLP](https://github.com/IllinoisCogComp/illinois-cogcomp-nlp) - This project collects a number of core libraries for Natural Language Processing (NLP) developed in the University of Illinois' Cognitive Computation Group, for example `illinois-core-utilities` which provides a set of NLP-friendly data structures and a number of NLP-related utilities that support writing NLP applications, running experiments, etc, `illinois-edison` a library for feature extraction from illinois-core-utilities data structures and many other packages.\n\n\u003ca name=\"java-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n\n* [aerosolve](https://github.com/airbnb/aerosolve) - A machine learning library by Airbnb designed from the ground up to be human friendly.\n* [AMIDST Toolbox](http://www.amidsttoolbox.com/) - A Java Toolbox for Scalable Probabilistic Machine Learning.\n* [Datumbox](https://github.com/datumbox/datumbox-framework) - Machine Learning framework for rapid development of Machine Learning and Statistical applications.\n* [ELKI](https://elki-project.github.io/) - Java toolkit for data mining. (unsupervised: clustering, outlier detection etc.)\n* [Encog](https://github.com/encog/encog-java-core) - An advanced neural network and machine learning framework. Encog contains classes to create a wide variety of networks, as well as support classes to normalize and process data for these neural networks. Encog trains using multithreaded resilient propagation. Encog can also make use of a GPU to further speed processing time. A GUI based workbench is also provided to help model and train neural networks.\n* [FlinkML in Apache Flink](https://ci.apache.org/projects/flink/flink-docs-master/apis/batch/libs/ml/index.html) - Distributed machine learning library in Flink.\n* [H2O](https://github.com/h2oai/h2o-3) - ML engine that supports distributed learning on Hadoop, Spark or your laptop via APIs in R, Python, Scala, REST/JSON.\n* [htm.java](https://github.com/numenta/htm.java) - General Machine Learning library using Numenta’s Cortical Learning Algorithm.\n* [java-deeplearning](https://github.com/deeplearning4j/deeplearning4j) - Distributed Deep Learning Platform for Java, Clojure, Scala.\n* [liblinear-java](https://github.com/bwaldvogel/liblinear-java) - Java version of liblinear.\n* [Mahout](https://github.com/apache/mahout) - Distributed machine learning.\n* [Meka](http://meka.sourceforge.net/) - An open source implementation of methods for multi-label classification and evaluation (extension to Weka).\n* [MLlib in Apache Spark](http://spark.apache.org/docs/latest/mllib-guide.html) - Distributed machine learning library in Spark\n* [Hydrosphere Mist](https://github.com/Hydrospheredata/mist) - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.\n* [Neuroph](http://neuroph.sourceforge.net/) - Neuroph is lightweight Java neural network framework\n* [ORYX](https://github.com/oryxproject/oryx) - Lambda Architecture Framework using Apache Spark and Apache Kafka with a specialization for real-time large-scale machine learning.\n* [Samoa](https://samoa.incubator.apache.org/) SAMOA is a framework that includes distributed machine learning for data streams with an interface to plug-in different stream processing platforms.\n* [RankLib](https://sourceforge.net/p/lemur/wiki/RankLib/) - RankLib is a library of learning to rank algorithms.\n* [rapaio](https://github.com/padreati/rapaio) - statistics, data mining and machine learning toolbox in Java.\n* [RapidMiner](https://rapidminer.com) - RapidMiner integration into Java code.\n* [Stanford Classifier](http://nlp.stanford.edu/software/classifier.shtml) - A classifier is a machine learning tool that will take data items and place them into one of k classes.\n* [SmileMiner](https://github.com/haifengl/smile) - Statistical Machine Intelligence \u0026 Learning Engine.\n* [SystemML](https://github.com/apache/incubator-systemml) - flexible, scalable machine learning (ML) language.\n* [WalnutiQ](https://github.com/WalnutiQ/wAlnut) - object oriented model of the human brain.\n* [Weka](http://www.cs.waikato.ac.nz/ml/weka/) - Weka is a collection of machine learning algorithms for data mining tasks.\n* [LBJava](https://github.com/IllinoisCogComp/lbjava/) - Learning Based Java is a modeling language for the rapid development of software systems, offers a convenient, declarative syntax for classifier and constraint definition directly in terms of the objects in the programmer's application.\n\n\n\u003ca name=\"java-speech-recognition\"\u003e\u003c/a\u003e\n#### Speech Recognition\n* [CMU Sphinx](http://cmusphinx.sourceforge.net/) - Open Source Toolkit For Speech Recognition purely based on Java speech recognition library.\n\n\u003ca name=\"java-data-analysis\"\u003e\u003c/a\u003e\n#### Data Analysis / Data Visualization\n\n* [Flink](http://flink.apache.org/) - Open source platform for distributed stream and batch data processing.\n* [Hadoop](https://github.com/apache/hadoop-mapreduce) - Hadoop/HDFS.\n* [Onyx](https://github.com/onyx-platform/onyx) - Distributed, masterless, high performance, fault tolerant data processing. Written entirely in Clojure.\n* [Spark](https://github.com/apache/spark) - Spark is a fast and general engine for large-scale data processing.\n* [Storm](http://storm.apache.org/) - Storm is a distributed realtime computation system.\n* [Impala](https://github.com/cloudera/impala) - Real-time Query for Hadoop.\n* [DataMelt](http://jwork.org/dmelt/) - Mathematics software for numeric computation, statistics, symbolic calculations, data analysis and data visualization.\n* [Dr. Michael Thomas Flanagan's Java Scientific Library](http://www.ee.ucl.ac.uk/~mflanaga/java/)\n\n\u003ca name=\"java-deep-learning\"\u003e\u003c/a\u003e\n#### Deep Learning\n\n* [Deeplearning4j](https://github.com/deeplearning4j/deeplearning4j) - Scalable deep learning for industry with parallel GPUs.\n\n\u003ca name=\"javascript\"\u003e\u003c/a\u003e\n## Javascript\n\n\u003ca name=\"javascript-nlp\"\u003e\u003c/a\u003e\n#### Natural Language Processing\n\n* [Twitter-text](https://github.com/twitter/twitter-text) - A JavaScript implementation of Twitter's text processing library.\n* [natural](https://github.com/NaturalNode/natural) - General natural language facilities for node.\n* [Knwl.js](https://github.com/loadfive/Knwl.js) - A Natural Language Processor in JS.\n* [Retext](https://github.com/wooorm/retext) - Extensible system for analyzing and manipulating natural language.\n* [NLP Compromise](https://github.com/nlp-compromise/compromise) - Natural Language processing in the browser.\n* [nlp.js](https://github.com/axa-group/nlp.js) - An NLP library built in node over Natural, with entity extraction, sentiment analysis, automatic language identify, and so more\n\n\n\n\u003ca name=\"javascript-data-analysis\"\u003e\u003c/a\u003e\n#### Data Analysis / Data Visualization\n\n* [D3.js](https://d3js.org/)\n* [High Charts](http://www.highcharts.com/)\n* [NVD3.js](http://nvd3.org/)\n* [dc.js](http://dc-js.github.io/dc.js/)\n* [chartjs](http://www.chartjs.org/)\n* [dimple](http://dimplejs.org/)\n* [amCharts](https://www.amcharts.com/)\n* [D3xter](https://github.com/NathanEpstein/D3xter) - Straight forward plotting built on D3.\n* [statkit](https://github.com/rigtorp/statkit) - Statistics kit for JavaScript.\n* [datakit](https://github.com/nathanepstein/datakit) - A lightweight framework for data analysis in JavaScript\n* [science.js](https://github.com/jasondavies/science.js/) - Scientific and statistical computing in JavaScript.\n* [Z3d](https://github.com/NathanEpstein/Z3d) - Easily make interactive 3d plots built on Three.js\n* [Sigma.js](http://sigmajs.org/) - JavaScript library dedicated to graph drawing.\n* [C3.js](http://c3js.org/)- customizable library based on D3.js for easy chart drawing.\n* [Datamaps](http://datamaps.github.io/)- Customizable SVG map/geo visualizations using D3.js.\n* [ZingChart](https://www.zingchart.com/)- library written on Vanilla JS for big data visualization.\n* [cheminfo](http://www.cheminfo.org/) - Platform for data visualization and analysis, using the [visualizer](https://github.com/npellet/visualizer) project.\n* [Learn JS Data](http://learnjsdata.com/)\n* [AnyChart](http://www.anychart.com/)\n* [FusionCharts](http://www.fusioncharts.com/)\n* [Nivo](http://nivo.rocks) - built on top of the awesome d3 and Reactjs libraries\n\n\n\u003ca name=\"javascript-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n\n* [Convnet.js](http://cs.stanford.edu/people/karpathy/convnetjs/) - ConvNetJS is a Javascript library for training Deep Learning models[DEEP LEARNING]\n* [Clusterfck](http://harthur.github.io/clusterfck/) - Agglomerative hierarchical clustering implemented in Javascript for Node.js and the browser.\n* [Clustering.js](https://github.com/emilbayes/clustering.js) - Clustering algorithms implemented in Javascript for Node.js and the browser.\n* [Decision Trees](https://github.com/serendipious/nodejs-decision-tree-id3) - NodeJS Implementation of Decision Tree using ID3 Algorithm.\n* [DN2A](https://github.com/dn2a/dn2a-javascript) - Digital Neural Networks Architecture.\n* [figue](https://code.google.com/archive/p/figue) - K-means, fuzzy c-means and agglomerative clustering.\n* [Gaussian Mixture Model](https://github.com/lukapopijac/gaussian-mixture-model) - Unsupervised machine learning with multivariate Gaussian mixture model.\n* [Node-fann](https://github.com/rlidwka/node-fann) - FANN (Fast Artificial Neural Network Library) bindings for Node.js\n* [Keras.js](https://github.com/transcranial/keras-js) - Run Keras models in the browser, with GPU support provided by WebGL 2.\n* [Kmeans.js](https://github.com/emilbayes/kMeans.js) - Simple Javascript implementation of the k-means algorithm, for node.js and the browser.\n* [LDA.js](https://github.com/primaryobjects/lda) - LDA topic modeling for Node.js\n* [Learning.js](https://github.com/yandongliu/learningjs) - Javascript implementation of logistic regression/c4.5 decision tree\n* [Kalimdor.js](https://github.com/jasonshin/kalimdorjs) - Machine Learning library for the web, Node.js and developers\n* [Machine Learning](http://joonku.com/project/machine_learning) - Machine learning library for Node.js\n* [machineJS](https://github.com/ClimbsRocks/machineJS) - Automated machine learning, data formatting, ensembling, and hyperparameter optimization for competitions and exploration- just give it a .csv file!\n* [mil-tokyo](https://github.com/mil-tokyo) - List of several machine learning libraries.\n* [Node-SVM](https://github.com/nicolaspanel/node-svm) - Support Vector Machine for Node.js\n* [Brain](https://github.com/harthur/brain) - Neural networks in JavaScript **[Deprecated]**\n* [Brain.js](https://github.com/harthur-org/brain.js) - Neural networks in JavaScript - continued community fork of [Brain](https://github.com/harthur/brain).\n* [Bayesian-Bandit](https://github.com/omphalos/bayesian-bandit.js) - Bayesian bandit implementation for Node and the browser.\n* [Synaptic](https://github.com/cazala/synaptic) - Architecture-free neural network library for Node.js and the browser.\n* [kNear](https://github.com/NathanEpstein/kNear) - JavaScript implementation of the k nearest neighbors algorithm for supervised learning.\n* [NeuralN](https://github.com/totemstech/neuraln) - C++ Neural Network library for Node.js. It has advantage on large dataset and multi-threaded training.\n* [kalman](https://github.com/itamarwe/kalman) - Kalman filter for Javascript.\n* [shaman](https://github.com/luccastera/shaman) - Node.js library with support for both simple and multiple linear regression.\n* [ml.js](https://github.com/mljs/ml) - Machine learning and numerical analysis tools for Node.js and the Browser!\n* [ml5](https://github.com/ml5js/ml5-library) - Friendly machine learning for the web!\n* [Pavlov.js](https://github.com/NathanEpstein/Pavlov.js) - Reinforcement learning using Markov Decision Processes.\n* [MXNet](https://github.com/dmlc/mxnet) - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.\n* [TensorFlow.js](https://js.tensorflow.org/) - A WebGL accelerated, browser based JavaScript library for training and deploying ML models.\n* [JSMLT](https://github.com/jsmlt/jsmlt) - Machine learning toolkit with classification and clustering for Node.js; supports visualization (see [visualml.io](https://visualml.io)).\n* [xgboost-node](https://github.com/nuanio/xgboost-node) - Run XGBoost model and make predictions in Node.js.\n* [Netron](https://github.com/lutzroeder/netron) - Visualizer for machine learning models.\n* [WebDNN](https://github.com/mil-tokyo/webdnn) - Fast Deep Neural Network Javascript Framework. WebDNN uses next generation JavaScript API, WebGPU for GPU execution, and WebAssembly for CPU execution.  \n\n\u003ca name=\"javascript-misc\"\u003e\u003c/a\u003e\n#### Misc\n\n* [stdlib](https://github.com/stdlib-js/stdlib) - A standard library for JavaScript and Node.js, with an emphasis on numeric computing. The library provides a collection of robust, high performance libraries for mathematics, statistics, streams, utilities, and more.\n* [sylvester](https://github.com/jcoglan/sylvester) - Vector and Matrix math for JavaScript.\n* [simple-statistics](https://github.com/simple-statistics/simple-statistics) - A JavaScript implementation of descriptive, regression, and inference statistics. Implemented in literate JavaScript with no dependencies, designed to work in all modern browsers (including IE) as well as in Node.js.\n* [regression-js](https://github.com/Tom-Alexander/regression-js) - A javascript library containing a collection of least squares fitting methods for finding a trend in a set of data.\n* [Lyric](https://github.com/flurry/Lyric) - Linear Regression library.\n* [GreatCircle](https://github.com/mwgg/GreatCircle) - Library for calculating great circle distance.\n* [MLPleaseHelp](https://github.com/jgreenemi/MLPleaseHelp) - MLPleaseHelp is a simple ML resource search engine. You can use this search engine right now at [https://jgreenemi.github.io/MLPleaseHelp/](https://jgreenemi.github.io/MLPleaseHelp/), provided via Github Pages.\n\n\u003ca name=\"javascript-demos\"\u003e\u003c/a\u003e\n#### Demos and Scripts\n* [The Bot](https://github.com/sta-ger/TheBot) - Example of how the neural network learns to predict the angle between two points created with [Synaptic](https://github.com/cazala/synaptic).\n* [Half Beer](https://github.com/sta-ger/HalfBeer) - Beer glass classifier created with [Synaptic](https://github.com/cazala/synaptic).\n\n\u003ca name=\"julia\"\u003e\u003c/a\u003e\n## Julia\n\n\u003ca name=\"julia-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n\n* [MachineLearning](https://github.com/benhamner/MachineLearning.jl) - Julia Machine Learning library.\n* [MLBase](https://github.com/JuliaStats/MLBase.jl) - A set of functions to support the development of machine learning algorithms.\n* [PGM](https://github.com/JuliaStats/PGM.jl) - A Julia framework for probabilistic graphical models.\n* [DA](https://github.com/trthatcher/DiscriminantAnalysis.jl) - Julia package for Regularized Discriminant Analysis.\n* [Regression](https://github.com/lindahua/Regression.jl) - Algorithms for regression analysis (e.g. linear regression and logistic regression).\n* [Local Regression](https://github.com/JuliaStats/Loess.jl) - Local regression, so smooooth!.\n* [Naive Bayes](https://github.com/nutsiepully/NaiveBayes.jl) - Simple Naive Bayes implementation in Julia.\n* [Mixed Models](https://github.com/dmbates/MixedModels.jl) - A Julia package for fitting (statistical) mixed-effects models.\n* [Simple MCMC](https://github.com/fredo-dedup/SimpleMCMC.jl) - basic mcmc sampler implemented in Julia.\n* [Distance](https://github.com/JuliaStats/Distance.jl) - Julia module for Distance evaluation.\n* [Decision Tree](https://github.com/bensadeghi/DecisionTree.jl) - Decision Tree Classifier and Regressor.\n* [Neural](https://github.com/compressed/BackpropNeuralNet.jl) - A neural network in Julia.\n* [MCMC](https://github.com/doobwa/MCMC.jl) - MCMC tools for Julia.\n* [Mamba](https://github.com/brian-j-smith/Mamba.jl) - Markov chain Monte Carlo (MCMC) for Bayesian analysis in Julia.\n* [GLM](https://github.com/JuliaStats/GLM.jl) - Generalized linear models in Julia.\n* [Gaussian Processes](https://github.com/STOR-i/GaussianProcesses.jl) - Julia package for Gaussian processes.\n* [Online Learning](https://github.com/lendle/OnlineLearning.jl)\n* [GLMNet](https://github.com/simonster/GLMNet.jl) - Julia wrapper for fitting Lasso/ElasticNet GLM models using glmnet.\n* [Clustering](https://github.com/JuliaStats/Clustering.jl) - Basic functions for clustering data: k-means, dp-means, etc.\n* [SVM](https://github.com/JuliaStats/SVM.jl) - SVM's for Julia.\n* [Kernel Density](https://github.com/JuliaStats/KernelDensity.jl) - Kernel density estimators for julia.\n* [Dimensionality Reduction](https://github.com/JuliaStats/DimensionalityReduction.jl) - Methods for dimensionality reduction.\n* [NMF](https://github.com/JuliaStats/NMF.jl) - A Julia package for non-negative matrix factorization.\n* [ANN](https://github.com/EricChiang/ANN.jl) - Julia artificial neural networks.\n* [Mocha](https://github.com/pluskid/Mocha.jl) - Deep Learning framework for Julia inspired by Caffe.\n* [XGBoost](https://github.com/dmlc/XGBoost.jl) - eXtreme Gradient Boosting Package in Julia.\n* [ManifoldLearning](https://github.com/wildart/ManifoldLearning.jl) - A Julia package for manifold learning and nonlinear dimensionality reduction.\n* [MXNet](https://github.com/dmlc/mxnet) - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.\n* [Merlin](https://github.com/hshindo/Merlin.jl) - Flexible Deep Learning Framework in Julia.\n* [ROCAnalysis](https://github.com/davidavdav/ROCAnalysis.jl) - Receiver Operating Characteristics and functions for evaluation probabilistic binary classifiers.\n* [GaussianMixtures](https://github.com/davidavdav/GaussianMixtures.jl) - Large scale Gaussian Mixture Models.\n* [ScikitLearn](https://github.com/cstjean/ScikitLearn.jl) - Julia implementation of the scikit-learn API.\n* [Knet](https://github.com/denizyuret/Knet.jl) - Koç University Deep Learning Framework.\n\n\u003ca name=\"julia-nlp\"\u003e\u003c/a\u003e\n#### Natural Language Processing\n\n* [Topic Models](https://github.com/slycoder/TopicModels.jl) - TopicModels for Julia.\n* [Text Analysis](https://github.com/johnmyleswhite/TextAnalysis.jl) - Julia package for text analysis.\n\n\n\u003ca name=\"julia-data-analysis\"\u003e\u003c/a\u003e\n#### Data Analysis / Data Visualization\n\n* [Graph Layout](https://github.com/IainNZ/GraphLayout.jl) - Graph layout algorithms in pure Julia.\n* [LightGraphs](https://github.com/JuliaGraphs/LightGraphs.jl) - Graph modeling and analysis.\n* [Data Frames Meta](https://github.com/JuliaStats/DataFramesMeta.jl) - Metaprogramming tools for DataFrames.\n* [Julia Data](https://github.com/nfoti/JuliaData) - library for working with tabular data in Julia.\n* [Data Read](https://github.com/WizardMac/ReadStat.jl) - Read files from Stata, SAS, and SPSS.\n* [Hypothesis Tests](https://github.com/JuliaStats/HypothesisTests.jl) - Hypothesis tests for Julia.\n* [Gadfly](https://github.com/GiovineItalia/Gadfly.jl) - Crafty statistical graphics for Julia.\n* [Stats](https://github.com/JuliaStats/Stats.jl) - Statistical tests for Julia.\n* [RDataSets](https://github.com/johnmyleswhite/RDatasets.jl) - Julia package for loading many of the data sets available in R.\n* [DataFrames](https://github.com/JuliaStats/DataFrames.jl) - library for working with tabular data in Julia.\n* [Distributions](https://github.com/JuliaStats/Distributions.jl) - A Julia package for probability distributions and associated functions.\n* [Data Arrays](https://github.com/JuliaStats/DataArrays.jl) - Data structures that allow missing values.\n* [Time Series](https://github.com/JuliaStats/TimeSeries.jl) - Time series toolkit for Julia.\n* [Sampling](https://github.com/lindahua/Sampling.jl) - Basic sampling algorithms for Julia.\n\n\u003ca name=\"julia-misc\"\u003e\u003c/a\u003e\n#### Misc Stuff / Presentations\n\n* [DSP](https://github.com/JuliaDSP/DSP.jl) - Digital Signal Processing (filtering, periodograms, spectrograms, window functions).\n* [JuliaCon Presentations](https://github.com/JuliaCon/presentations) - Presentations for JuliaCon.\n* [SignalProcessing](https://github.com/davidavdav/SignalProcessing.jl) - Signal Processing tools for Julia.\n* [Images](https://github.com/JuliaImages/Images.jl) - An image library for Julia.\n\n\u003ca name=\"lua\"\u003e\u003c/a\u003e\n## Lua\n\n\u003ca name=\"lua-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n\n* [Torch7](http://torch.ch/)\n  * [cephes](https://github.com/deepmind/torch-cephes) - Cephes mathematical functions library, wrapped for Torch. Provides and wraps the 180+ special mathematical functions from the Cephes mathematical library, developed by Stephen L. Moshier. It is used, among many other places, at the heart of SciPy.\n  * [autograd](https://github.com/twitter/torch-autograd) - Autograd automatically differentiates native Torch code. Inspired by the original Python version.\n  * [graph](https://github.com/torch/graph) - Graph package for Torch.\n  * [randomkit](https://github.com/deepmind/torch-randomkit) - Numpy's randomkit, wrapped for Torch.\n  * [signal](http://soumith.ch/torch-signal/signal/) - A signal processing toolbox for Torch-7. FFT, DCT, Hilbert, cepstrums, stft.\n  * [nn](https://github.com/torch/nn) - Neural Network package for Torch.\n  * [torchnet](https://github.com/torchnet/torchnet) - framework for torch which provides a set of abstractions aiming at encouraging code re-use as well as encouraging modular programming.\n  * [nngraph](https://github.com/torch/nngraph) - This package provides graphical computation for nn library in Torch7.\n  * [nnx](https://github.com/clementfarabet/lua---nnx) - A completely unstable and experimental package that extends Torch's builtin nn library.\n  * [rnn](https://github.com/Element-Research/rnn) - A Recurrent Neural Network library that extends Torch's nn. RNNs, LSTMs, GRUs, BRNNs, BLSTMs, etc.\n  * [dpnn](https://github.com/Element-Research/dpnn) - Many useful features that aren't part of the main nn package.\n  * [dp](https://github.com/nicholas-leonard/dp) - A deep learning library designed for streamlining research and development using the Torch7 distribution. It emphasizes flexibility through the elegant use of object-oriented design patterns.\n  * [optim](https://github.com/torch/optim) - An optimization library for Torch. SGD, Adagrad, Conjugate-Gradient, LBFGS, RProp and more.\n  * [unsup](https://github.com/koraykv/unsup) - A package for unsupervised learning in Torch. Provides modules that are compatible with nn (LinearPsd, ConvPsd, AutoEncoder, ...), and self-contained algorithms (k-means, PCA).\n  * [manifold](https://github.com/clementfarabet/manifold) - A package to manipulate manifolds.\n  * [svm](https://github.com/koraykv/torch-svm) - Torch-SVM library.\n  * [lbfgs](https://github.com/clementfarabet/lbfgs) - FFI Wrapper for liblbfgs.\n  * [vowpalwabbit](https://github.com/clementfarabet/vowpal_wabbit) - An old vowpalwabbit interface to torch.\n  * [OpenGM](https://github.com/clementfarabet/lua---opengm) - OpenGM is a C++ library for graphical modeling, and inference. The Lua bindings provide a simple way of describing graphs, from Lua, and then optimizing them with OpenGM.\n  * [spaghetti](https://github.com/MichaelMathieu/lua---spaghetti) - Spaghetti (sparse linear) module for torch7 by @MichaelMathieu\n  * [LuaSHKit](https://github.com/ocallaco/LuaSHkit) - A lua wrapper around the Locality sensitive hashing library SHKit\n  * [kernel smoothing](https://github.com/rlowrance/kernel-smoothers) - KNN, kernel-weighted average, local linear regression smoothers.\n  * [cutorch](https://github.com/torch/cutorch) - Torch CUDA Implementation.\n  * [cunn](https://github.com/torch/cunn) - Torch CUDA Neural Network Implementation.\n  * [imgraph](https://github.com/clementfarabet/lua---imgraph) - An image/graph library for Torch. This package provides routines to construct graphs on images, segment them, build trees out of them, and convert them back to images.\n  * [videograph](https://github.com/clementfarabet/videograph) - A video/graph library for Torch. This package provides routines to construct graphs on videos, segment them, build trees out of them, and convert them back to videos.\n  * [saliency](https://github.com/marcoscoffier/torch-saliency) - code and tools around integral images. A library for finding interest points based on fast integral histograms.\n  * [stitch](https://github.com/marcoscoffier/lua---stitch) - allows us to use hugin to stitch images and apply same stitching to a video sequence.\n  * [sfm](https://github.com/marcoscoffier/lua---sfm) - A bundle adjustment/structure from motion package.\n  * [fex](https://github.com/koraykv/fex) - A package for feature extraction in Torch. Provides SIFT and dSIFT modules.\n  * [OverFeat](https://github.com/sermanet/OverFeat) - A state-of-the-art generic dense feature extractor.\n  * [wav2letter](https://github.com/facebookresearch/wav2letter) - a simple and efficient end-to-end Automatic Speech Recognition (ASR) system from Facebook AI Research.\n* [Numeric Lua](http://numlua.luaforge.net/)\n* [Lunatic Python](http://labix.org/lunatic-python)\n* [SciLua](http://scilua.org/)\n* [Lua - Numerical Algorithms](https://bitbucket.org/lucashnegri/lna)\n* [Lunum](https://github.com/jzrake/lunum)\n\n\u003ca name=\"lua-demos\"\u003e\u003c/a\u003e\n#### Demos and Scripts\n* [Core torch7 demos repository](https://github.com/e-lab/torch7-demos).\n  * linear-regression, logistic-regression\n  * face detector (training and detection as separate demos)\n  * mst-based-segmenter\n  * train-a-digit-classifier\n  * train-autoencoder\n  * optical flow demo\n  * train-on-housenumbers\n  * train-on-cifar\n  * tracking with deep nets\n  * kinect demo\n  * filter-bank visualization\n  * saliency-networks\n* [Training a Convnet for the Galaxy-Zoo Kaggle challenge(CUDA demo)](https://github.com/soumith/galaxyzoo)\n* [Music Tagging](https://github.com/mbhenaff/MusicTagging) - Music Tagging scripts for torch7.\n* [torch-datasets](https://github.com/rosejn/torch-datasets) - Scripts to load several popular datasets including:\n  * BSR 500\n  * CIFAR-10\n  * COIL\n  * Street View House Numbers\n  * MNIST\n  * NORB\n* [Atari2600](https://github.com/fidlej/aledataset) - Scripts to generate a dataset with static frames from the Arcade Learning Environment.\n\n\n\n\u003ca name=\"matlab\"\u003e\u003c/a\u003e\n## Matlab\n\n\u003ca name=\"matlab-cv\"\u003e\u003c/a\u003e\n#### Computer Vision\n\n* [Contourlets](http://www.ifp.illinois.edu/~minhdo/software/contourlet_toolbox.tar) - MATLAB source code that implements the contourlet transform and its utility functions.\n* [Shearlets](http://www.shearlab.org/software) - MATLAB code for shearlet transform.\n* [Curvelets](http://www.curvelet.org/software.html) - The Curvelet transform is a higher dimensional generalization of the Wavelet transform designed to represent images at different scales and different angles.\n* [Bandlets](http://www.cmap.polytechnique.fr/~peyre/download/) - MATLAB code for bandlet transform.\n* [mexopencv](http://kyamagu.github.io/mexopencv/) - Collection and a development kit of MATLAB mex functions for OpenCV library.\n\n\u003ca name=\"matlab-nlp\"\u003e\u003c/a\u003e\n#### Natural Language Processing\n\n* [NLP](https://amplab.cs.berkeley.edu/an-nlp-library-for-matlab/) - An NLP library for Matlab.\n\n\u003ca name=\"matlab-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n\n* [Training a deep autoencoder or a classifier\non MNIST digits](http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html) - Training a deep autoencoder or a classifier\non MNIST digits[DEEP LEARNING].\n* [Convolutional-Recursive Deep Learning for 3D Object Classification](http://www.socher.org/index.php/Main/Convolutional-RecursiveDeepLearningFor3DObjectClassification) - Convolutional-Recursive Deep Learning for 3D Object Classification[DEEP LEARNING].\n* [t-Distributed Stochastic Neighbor Embedding](http://homepage.tudelft.nl/19j49/t-SNE.html) - t-Distributed Stochastic Neighbor Embedding (t-SNE) is a (prize-winning) technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets.\n* [Spider](http://people.kyb.tuebingen.mpg.de/spider/) - The spider is intended to be a complete object orientated environment for machine learning in Matlab.\n* [LibSVM](http://www.csie.ntu.edu.tw/~cjlin/libsvm/#matlab) - A Library for Support Vector Machines.\n* [ThunderSVM](https://github.com/zeyiwen/thundersvm) - An Open-Source SVM Library on GPUs and CPUs\n* [LibLinear](http://www.csie.ntu.edu.tw/~cjlin/liblinear/#download) - A Library for Large Linear Classification.\n* [Machine Learning Module](https://github.com/josephmisiti/machine-learning-module) - Class on machine w/ PDF, lectures, code\n* [Caffe](http://caffe.berkeleyvision.org)  - A deep learning framework developed with cleanliness, readability, and speed in mind.\n* [Pattern Recognition Toolbox](https://github.com/covartech/PRT)  - A complete object-oriented environment for machine learning in Matlab.\n* [Pattern Recognition and Machine Learning](https://github.com/PRML/PRMLT) - This package contains the matlab implementation of the algorithms described in the book Pattern Recognition and Machine Learning by C. Bishop.\n* [Optunity](http://optunity.readthedocs.io/en/latest/) - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search. Optunity is written in Python but interfaces seamlessly with MATLAB.\n* [MXNet](https://github.com/apache/incubator-mxnet/) - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.\n* [Machine Learning in MatLab/Octave](https://github.com/trekhleb/machine-learning-octave) - examples of popular machine learning algorithms (neural networks, linear/logistic regressions, K-Means, etc.) with code examples and mathematics behind them being explained.\n\n\n\u003ca name=\"matlab-data-analysis\"\u003e\u003c/a\u003e\n#### Data Analysis / Data Visualization\n\n* [matlab_gbl](https://www.cs.purdue.edu/homes/dgleich/packages/matlab_bgl/) - MatlabBGL is a Matlab package for working with graphs.\n* [gamic](http://www.mathworks.com/matlabcentral/fileexchange/24134-gaimc---graph-algorithms-in-matlab-code) - Efficient pure-Matlab implementations of graph algorithms to complement MatlabBGL's mex functions.\n\n\u003ca name=\"net\"\u003e\u003c/a\u003e\n## .NET\n\n\u003ca name=\"net-cv\"\u003e\u003c/a\u003e\n#### Computer Vision\n\n* [OpenCVDotNet](https://code.google.com/archive/p/opencvdotnet) - A wrapper for the OpenCV project to be used with .NET applications.\n* [Emgu CV](http://www.emgu.com/wiki/index.php/Main_Page) - Cross platform wrapper of OpenCV which can be compiled in Mono to be run on Windows, Linus, Mac OS X, iOS, and Android.\n* [AForge.NET](http://www.aforgenet.com/framework/) - Open source C# framework for developers and researchers in the fields of Computer Vision and Artificial Intelligence. Development has now shifted to GitHub.\n* [Accord.NET](http://accord-framework.net) - Together with AForge.NET, this library can provide image processing and computer vision algorithms to Windows, Windows RT and Windows Phone. Some components are also available for Java and Android.\n\n\u003ca name=\"net-nlp\"\u003e\u003c/a\u003e\n#### Natural Language Processing\n\n* [Stanford.NLP for .NET](https://github.com/sergey-tihon/Stanford.NLP.NET/) - A full port of Stanford NLP packages to .NET and also available precompiled as a NuGet package.\n\n\u003ca name=\"net-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n\n* [Accord-Framework](http://accord-framework.net/) -The Accord.NET Framework is a complete framework for building machine learning, computer vision, computer audition, signal processing and statistical applications.\n* [Accord.MachineLearning](http://www.nuget.org/packages/Accord.MachineLearning/) - Support Vector Machines, Decision Trees, Naive Bayesian models, K-means, Gaussian Mixture models and general algorithms such as Ransac, Cross-validation and Grid-Search for machine-learning applications. This package is part of the Accord.NET Framework.\n* [DiffSharp](http://diffsharp.github.io/DiffSharp/) - An automatic differentiation (AD) library providing exact and efficient derivatives (gradients, Hessians, Jacobians, directional derivatives, and matrix-free Hessian- and Jacobian-vector products) for machine learning and optimization applications. Operations can be nested to any level, meaning that you can compute exact higher-order derivatives and differentiate functions that are internally making use of differentiation, for applications such as hyperparameter optimization.\n* [Encog](http://www.nuget.org/packages/encog-dotnet-core/) -  An advanced neural network and machine learning framework. Encog contains classes to create a wide variety of networks, as well as support classes to normalize and process data for these neural networks. Encog trains using multithreaded resilient propagation. Encog can also make use of a GPU to further speed processing time. A GUI based workbench is also provided to help model and train neural networks.\n* [GeneticSharp](https://github.com/giacomelli/GeneticSharp) - Multi-platform genetic algorithm library for .NET Core and .NET Framework. The library has several implementations of GA operators, like: selection, crossover, mutation, reinsertion and termination.\n* [Infer.NET](http://infernet.azurewebsites.net/) - Infer.NET is a framework for running Bayesian inference in graphical models. One can use Infer.NET to solve many different kinds of machine learning problems, from standard problems like classification, recommendation or clustering through to customised solutions to domain-specific problems. Infer.NET has been used in a wide variety of domains including information retrieval, bioinformatics, epidemiology, vision, and many others.\n* [ML.NET](https://github.com/dotnet/machinelearning) - ML.NET is a cross-platform open-source machine learning framework which makes machine learning accessible to .NET developers. ML.NET was originally developed in Microsoft Research and evolved into a significant framework over the last decade and is used across many product groups in Microsoft like Windows, Bing, PowerPoint, Excel and more.\n* [Neural Network Designer](https://sourceforge.net/projects/nnd/) - DBMS management system and designer for neural networks. The designer application is developed using WPF, and is a user interface which allows you to design your neural network, query the network, create and configure chat bots that are capable of asking questions and learning from your feed back.  The chat bots can even scrape the internet for information to return in their output as well as to use for learning.\n* [Vulpes](https://github.com/fsprojects/Vulpes) - Deep belief and deep learning implementation written in F# and leverages CUDA GPU execution with Alea.cuBase.\n\n\u003ca name=\"net-data-analysis\"\u003e\u003c/a\u003e\n#### Data Analysis / Data Visualization\n\n* [numl](http://www.nuget.org/packages/numl/) - numl is a machine learning library intended to ease the use of using standard modeling techniques for both prediction and clustering.\n* [Math.NET Numerics](http://www.nuget.org/packages/MathNet.Numerics/) - Numerical foundation of the Math.NET project, aiming to provide methods and algorithms for numerical computations in science, engineering and every day use. Supports .Net 4.0, .Net 3.5 and Mono on Windows, Linux and Mac; Silverlight 5, WindowsPhone/SL 8, WindowsPhone 8.1 and Windows 8 with PCL Portable Profiles 47 and 344; Android/iOS with Xamarin.\n* [Sho](https://www.microsoft.com/en-us/research/project/sho-the-net-playground-for-data/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fprojects%2Fsho%2F) - Sho is an interactive environment for data analysis and scientific computing that lets you seamlessly connect scripts (in IronPython) with compiled code (in .NET) to enable fast and flexible prototyping. The environment includes powerful and efficient libraries for linear algebra as well as data visualization that can be used from any .NET language, as well as a feature-rich interactive shell for rapid development.\n\n\u003ca name=\"objectivec\"\u003e\u003c/a\u003e\n## Objective C\n\n\u003ca name=\"objectivec-general-purpose\"\u003e\u003c/a\u003e\n### General-Purpose Machine Learning\n\n* [YCML](https://github.com/yconst/YCML) - A Machine Learning framework for Objective-C and Swift (OS X / iOS).\n* [MLPNeuralNet](https://github.com/nikolaypavlov/MLPNeuralNet) - Fast multilayer perceptron neural network library for iOS and Mac OS X. MLPNeuralNet predicts new examples by trained neural network. It is built on top of the Apple's Accelerate Framework, using vectorized operations and hardware acceleration if available.\n* [MAChineLearning](https://github.com/gianlucabertani/MAChineLearning) - An Objective-C multilayer perceptron library, with full support for training through backpropagation. Implemented using vDSP and vecLib, it's 20 times faster than its Java equivalent. Includes sample code for use from Swift.\n* [BPN-NeuralNetwork](https://github.com/Kalvar/ios-BPN-NeuralNetwork) - It implemented 3 layers neural network ( Input Layer, Hidden Layer and Output Layer ) and it named Back Propagation Neural Network (BPN). This network can be used in products recommendation, user behavior analysis, data mining and data analysis.\n* [Multi-Perceptron-NeuralNetwork](https://github.com/Kalvar/ios-Multi-Perceptron-NeuralNetwork) - it implemented multi-perceptrons neural network (ニューラルネットワーク) based on Back Propagation Neural Network (BPN) and designed unlimited-hidden-layers.\n* [KRHebbian-Algorithm](https://github.com/Kalvar/ios-KRHebbian-Algorithm) - It is a non-supervisor and self-learning algorithm (adjust the weights) in neural network of Machine Learning.\n* [KRKmeans-Algorithm](https://github.com/Kalvar/ios-KRKmeans-Algorithm) - It implemented K-Means the clustering and classification algorithm. It could be used in data mining and image compression.\n* [KRFuzzyCMeans-Algorithm](https://github.com/Kalvar/ios-KRFuzzyCMeans-Algorithm) - It implemented Fuzzy C-Means (FCM) the fuzzy clustering / classification algorithm on Machine Learning. It could be used in data mining and image compression.\n\n\u003ca name=\"ocaml\"\u003e\u003c/a\u003e\n## OCaml\n\n\u003ca name=\"ocaml-general-purpose\"\u003e\u003c/a\u003e\n### General-Purpose Machine Learning\n\n* [Oml](https://github.com/hammerlab/oml/) - A general statistics and machine learning library.\n* [GPR](http://mmottl.github.io/gpr/) - Efficient Gaussian Process Regression in OCaml.\n* [Libra-Tk](http://libra.cs.uoregon.edu) - Algorithms for learning and inference with discrete probabilistic models.\n* [TensorFlow](https://github.com/LaurentMazare/tensorflow-ocaml) - OCaml bindings for TensorFlow.\n\n\u003ca name=\"perl\"\u003e\u003c/a\u003e\n## Perl\n\n\u003ca name=\"perl-data\"\u003e\u003c/a\u003e\n### Data Analysis / Data Visualization\n\n* [Perl Data Language](https://metacpan.org/pod/Paws::MachineLearning),\na pluggable architecture for data and image processing, which can\nbe\n[used for machine learning](https://github.com/zenogantner/PDL-ML). \n\n\u003ca name=\"perl-ml\"\u003e\u003c/a\u003e\n### General-Purpose Machine Learning\n\n* [MXnet for Deep Learning, in Perl](https://github.com/dmlc/mxnet/tree/master/perl-package),\nalso [released in CPAN](https://metacpan.org/pod/AI::MXNet).\n* [Paws::MachineLearning](https://metacpan.org/pod/Paws::MachineLearning),\nusing AWS machine learning platform from Perl.\n* [Algorithm::SVMLight](https://metacpan.org/pod/Algorithm::SVMLight),\n  implementation of Support Vector Machines with SVMLight under it.\n  \n* Several machine learning and artificial intelligence models are\n  included in the [`AI`](https://metacpan.org/search?size=20\u0026q=AI)\n  namespace. For instance, you can\n  find [Naïve Bayes](https://metacpan.org/pod/AI::NaiveBayes). \n\n\u003ca name=\"perl6\"\u003e\u003c/a\u003e\n## Perl 6\n\n*\n  [Support Vector Machines](https://github.com/titsuki/p6-Algorithm-LibSVM)\n* [Naïve Bayes](https://github.com/titsuki/p6-Algorithm-NaiveBayes)\n\n### Data Analysis / Data Visualization\n\n* [Perl Data Language](https://metacpan.org/pod/Paws::MachineLearning),\na pluggable architecture for data and image processing, which can\nbe\n[used for machine learning](https://github.com/zenogantner/PDL-ML). \n\n### General-Purpose Machine Learning\n\n\u003ca name=\"php\"\u003e\u003c/a\u003e\n## PHP\n\n\u003ca name=\"php-nlp\"\u003e\u003c/a\u003e\n### Natural Language Processing\n\n* [jieba-php](https://github.com/fukuball/jieba-php) - Chinese Words Segmentation Utilities.\n\n\u003ca name=\"php-general-purpose\"\u003e\u003c/a\u003e\n### General-Purpose Machine Learning\n\n* [PHP-ML](https://github.com/php-ai/php-ml) - Machine Learning library for PHP. Algorithms, Cross Validation, Neural Network, Preprocessing, Feature Extraction and much more in one library.\n* [PredictionBuilder](https://github.com/denissimon/prediction-builder) - A library for machine learning that builds predictions using a linear regression.\n* [19 Questions](https://github.com/fulldecent/19-questions) - A machine learning / bayesian inference assigning attributes to objects.\n\n\u003ca name=\"python\"\u003e\u003c/a\u003e\n## Python\n\n\u003ca name=\"python-cv\"\u003e\u003c/a\u003e\n#### Computer Vision\n\n* [Scikit-Image](https://github.com/scikit-image/scikit-image) - A collection of algorithms for image processing in Python.\n* [SimpleCV](http://simplecv.org/) - An open source computer vision framework that gives access to several high-powered computer vision libraries, such as OpenCV. Written on Python and runs on Mac, Windows, and Ubuntu Linux.\n* [Vigranumpy](https://github.com/ukoethe/vigra) - Python bindings for the VIGRA C++ computer vision library.\n* [OpenFace](https://cmusatyalab.github.io/openface/) - Free and open source face recognition with deep neural networks.\n* [PCV](https://github.com/jesolem/PCV) - Open source Python module for computer vision.\n* [face_recognition](https://github.com/ageitgey/face_recognition) - Face recognition library that recognize and manipulate faces from Python or from the command line.\n* [dockerface](https://github.com/natanielruiz/dockerface) - Easy to install and use deep learning Faster R-CNN face detection for images and video in a docker container.\n* [Detectron](https://github.com/facebookresearch/Detectron) - FAIR's software system that implements state-of-the-art object detection algorithms, including Mask R-CNN. It is written in Python and powered by the Caffe2 deep learning framework.\n* [albumentations](https://github.com/albu/albumentations) - А fast and framework agnostic image augmentation library that implements a diverse set of augmentation techniques. Supports classification, segmentation, detection out of the box. Was used to win a number of Deep Learning competitions at Kaggle, Topcoder and those that were a part of the CVPR workshops.\n* [pytessarct](https://github.com/madmaze/pytesseract)-Python-tesseract is an optical character recognition (OCR) tool for python. That is, it will recognize and \"read\" the text embedded in images.Python-tesseract is a wrapper for \u003ca href=\"https://github.com/tesseract-ocr/tesseract\"\u003eGoogle's Tesseract-OCR Engine\u003c/a\u003e.\n* [imutils](https://github.com/jrosebr1/imutils)-A library containg Convenience functions to make basic image processing operations such as translation, rotation, resizing, skeletonization, and displaying Matplotlib images easier with OpenCV and Python.\n* [PyTorchCV](https://github.com/CVBox/PyTorchCV) - A PyTorch-Based Framework for Deep Learning in Computer Vision.\n\n\u003ca name=\"python-nlp\"\u003e\u003c/a\u003e\n#### Natural Language Processing\n\n* [NLTK](http://www.nltk.org/) - A leading platform for building Python programs to work with human language data.\n* [Pattern](http://www.clips.ua.ac.be/pattern) - A web mining module for the Python programming language. It has tools for natural language processing, machine learning, among others.\n* [Quepy](https://github.com/machinalis/quepy) - A python framework to transform natural language questions to queries in a database query language.\n* [TextBlob](http://textblob.readthedocs.io/en/dev/) - Providing a consistent API for diving into common natural language processing (NLP) tasks. Stands on the giant shoulders of NLTK and Pattern, and plays nicely with both.\n* [YAlign](https://github.com/machinalis/yalign) - A sentence aligner, a friendly tool for extracting parallel sentences from comparable corpora.\n* [jieba](https://github.com/fxsjy/jieba#jieba-1) - Chinese Words Segmentation Utilities.\n* [SnowNLP](https://github.com/isnowfy/snownlp) - A library for processing Chinese text.\n* [spammy](https://github.com/prodicus/spammy) - A library for email Spam filtering built on top of nltk\n* [loso](https://github.com/victorlin/loso) - Another Chinese segmentation library.\n* [genius](https://github.com/duanhongyi/genius) - A Chinese segment base on Conditional Random Field.\n* [KoNLPy](http://konlpy.org) - A Python package for Korean natural language processing.\n* [nut](https://github.com/pprett/nut) - Natural language Understanding Toolkit.\n* [Rosetta](https://github.com/columbia-applied-data-science/rosetta) - Text processing tools and wrappers (e.g. Vowpal Wabbit)\n* [BLLIP Parser](https://pypi.python.org/pypi/bllipparser/) - Python bindings for the BLLIP Natural Language Parser (also known as the Charniak-Johnson parser).\n* [PyNLPl](https://github.com/proycon/pynlpl) - Python Natural Language Processing Library. General purpose NLP library for Python. Also contains some specific modules for parsing common NLP formats, most notably for [FoLiA](http://proycon.github.io/folia/), but also ARPA language models, Moses phrasetables, GIZA++ alignments.\n* [python-ucto](https://github.com/proycon/python-ucto) - Python binding to ucto (a unicode-aware rule-based tokenizer for various languages).\n* [python-frog](https://github.com/proycon/python-frog) - Python binding to Frog, an NLP suite for Dutch. (pos tagging, lemmatisation, dependency parsing, NER)\n* [python-zpar](https://github.com/EducationalTestingService/python-zpar) - Python bindings for [ZPar](https://github.com/frcchang/zpar), a statistical part-of-speech-tagger, constiuency parser, and dependency parser for English.\n* [colibri-core](https://github.com/proycon/colibri-core) - Python binding to C++ library for extracting and working with with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.\n* [spaCy](https://github.com/honnibal/spaCy/) - Industrial strength NLP with Python and Cython.\n* [PyStanfordDependencies](https://github.com/dmcc/PyStanfordDependencies) - Python interface for converting Penn Treebank trees to Stanford Dependencies.\n* [Distance](https://github.com/doukremt/distance) - Levenshtein and Hamming distance computation.\n* [Fuzzy Wuzzy](https://github.com/seatgeek/fuzzywuzzy) - Fuzzy String Matching in Python.\n* [jellyfish](https://github.com/jamesturk/jellyfish) - a python library for doing approximate and phonetic matching of strings.\n* [editdistance](https://pypi.python.org/pypi/editdistance) - fast implementation of edit distance.\n* [textacy](https://github.com/chartbeat-labs/textacy) - higher-level NLP built on Spacy.\n* [stanford-corenlp-python](https://github.com/dasmith/stanford-corenlp-python) - Python wrapper for [Stanford CoreNLP](https://github.com/stanfordnlp/CoreNLP)\n* [CLTK](https://github.com/cltk/cltk) - The Classical Language Toolkit.\n* [rasa_nlu](https://github.com/golastmile/rasa_nlu) - turn natural language into structured data.\n* [yase](https://github.com/PPACI/yase) - Transcode sentence (or other sequence) to list of word vector .\n* [Polyglot](https://github.com/aboSamoor/polyglot) - Multilingual text (NLP) processing toolkit.\n* [DrQA](https://github.com/facebookresearch/DrQA) - Reading Wikipedia to answer open-domain questions.\n* [Dedupe](https://github.com/dedupeio/dedupe) - A python library for accurate and scalable fuzzy matching, record deduplication and entity-resolution.\n* [Snips NLU](https://github.com/snipsco/snips-nlu) - Natural Language Understanding library for intent classification and entity extraction\n* [NeuroNER](https://github.com/Franck-Dernoncourt/NeuroNER) - Named-entity recognition using neural networks providing state-of-the-art-results\n\n\u003ca name=\"python-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n* [steppy](https://github.com/neptune-ml/steppy) -\u003e Lightweight, Python library for fast and reproducible machine learning experimentation. Introduces very simple interface that enables clean machine learning pipeline design.\n* [steppy-toolkit](https://github.com/neptune-ml/steppy-toolkit) -\u003e Curated collection of the neural networks, transformers and models that make your machine learning work faster and more effective.\n* [CNTK](https://github.com/Microsoft/CNTK) - Microsoft Cognitive Toolkit (CNTK), an open source deep-learning toolkit. Documentation can be found [here](https://docs.microsoft.com/cognitive-toolkit/).\n* [auto_ml](https://github.com/ClimbsRocks/auto_ml) - Automated machine learning for production and analytics. Lets you focus on the fun parts of ML, while outputting production-ready code, and detailed analytics of your dataset and results. Includes support for NLP, XGBoost, CatBoost, LightGBM, and soon, deep learning. \n* [machine learning](https://github.com/jeff1evesque/machine-learning) - automated build consisting of a [web-interface](https://github.com/jeff1evesque/machine-learning#web-interface), and set of [programmatic-interface](https://github.com/jeff1evesque/machine-learning#programmatic-interface) API, for support vector machines.  Corresponding dataset(s) are stored into a SQL database, then generated model(s) used for prediction(s), are stored into a NoSQL datastore.\n* [XGBoost](https://github.com/dmlc/xgboost) - Python bindings for eXtreme Gradient Boosting (Tree) Library.\n* [Apache SINGA](https://singa.apache.org) - An Apache Incubating project for developing an open source machine learning library.\n* [Bayesian Methods for Hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers) - Book/iPython notebooks on Probabilistic Programming in Python.\n* [Featureforge](https://github.com/machinalis/featureforge) A set of tools for creating and testing machine learning features, with a scikit-learn compatible API.\n* [MLlib in Apache Spark](http://spark.apache.org/docs/latest/mllib-guide.html) - Distributed machine learning library in Spark\n* [Hydrosphere Mist](https://github.com/Hydrospheredata/mist) - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.\n* [scikit-learn](http://scikit-learn.org/) - A Python module for machine learning built on top of SciPy.\n* [metric-learn](https://github.com/all-umass/metric-learn) - A Python module for metric learning.\n* [SimpleAI](https://github.com/simpleai-team/simpleai) Python implementation of many of the artificial intelligence algorithms described on the book \"Artificial Intelligence, a Modern Approach\". It focuses on providing an easy to use, well documented and tested library.\n* [astroML](http://www.astroml.org/) - Machine Learning and Data Mining for Astronomy.\n* [graphlab-create](https://turi.com/products/create/docs/) - A library with various machine learning models (regression, clustering, recommender systems, graph analytics, etc.) implemented on top of a disk-backed DataFrame.\n* [BigML](https://bigml.com) - A library that contacts external servers.\n* [pattern](https://github.com/clips/pattern) - Web mining module for Python.\n* [NuPIC](https://github.com/numenta/nupic) - Numenta Platform for Intelligent Computing.\n* [Pylearn2](https://github.com/lisa-lab/pylearn2) - A Machine Learning library based on [Theano](https://github.com/Theano/Theano).\n* [keras](https://github.com/keras-team/keras) - High-level neural networks frontend for [TensorFlow](https://github.com/tensorflow/tensorflow), [CNTK](https://github.com/Microsoft/CNTK) and [Theano](https://github.com/Theano/Theano).\n* [Lasagne](https://github.com/Lasagne/Lasagne) - Lightweight library to build and train neural networks in Theano.\n* [hebel](https://github.com/hannes-brt/hebel) - GPU-Accelerated Deep Learning Library in Python.\n* [Chainer](https://github.com/pfnet/chainer) - Flexible neural network framework.\n* [prophet](https://facebookincubator.github.io/prophet) - Fast and automated time series forecasting framework by Facebook.\n* [gensim](https://github.com/RaRe-Technologies/gensim) - Topic Modelling for Humans.\n* [topik](https://github.com/ContinuumIO/topik) - Topic modelling toolkit.\n* [PyBrain](https://github.com/pybrain/pybrain) - Another Python Machine Learning Library.\n* [Brainstorm](https://github.com/IDSIA/brainstorm) - Fast, flexible and fun neural networks. This is the successor of PyBrain.\n* [Surprise](http://surpriselib.com) - A scikit for building and analyzing recommender systems.\n* [Crab](https://github.com/muricoca/crab) - A flexible, fast recommender engine.\n* [python-recsys](https://github.com/ocelma/python-recsys) - A Python library for implementing a Recommender System.\n* [thinking bayes](https://github.com/AllenDowney/ThinkBayes) - Book on Bayesian Analysis.\n* [Image-to-Image Translation with Conditional Adversarial Networks](https://github.com/williamFalcon/pix2pix-keras) - Implementation of image to image (pix2pix) translation from the paper by [isola et al](https://arxiv.org/pdf/1611.07004.pdf).[DEEP LEARNING]\n* [Restricted Boltzmann Machines](https://github.com/echen/restricted-boltzmann-machines) -Restricted Boltzmann Machines in Python. [DEEP LEARNING]\n* [Bolt](https://github.com/pprett/bolt) - Bolt Online Learning Toolbox.\n* [CoverTree](https://github.com/patvarilly/CoverTree) - Python implementation of cover trees, near-drop-in replacement for scipy.spatial.kdtree\n* [nilearn](https://github.com/nilearn/nilearn) - Machine learning for NeuroImaging in Python.\n* [neuropredict](https://github.com/raamana/neuropredict) - Aimed at novice machine learners and non-expert programmers, this package offers easy (no coding needed) and comprehensive machine learning (evaluation and full report of predictive performance WITHOUT requiring you to code) in Python for NeuroImaging and any other type of features. This is aimed at absorbing the much of the ML workflow, unlike other packages like nilearn and pymvpa, which require you to learn their API and code to produce anything useful.\n* [imbalanced-learn](http://contrib.scikit-learn.org/imbalanced-learn/) - Python module to perform under sampling and over sampling with various techniques.\n* [Shogun](https://github.com/shogun-toolbox/shogun) - The Shogun Machine Learning Toolbox.\n* [Pyevolve](https://github.com/perone/Pyevolve) - Genetic algorithm framework.\n* [Caffe](http://caffe.berkeleyvision.org)  - A deep learning framework developed with cleanliness, readability, and speed in mind.\n* [breze](https://github.com/breze-no-salt/breze) - Theano based library for deep and recurrent neural networks.\n* [pyhsmm](https://github.com/mattjj/pyhsmm) - library for approximate unsupervised inference in Bayesian Hidden Markov Models (HMMs) and explicit-duration Hidden semi-Markov Models (HSMMs), focusing on the Bayesian Nonparametric extensions, the HDP-HMM and HDP-HSMM, mostly with weak-limit approximations.\n* [mrjob](https://pythonhosted.org/mrjob/) - A library to let Python program run on Hadoop.\n* [SKLL](https://github.com/EducationalTestingService/skll) - A wrapper around scikit-learn that makes it simpler to conduct experiments.\n* [neurolab](https://github.com/zueve/neurolab) - https://github.com/zueve/neurolab\n* [Spearmint](https://github.com/JasperSnoek/spearmint) - Spearmint is a package to perform Bayesian optimization according to the algorithms outlined in the paper: Practical Bayesian Optimization of Machine Learning Algorithms. Jasper Snoek, Hugo Larochelle and Ryan P. Adams. Advances in Neural Information Processing Systems, 2012.\n* [Pebl](https://github.com/abhik/pebl/) - Python Environment for Bayesian Learning.\n* [Theano](https://github.com/Theano/Theano/) - Optimizing GPU-meta-programming code generating array oriented optimizing math compiler in Python.\n* [TensorFlow](https://github.com/tensorflow/tensorflow/) - Open source software library for numerical computation using data flow graphs.\n* [yahmm](https://github.com/jmschrei/yahmm/) - Hidden Markov Models for Python, implemented in Cython for speed and efficiency.\n* [python-timbl](https://github.com/proycon/python-timbl) - A Python extension module wrapping the full TiMBL C++ programming interface. Timbl is an elaborate k-Nearest Neighbours machine learning toolkit.\n* [deap](https://github.com/deap/deap) - Evolutionary algorithm framework.\n* [pydeep](https://github.com/andersbll/deeppy) - Deep Learning In Python.\n* [mlxtend](https://github.com/rasbt/mlxtend) - A library consisting of useful tools for data science and machine learning tasks.\n* [neon](https://github.com/NervanaSystems/neon) - Nervana's [high-performance](https://github.com/soumith/convnet-benchmarks) Python-based Deep Learning framework [DEEP LEARNING].\n* [Optunity](http://optunity.readthedocs.io/en/latest/) - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search.\n* [Neural Networks and Deep Learning](https://github.com/mnielsen/neural-networks-and-deep-learning) - Code samples for my book \"Neural Networks and Deep Learning\" [DEEP LEARNING].\n* [Annoy](https://github.com/spotify/annoy) - Approximate nearest neighbours implementation.\n* [skflow](https://github.com/tensorflow/skflow) - Simplified interface for TensorFlow, mimicking Scikit Learn.\n* [TPOT](https://github.com/rhiever/tpot) - Tool that automatically creates and optimizes machine learning pipelines using genetic programming. Consider it your personal data science assistant, automating a tedious part of machine learning.\n* [pgmpy](https://github.com/pgmpy/pgmpy) A python library for working with Probabilistic Graphical Models.\n* [DIGITS](https://github.com/NVIDIA/DIGITS) - The Deep Learning GPU Training System (DIGITS) is a web application for training deep learning models.\n* [Orange](http://orange.biolab.si/) - Open source data visualization and data analysis for novices and experts.\n* [MXNet](https://github.com/dmlc/mxnet) - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.\n* [milk](https://github.com/luispedro/milk) - Machine learning toolkit focused on supervised classification.\n* [TFLearn](https://github.com/tflearn/tflearn) - Deep learning library featuring a higher-level API for TensorFlow.\n* [REP](https://github.com/yandex/rep) - an IPython-based environment for conducting data-driven research in a consistent and reproducible way. REP is not trying to substitute scikit-learn, but extends it and provides better user experience.\n* [rgf_python](https://github.com/fukatani/rgf_python) - Python bindings for Regularized Greedy Forest (Tree) Library.\n* [skbayes](https://github.com/AmazaspShumik/sklearn-bayes) - Python package for Bayesian Machine Learning with scikit-learn API.\n* [fuku-ml](https://github.com/fukuball/fuku-ml) - Simple machine learning library, including Perceptron, Regression, Support Vector Machine, Decision Tree and more, it's easy to use and easy to learn for beginners.\n* [Xcessiv](https://github.com/reiinakano/xcessiv) - A web-based application for quick, scalable, and automated hyperparameter tuning and stacked ensembling.\n* [PyTorch](https://github.com/pytorch/pytorch) - Tensors and Dynamic neural networks in Python with strong GPU acceleration\n* [ML-From-Scratch](https://github.com/eriklindernoren/ML-From-Scratch) - Implementations of Machine Learning models from scratch in Python with a focus on transparency. Aims to showcase the nuts and bolts of ML in an accessible way.\n* [Edward](http://edwardlib.org/) - A library for probabilistic modeling, inference, and criticism. Built on top of TensorFlow.\n* [xRBM](https://github.com/omimo/xRBM) - A library for Restricted Boltzmann Machine (RBM) and its conditional variants in Tensorflow.\n* [CatBoost](https://github.com/catboost/catboost) - General purpose gradient boosting on decision trees library with categorical features support out of the box. It is easy to install, well documented and supports CPU and GPU (even multi-GPU) computation.\n* [stacked_generalization](https://github.com/fukatani/stacked_generalization) - Implementation of machine learning stacking technic as handy library in Python.\n* [modAL](https://cosmic-cortex.github.io/modAL) - A modular active learning framework for Python, built on top of scikit-learn.\n* [Cogitare](https://github.com/cogitare-ai/cogitare): A Modern, Fast, and Modular Deep Learning and Machine Learning framework for Python. \n* [Parris](https://github.com/jgreenemi/Parris) - Parris, the automated infrastructure setup tool for machine learning algorithms.\n* [neonrvm](https://github.com/siavashserver/neonrvm) - neonrvm is an open source machine learning library based on RVM technique. It's written in C programming language and comes with Python programming language bindings.\n* [Turi Create](https://github.com/apple/turicreate)  - Machine learning from Apple. Turi Create simplifies the development of custom machine learning models. You don't have to be a machine learning expert to add recommendations, object detection, image classification, image similarity or activity classification to your app.\n* [xLearn](https://github.com/aksnzhy/xlearn) - A high performance, easy-to-use, and scalable machine learning package, which can be used to solve large-scale machine learning problems. xLearn is especially useful for solving machine learning problems on large-scale sparse data, which is very common in Internet services such as online advertisement and recommender systems.\n* [mlens](https://github.com/flennerhag/mlens) - A high performance, memory efficient, maximally parallelized ensemble learning, integrated with scikit-learn.\n* [Netron](https://github.com/lutzroeder/netron) - Visualizer for machine learning models.\n* [Thampi](https://github.com/scoremedia/thampi) - Machine Learning Prediction System on AWS Lambda\n\n\u003ca name=\"python-data-analysis\"\u003e\u003c/a\u003e\n#### Data Analysis / Data Visualization\n\n* [SciPy](http://www.scipy.org/) - A Python-based ecosystem of open-source software for mathematics, science, and engineering.\n* [NumPy](http://www.numpy.org/) - A fundamental package for scientific computing with Python.\n* [Numba](http://numba.pydata.org/) - Python JIT (just in time) compiler to LLVM aimed at scientific Python by the developers of Cython and NumPy.\n* [NetworkX](https://networkx.github.io/) - A high-productivity software for complex networks.\n* [igraph](http://igraph.org/python/) - binding to igraph library - General purpose graph library.\n* [Pandas](http://pandas.pydata.org/) - A library providing high-performance, easy-to-use data structures and data analysis tools.\n* [Open Mining](https://github.com/mining/mining) - Business Intelligence (BI) in Python (Pandas web interface)\n* [PyMC](https://github.com/pymc-devs/pymc) - Markov Chain Monte Carlo sampling toolkit.\n* [zipline](https://github.com/quantopian/zipline) - A Pythonic algorithmic trading library.\n* [PyDy](http://www.pydy.org/) - Short for Python Dynamics, used to assist with workflow in the modeling of dynamic motion based around NumPy, SciPy, IPython, and matplotlib.\n* [SymPy](https://github.com/sympy/sympy) - A Python library for symbolic mathematics.\n* [statsmodels](https://github.com/statsmodels/statsmodels) - Statistical modeling and econometrics in Python.\n* [astropy](http://www.astropy.org/) - A community Python library for Astronomy.\n* [matplotlib](http://matplotlib.org/) - A Python 2D plotting library.\n* [bokeh](https://github.com/bokeh/bokeh) - Interactive Web Plotting for Python.\n* [plotly](https://plot.ly/python/) - Collaborative web plotting for Python and matplotlib.\n* [vincent](https://github.com/wrobstory/vincent) - A Python to Vega translator.\n* [d3py](https://github.com/mikedewar/d3py) - A plotting library for Python, based on [D3.js](https://d3js.org/).\n* [PyDexter](https://github.com/D3xterjs/pydexter) - Simple plotting for Python. Wrapper for D3xterjs; easily render charts in-browser.\n* [ggplot](https://github.com/yhat/ggpy) - Same API as ggplot2 for R.\n* [ggfortify](https://github.com/sinhrks/ggfortify) - Unified interface to ggplot2 popular R packages.\n* [Kartograph.py](https://github.com/kartograph/kartograph.py) - Rendering beautiful SVG maps in Python.\n* [pygal](http://pygal.org/en/stable/) - A Python SVG Charts Creator.\n* [PyQtGraph](https://github.com/pyqtgraph/pyqtgraph) - A pure-python graphics and GUI library built on PyQt4 / PySide and NumPy.\n* [pycascading](https://github.com/twitter/pycascading)\n* [Petrel](https://github.com/AirSage/Petrel) - Tools for writing, submitting, debugging, and monitoring Storm topologies in pure Python.\n* [Blaze](https://github.com/blaze/blaze) - NumPy and Pandas interface to Big Data.\n* [emcee](https://github.com/dfm/emcee) - The Python ensemble sampling toolkit for affine-invariant MCMC.\n* [windML](http://www.windml.org) - A Python Framework for Wind Energy Analysis and Prediction.\n* [vispy](https://github.com/vispy/vispy) - GPU-based high-performance interactive OpenGL 2D/3D data visualization library.\n* [cerebro2](https://github.com/numenta/nupic.cerebro2) A web-based visualization and debugging platform for NuPIC.\n* [NuPIC Studio](https://github.com/htm-community/nupic.studio) An all-in-one NuPIC Hierarchical Temporal Memory visualization and debugging super-tool!\n* [SparklingPandas](https://github.com/sparklingpandas/sparklingpandas) Pandas on PySpark (POPS).\n* [Seaborn](http://seaborn.pydata.org/) - A python visualization library based on matplotlib.\n* [bqplot](https://github.com/bloomberg/bqplot) - An API for plotting in Jupyter (IPython).\n* [pastalog](https://github.com/rewonc/pastalog) - Simple, realtime visualization of neural network training performance.\n* [caravel](https://github.com/airbnb/superset) - A data exploration platform designed to be visual, intuitive, and interactive.\n* [Dora](https://github.com/nathanepstein/dora) - Tools for exploratory data analysis in Python.\n* [Ruffus](http://www.ruffus.org.uk) - Computation Pipeline library for python.\n* [SOMPY](https://github.com/sevamoo/SOMPY) - Self Organizing Map written in Python (Uses neural networks for data analysis).\n* [somoclu](https://github.com/peterwittek/somoclu) Massively parallel self-organizing maps: accelerate training on multicore CPUs, GPUs, and clusters, has python API.\n* [HDBScan](https://github.com/lmcinnes/hdbscan) - implementation of the hdbscan algorithm in Python - used for clustering\n* [visualize_ML](https://github.com/ayush1997/visualize_ML) - A python package for data exploration and data analysis.\n* [scikit-plot](https://github.com/reiinakano/scikit-plot) - A visualization library for quick and easy generation of common plots in data analysis and machine learning.\n* [Bowtie](https://github.com/jwkvam/bowtie) - A dashboard library for interactive visualizations using flask socketio and react.\n* [lime](https://github.com/marcotcr/lime) - Lime is about explaining what machine learning classifiers (or models) are doing. It is able to explain any black box classifier, with two or more classes.\n* [PyCM](https://github.com/sepandhaghighi/pycm) - PyCM is a multi-class confusion matrix library written in Python that supports both input data vectors and direct matrix, and a proper tool for post-classification model evaluation that supports most classes and overall statistics parameters\n* [Dash](https://github.com/plotly/dash) - A framework for creating analytical web applications built on top of Plotly.js, React, and Flask\n* [Lambdo](https://github.com/asavinov/lambdo) - A workflow engine for solving machine learning problems by combining in one analysis pipeline (i) feature engineering and machine learning (ii) model training and prediction (iii) table population and column evaluation via user-defined (Python) functions.\n\n\u003ca name=\"python-misc\"\u003e\u003c/a\u003e\n#### Misc Scripts / iPython Notebooks / Codebases\n* [Map/Reduce implementations of common ML algorithms](https://github.com/Yannael/BigDataAnalytics_INFOH515): Jupyter notebooks that cover how to implement from scratch different ML algorithms (ordinary least squares, gradient descent, k-means, alternating least squares), using Python NumPy, and how to then make these implementations scalable using Map/Reduce and Spark. \n* [BioPy](https://github.com/jaredthecoder/BioPy) - Biologically-Inspired and Machine Learning Algorithms in Python.\n* [SVM Explorer](https://github.com/plotly/dash-svm) - Interactive SVM Explorer, using Dash and scikit-learn\n* [pattern_classification](https://github.com/rasbt/pattern_classification)\n* [thinking stats 2](https://github.com/Wavelets/ThinkStats2)\n* [hyperopt](https://github.com/hyperopt/hyperopt-sklearn)\n* [numpic](https://github.com/numenta/nupic)\n* [2012-paper-diginorm](https://github.com/dib-lab/2012-paper-diginorm)\n* [A gallery of interesting IPython notebooks](https://github.com/ipython/ipython/wiki/A-gallery-of-interesting-IPython-Notebooks)\n* [ipython-notebooks](https://github.com/ogrisel/notebooks)\n* [data-science-ipython-notebooks](https://github.com/donnemartin/data-science-ipython-notebooks) - Continually updated Data Science Python Notebooks: Spark, Hadoop MapReduce, HDFS, AWS, Kaggle, scikit-learn, matplotlib, pandas, NumPy, SciPy, and various command lines.\n* [decision-weights](https://github.com/CamDavidsonPilon/decision-weights)\n* [Sarah Palin LDA](https://github.com/Wavelets/sarah-palin-lda) - Topic Modeling the Sarah Palin emails.\n* [Diffusion Segmentation](https://github.com/Wavelets/diffusion-segmentation) - A collection of image segmentation algorithms based on diffusion methods.\n* [Scipy Tutorials](https://github.com/Wavelets/scipy-tutorials) - SciPy tutorials. This is outdated, check out scipy-lecture-notes.\n* [Crab](https://github.com/marcelcaraciolo/crab) - A recommendation engine library for Python.\n* [BayesPy](https://github.com/maxsklar/BayesPy) - Bayesian Inference Tools in Python.\n* [scikit-learn tutorials](https://github.com/GaelVaroquaux/scikit-learn-tutorial) - Series of notebooks for learning scikit-learn.\n* [sentiment-analyzer](https://github.com/madhusudancs/sentiment-analyzer) - Tweets Sentiment Analyzer\n* [sentiment_classifier](https://github.com/kevincobain2000/sentiment_classifier) - Sentiment classifier using word sense disambiguation.\n* [group-lasso](https://github.com/fabianp/group_lasso) - Some experiments with the coordinate descent algorithm used in the (Sparse) Group Lasso model.\n* [jProcessing](https://github.com/kevincobain2000/jProcessing) - Kanji / Hiragana / Katakana to Romaji Converter. Edict Dictionary \u0026 parallel sentences Search. Sentence Similarity between two JP Sentences. Sentiment Analysis of Japanese Text. Run Cabocha(ISO--8859-1 configured) in Python.\n* [mne-python-notebooks](https://github.com/mne-tools/mne-python-notebooks) - IPython notebooks for EEG/MEG data processing using mne-python.\n* [Neon Course](https://github.com/NervanaSystems/neon_course) - IPython notebooks for a complete course around understanding Nervana's Neon.\n* [pandas cookbook](https://github.com/jvns/pandas-cookbook) - Recipes for using Python's pandas library.\n* [climin](https://github.com/BRML/climin) - Optimization library focused on machine learning, pythonic implementations of gradient descent, LBFGS, rmsprop, adadelta and others.\n* [Allen Downey’s Data Science Course](https://github.com/AllenDowney/DataScience) - Code for Data Science at Olin College, Spring 2014.\n* [Allen Downey’s Think Bayes Code](https://github.com/AllenDowney/ThinkBayes) - Code repository for Think Bayes.\n* [Allen Downey’s Think Complexity Code](https://github.com/AllenDowney/ThinkComplexity) - Code for Allen Downey's book Think Complexity.\n* [Allen Downey’s Think OS Code](https://github.com/AllenDowney/ThinkOS) - Text and supporting code for Think OS: A Brief Introduction to Operating Systems.\n* [Python Programming for the Humanities](http://www.karsdorp.io/python-course/) - Course for Python programming for the Humanities, assuming no prior knowledge. Heavy focus on text processing / NLP.\n* [GreatCircle](https://github.com/mwgg/GreatCircle) - Library for calculating great circle distance.\n* [Optunity examples](http://optunity.readthedocs.io/en/latest/notebooks/index.html) - Examples demonstrating how to use Optunity in synergy with machine learning libraries.\n* [Dive into Machine Learning  with Python Jupyter notebook and scikit-learn](https://github.com/hangtwenty/dive-into-machine-learning) - \"I learned Python by hacking first, and getting serious *later.* I wanted to do this with Machine Learning. If this is your style, join me in getting a bit ahead of yourself.\"\n* [TDB](https://github.com/ericjang/tdb) - TensorDebugger (TDB) is a visual debugger for deep learning. It features interactive, node-by-node debugging and visualization for TensorFlow.\n* [Suiron](https://github.com/kendricktan/suiron/) - Machine Learning for RC Cars.\n* [Introduction to machine learning with scikit-learn](https://github.com/justmarkham/scikit-learn-videos) - IPython notebooks from Data School's video tutorials on scikit-learn.\n* [Practical XGBoost in Python](http://education.parrotprediction.teachable.com/p/practical-xgboost-in-python) - comprehensive online course about using XGBoost in Python.\n* [Introduction to Machine Learning with Python](https://github.com/amueller/introduction_to_ml_with_python) - Notebooks and code for the book \"Introduction to Machine Learning with Python\"\n* [Pydata book](https://github.com/wesm/pydata-book) - Materials and IPython notebooks for \"Python for Data Analysis\" by Wes McKinney, published by O'Reilly Media\n\n\u003ca name=\"python-neural-networks\"\u003e\u003c/a\u003e\n#### Neural Networks\n* [NeuralTalk](https://github.com/karpathy/neuraltalk) - NeuralTalk is a Python+numpy project for learning Multimodal Recurrent Neural Networks that describe images with sentences.\n* [Neuron](https://github.com/molcik/python-neuron) - Neuron is simple class for time series predictions. It's utilize LNU (Linear Neural Unit), QNU (Quadratic Neural Unit), RBF (Radial Basis Function), MLP (Multi Layer Perceptron), MLP-ELM (Multi Layer Perceptron - Extreme Learning Machine) neural networks learned with Gradient descent or LeLevenberg–Marquardt algorithm.\n* [Data Driven Code](https://github.com/atmb4u/data-driven-code) - Very simple implementation of neural networks for dummies in python without using any libraries, with detailed comments.\n\n\u003ca name=\"python-kaggle\"\u003e\u003c/a\u003e\n#### Kaggle Competition Source Code\n* [open-solution-home-credit](https://github.com/neptune-ml/open-solution-home-credit) -\u003e source code and [experiments results](https://app.neptune.ml/neptune-ml/Home-Credit-Default-Risk) for [Home Credit Default Risk](https://www.kaggle.com/c/home-credit-default-risk).\n* [open-solution-googleai-object-detection](https://github.com/neptune-ml/open-solution-googleai-object-detection) -\u003e source code and [experiments results](https://app.neptune.ml/neptune-ml/Google-AI-Object-Detection-Challenge) for [Google AI Open Images - Object Detection Track](https://www.kaggle.com/c/google-ai-open-images-object-detection-track).\n* [open-solution-salt-identification](https://github.com/neptune-ml/open-solution-salt-identification) -\u003e source code and [experiments results](https://app.neptune.ml/neptune-ml/Salt-Detection) for [TGS Salt Identification Challenge](https://www.kaggle.com/c/tgs-salt-identification-challenge).\n* [open-solution-ship-detection](https://github.com/neptune-ml/open-solution-ship-detection) -\u003e source code and [experiments results](https://app.neptune.ml/neptune-ml/Ships) for [Airbus Ship Detection Challenge](https://www.kaggle.com/c/airbus-ship-detection).\n* [open-solution-data-science-bowl-2018](https://github.com/neptune-ml/open-solution-data-science-bowl-2018) -\u003e source code and [experiments results](https://app.neptune.ml/neptune-ml/Data-Science-Bowl-2018) for [2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018).\n* [open-solution-value-prediction](https://github.com/neptune-ml/open-solution-value-prediction) -\u003e source code and [experiments results](https://app.neptune.ml/neptune-ml/Santander-Value-Prediction-Challenge) for [Santander Value Prediction Challenge](https://www.kaggle.com/c/santander-value-prediction-challenge).\n* [open-solution-toxic-comments](https://github.com/neptune-ml/open-solution-toxic-comments) -\u003e source code for [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge).\n* [wiki challenge](https://github.com/hammer/wikichallenge) - An implementation of Dell Zhang's solution to Wikipedia's Participation Challenge on Kaggle.\n* [kaggle insults](https://github.com/amueller/kaggle_insults) - Kaggle Submission for \"Detecting Insults in Social Commentary\".\n* [kaggle_acquire-valued-shoppers-challenge](https://github.com/MLWave/kaggle_acquire-valued-shoppers-challenge) - Code for the Kaggle acquire valued shoppers challenge.\n* [kaggle-cifar](https://github.com/zygmuntz/kaggle-cifar) - Code for the CIFAR-10 competition at Kaggle, uses cuda-convnet.\n* [kaggle-blackbox](https://github.com/zygmuntz/kaggle-blackbox) - Deep learning made easy.\n* [kaggle-accelerometer](https://github.com/zygmuntz/kaggle-accelerometer) - Code for Accelerometer Biometric Competition at Kaggle.\n* [kaggle-advertised-salaries](https://github.com/zygmuntz/kaggle-advertised-salaries) - Predicting job salaries from ads - a Kaggle competition.\n* [kaggle amazon](https://github.com/zygmuntz/kaggle-amazon) - Amazon access control challenge.\n* [kaggle-bestbuy_big](https://github.com/zygmuntz/kaggle-bestbuy_big) - Code for the Best Buy competition at Kaggle.\n* [kaggle-bestbuy_small](https://github.com/zygmuntz/kaggle-bestbuy_small)\n* [Kaggle Dogs vs. Cats](https://github.com/kastnerkyle/kaggle-dogs-vs-cats) - Code for Kaggle Dogs vs. Cats competition.\n* [Kaggle Galaxy Challenge](https://github.com/benanne/kaggle-galaxies) - Winning solution for the Galaxy Challenge on Kaggle.\n* [Kaggle Gender](https://github.com/zygmuntz/kaggle-gender) - A Kaggle competition: discriminate gender based on handwriting.\n* [Kaggle Merck](https://github.com/zygmuntz/kaggle-merck) - Merck challenge at Kaggle.\n* [Kaggle Stackoverflow](https://github.com/zygmuntz/kaggle-stackoverflow) - Predicting closed questions on Stack Overflow.\n* [kaggle_acquire-valued-shoppers-challenge](https://github.com/MLWave/kaggle_acquire-valued-shoppers-challenge) - Code for the Kaggle acquire valued shoppers challenge.\n* [wine-quality](https://github.com/zygmuntz/wine-quality) - Predicting wine quality.\n\n\u003ca name=\"python-reinforcement-learning\"\u003e\u003c/a\u003e\n#### Reinforcement Learning\n* [DeepMind Lab](https://github.com/deepmind/lab) - DeepMind Lab is a 3D learning environment based on id Software's Quake III Arena via ioquake3 and other open source software. Its primary purpose is to act as a testbed for research in artificial intelligence, especially deep reinforcement learning.\n* [Gym](https://github.com/openai/gym) - OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms.\n* [Serpent.AI](https://github.com/SerpentAI/SerpentAI) - Serpent.AI is a game agent framework that allows you to turn any video game you own into a sandbox to develop AI and machine learning experiments. For both researchers and hobbyists. \n* [Universe](https://github.com/openai/universe) - Universe is a software platform for measuring and training an AI's general intelligence across the world's supply of games, websites and other applications.\n* [ViZDoom](https://github.com/mwydmuch/ViZDoom) - ViZDoom allows developing AI bots that play Doom using only the visual information (the screen buffer). It is primarily intended for research in machine visual learning, and deep reinforcement learning, in particular.\n* [Roboschool](https://github.com/openai/roboschool) - Open-source software for robot simulation, integrated with OpenAI Gym.\n* [Retro](https://github.com/openai/retro) - Retro Games in Gym\n* [SLM Lab](https://github.com/kengz/SLM-Lab) - Modular Deep Reinforcement Learning framework in PyTorch.\n* [Coach](https://github.com/NervanaSystems/coach) - Reinforcement Learning Coach by Intel® AI Lab enables easy experimentation with state of the art Reinforcement Learning algorithms\n\n\u003ca name=\"ruby\"\u003e\u003c/a\u003e\n## Ruby\n\n\u003ca name=\"ruby-nlp\"\u003e\u003c/a\u003e\n#### Natural Language Processing\n\n* [Awesome NLP with Ruby](https://github.com/arbox/nlp-with-ruby) - Curated link list for practical natural language processing in Ruby.\n* [Treat](https://github.com/louismullie/treat) -  Text REtrieval and Annotation Toolkit, definitely the most comprehensive toolkit I’ve encountered so far for Ruby.\n* [Ruby Linguistics](https://deveiate.org/projects/Linguistics) -  Linguistics is a framework for building linguistic utilities for Ruby objects in any language. It includes a generic language-independent front end, a module for mapping language codes into language names, and a module which contains various English-language utilities.\n* [Stemmer](https://github.com/aurelian/ruby-stemmer) - Expose libstemmer_c to Ruby.\n* [Ruby Wordnet](https://deveiate.org/projects/Ruby-WordNet/) - This library is a Ruby interface to WordNet.\n* [Raspel](https://sourceforge.net/projects/raspell/) - raspell is an interface binding for ruby.\n* [UEA Stemmer](https://github.com/ealdent/uea-stemmer) - Ruby port of UEALite Stemmer - a conservative stemmer for search and indexing.\n* [Twitter-text-rb](https://github.com/twitter/twitter-text-rb) - A library that does auto linking and extraction of usernames, lists and hashtags in tweets.\n\n\u003ca name=\"ruby-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n\n* [Awesome Machine Learning with Ruby](https://github.com/arbox/machine-learning-with-ruby) - Curated list of ML related resources for Ruby.\n* [Ruby Machine Learning](https://github.com/tsycho/ruby-machine-learning) - Some Machine Learning algorithms, implemented in Ruby.\n* [Machine Learning Ruby](https://github.com/mizoR/machine-learning-ruby)\n* [jRuby Mahout](https://github.com/vasinov/jruby_mahout) - JRuby Mahout is a gem that unleashes the power of Apache Mahout in the world of JRuby.\n* [CardMagic-Classifier](https://github.com/cardmagic/classifier) - A general classifier module to allow Bayesian and other types of classifications.\n* [rb-libsvm](https://github.com/febeling/rb-libsvm) - Ruby language bindings for LIBSVM which is a Library for Support Vector Machines.\n* [Random Forester](https://github.com/asafschers/random_forester) - Creates Random Forest classifiers from PMML files.\n\n\u003ca name=\"ruby-data-analysis\"\u003e\u003c/a\u003e\n#### Data Analysis / Data Visualization\n\n* [rsruby](https://github.com/alexgutteridge/rsruby) - Ruby - R bridge.\n* [data-visualization-ruby](https://github.com/chrislo/data_visualisation_ruby) - Source code and supporting content for my Ruby Manor presentation on Data Visualisation with Ruby.\n* [ruby-plot](https://www.ruby-toolbox.com/projects/ruby-plot) - gnuplot wrapper for Ruby, especially for plotting ROC curves into SVG files.\n* [plot-rb](https://github.com/zuhao/plotrb) - A plotting library in Ruby built on top of Vega and D3.\n* [scruffy](http://www.rubyinside.com/scruffy-a-beautiful-graphing-toolkit-for-ruby-194.html) - A beautiful graphing toolkit for Ruby.\n* [SciRuby](http://sciruby.com/)\n* [Glean](https://github.com/glean/glean) - A data management tool for humans.\n* [Bioruby](https://github.com/bioruby/bioruby)\n* [Arel](https://github.com/nkallen/arel)\n\n\u003ca name=\"ruby-misc\"\u003e\u003c/a\u003e\n#### Misc\n\n* [Big Data For Chimps](https://github.com/infochimps-labs/big_data_for_chimps)\n* [Listof](https://github.com/kevincobain2000/listof) - Community based data collection, packed in gem. Get list of pretty much anything (stop words, countries, non words) in txt, json or hash. [Demo/Search for a list](http://kevincobain2000.github.io/listof/)\n\n\n\u003ca name=\"rust\"\u003e\u003c/a\u003e\n## Rust\n\n\u003ca name=\"rust-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n* [deeplearn-rs](https://github.com/tedsta/deeplearn-rs) - deeplearn-rs provides simple networks that use matrix multiplication, addition, and ReLU under the MIT license.\n* [rustlearn](https://github.com/maciejkula/rustlearn) - a machine learning framework featuring logistic regression, support vector machines, decision trees and random forests.\n* [rusty-machine](https://github.com/AtheMathmo/rusty-machine) - a pure-rust machine learning library.\n* [leaf](https://github.com/autumnai/leaf) - open source framework for machine intelligence, sharing concepts from TensorFlow and Caffe.  Available under the MIT license. [**[Deprecated]**](https://medium.com/@mjhirn/tensorflow-wins-89b78b29aafb#.s0a3uy4cc)\n* [RustNN](https://github.com/jackm321/RustNN) - RustNN is a feedforward neural network library.\n* [RusticSOM](https://github.com/avinashshenoy97/RusticSOM) - A Rust library for Self Organising Maps (SOM).\n\n\n\u003ca name=\"r\"\u003e\u003c/a\u003e\n## R\n\n\u003ca name=\"r-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n\n* [ahaz](http://cran.r-project.org/web/packages/ahaz/index.html) - ahaz: Regularization for semiparametric additive hazards regression.\n* [arules](http://cran.r-project.org/web/packages/arules/index.html) - arules: Mining Association Rules and Frequent Itemsets\n* [biglasso](https://cran.r-project.org/web/packages/biglasso/index.html) - biglasso: Extending Lasso Model Fitting to Big Data in R.\n* [bigrf](http://cran.r-project.org/web/packages/bigrf/index.html) - bigrf: Big Random Forests: Classification and Regression Forests for Large Data Sets.\n* [bigRR](http://cran.r-project.org/web/packages/bigRR/index.html) - bigRR: Generalized Ridge Regression (with special advantage for p \u003e\u003e n cases).\n* [bmrm](http://cran.r-project.org/web/packages/bmrm/index.html) - bmrm: Bundle Methods for Regularized Risk Minimization Package.\n* [Boruta](http://cran.r-project.org/web/packages/Boruta/index.html) - Boruta: A wrapper algorithm for all-relevant feature selection.\n* [bst](http://cran.r-project.org/web/packages/bst/index.html) - bst: Gradient Boosting.\n* [C50](http://cran.r-project.org/web/packages/C50/index.html) - C50: C5.0 Decision Trees and Rule-Based Models.\n* [caret](http://caret.r-forge.r-project.org/) - Classification and Regression Training: Unified interface to ~150 ML algorithms in R.\n* [caretEnsemble](http://cran.r-project.org/web/packages/caretEnsemble/index.html) - caretEnsemble: Framework for fitting multiple caret models as well as creating ensembles of such models.\n* [CatBoost](https://github.com/catboost/catboost) - General purpose gradient boosting on decision trees library with categorical features support out of the box for R.\n* [Clever Algorithms For Machine Learning](https://github.com/jbrownlee/CleverAlgorithmsMachineLearning)\n* [CORElearn](http://cran.r-project.org/web/packages/CORElearn/index.html) - CORElearn: Classification, regression, feature evaluation and ordinal evaluation.\n* [CoxBoost](http://cran.r-project.org/web/packages/CoxBoost/index.html) - CoxBoost: Cox models by likelihood based boosting for a single survival endpoint or competing risks\n* [Cubist](http://cran.r-project.org/web/packages/Cubist/index.html) - Cubist: Rule- and Instance-Based Regression Modeling.\n* [e1071](http://cran.r-project.org/web/packages/e1071/index.html) - e1071: Misc Functions of the Department of Statistics (e1071), TU Wien\n* [earth](http://cran.r-project.org/web/packages/earth/index.html) - earth: Multivariate Adaptive Regression Spline Models\n* [elasticnet](http://cran.r-project.org/web/packages/elasticnet/index.html) - elasticnet: Elastic-Net for Sparse Estimation and Sparse PCA.\n* [ElemStatLearn](http://cran.r-project.org/web/packages/ElemStatLearn/index.html) - ElemStatLearn: Data sets, functions and examples from the book: \"The Elements of Statistical Learning, Data Mining, Inference, and Prediction\" by Trevor Hastie, Robert Tibshirani and Jerome Friedman Prediction\" by Trevor Hastie, Robert Tibshirani and Jerome Friedman.\n* [evtree](http://cran.r-project.org/web/packages/evtree/index.html) - evtree: Evolutionary Learning of Globally Optimal Trees.\n* [forecast](http://cran.r-project.org/web/packages/forecast/index.html) - forecast: Timeseries forecasting using ARIMA, ETS, STLM, TBATS, and neural network models.\n* [forecastHybrid](http://cran.r-project.org/web/packages/forecastHybrid/index.html) - forecastHybrid: Automatic ensemble and cross validation of ARIMA, ETS, STLM, TBATS, and neural network models from the \"forecast\" package.\n* [fpc](http://cran.r-project.org/web/packages/fpc/index.html) - fpc: Flexible procedures for clustering.\n* [frbs](http://cran.r-project.org/web/packages/frbs/index.html) - frbs: Fuzzy Rule-based Systems for Classification and Regression Tasks.\n* [GAMBoost](http://cran.r-project.org/web/packages/GAMBoost/index.html) - GAMBoost: Generalized linear and additive models by likelihood based boosting.\n* [gamboostLSS](http://cran.r-project.org/web/packages/gamboostLSS/index.html) - gamboostLSS: Boosting Methods for GAMLSS.\n* [gbm](http://cran.r-project.org/web/packages/gbm/index.html) - gbm: Generalized Boosted Regression Models.\n* [glmnet](http://cran.r-project.org/web/packages/glmnet/index.html) - glmnet: Lasso and elastic-net regularized generalized linear models.\n* [glmpath](http://cran.r-project.org/web/packages/glmpath/index.html) - glmpath: L1 Regularization Path for Generalized Linear Models and Cox Proportional Hazards Model.\n* [GMMBoost](http://cran.r-project.org/web/packages/GMMBoost/index.html) - GMMBoost: Likelihood-based Boosting for Generalized mixed models.\n* [grplasso](http://cran.r-project.org/web/packages/grplasso/index.html) - grplasso: Fitting user specified models with Group Lasso penalty.\n* [grpreg](http://cran.r-project.org/web/packages/grpreg/index.html) - grpreg: Regularization paths for regression models with grouped covariates.\n* [h2o](http://cran.r-project.org/web/packages/h2o/index.html) - A framework for fast, parallel, and distributed machine learning algorithms at scale -- Deeplearning, Random forests, GBM, KMeans, PCA, GLM.\n* [hda](http://cran.r-project.org/web/packages/hda/index.html) - hda: Heteroscedastic Discriminant Analysis.\n* [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)\n* [ipred](http://cran.r-project.org/web/packages/ipred/index.html) - ipred: Improved Predictors.\n* [kernlab](http://cran.r-project.org/web/packages/kernlab/index.html) - kernlab: Kernel-based Machine Learning Lab.\n* [klaR](http://cran.r-project.org/web/packages/klaR/index.html) - klaR: Classification and visualization.\n* [L0Learn](https://cran.r-project.org/web/packages/L0Learn/index.html) - L0Learn: Fast algorithms for best subset selection.\n* [lars](http://cran.r-project.org/web/packages/lars/index.html) - lars: Least Angle Regression, Lasso and Forward Stagewise.\n* [lasso2](http://cran.r-project.org/web/packages/lasso2/index.html) - lasso2: L1 constrained estimation aka ‘lasso’.\n* [LiblineaR](http://cran.r-project.org/web/packages/LiblineaR/index.html) - LiblineaR: Linear Predictive Models Based On The Liblinear C/C++ Library.\n* [LogicReg](http://cran.r-project.org/web/packages/LogicReg/index.html) - LogicReg: Logic Regression.\n* [Machine Learning For Hackers](https://github.com/johnmyleswhite/ML_for_Hackers)\n* [maptree](http://cran.r-project.org/web/packages/maptree/index.html) - maptree: Mapping, pruning, and graphing tree models.\n* [mboost](http://cran.r-project.org/web/packages/mboost/index.html) - mboost: Model-Based Boosting.\n* [medley](https://www.kaggle.com/forums/f/15/kaggle-forum/t/3661/medley-a-new-r-package-for-blending-regression-models?forumMessageId=21278) - medley: Blending regression models, using a greedy stepwise approach.\n* [mlr](http://cran.r-project.org/web/packages/mlr/index.html) - mlr: Machine Learning in R.\n* [mvpart](http://cran.r-project.org/web/packages/mvpart/index.html) - mvpart: Multivariate partitioning.\n* [ncvreg](http://cran.r-project.org/web/packages/ncvreg/index.html) - ncvreg: Regularization paths for SCAD- and MCP-penalized regression models.\n* [nnet](http://cran.r-project.org/web/packages/nnet/index.html) - nnet: Feed-forward Neural Networks and Multinomial Log-Linear Models.\n* [oblique.tree](http://cran.r-project.org/web/packages/oblique.tree/index.html) - oblique.tree: Oblique Trees for Classification Data.\n* [pamr](http://cran.r-project.org/web/packages/pamr/index.html) - pamr: Pam: prediction analysis for microarrays.\n* [party](http://cran.r-project.org/web/packages/party/index.html) - party: A Laboratory for Recursive Partytioning.\n* [partykit](http://cran.r-project.org/web/packages/partykit/index.html) - partykit: A Toolkit for Recursive Partytioning.\n* [penalized](http://cran.r-project.org/web/packages/penalized/index.html) - penalized: L1 (lasso and fused lasso) and L2 (ridge) penalized estimation in GLMs and in the Cox model.\n* [penalizedLDA](http://cran.r-project.org/web/packages/penalizedLDA/index.html) - penalizedLDA: Penalized classification using Fisher's linear discriminant.\n* [penalizedSVM](http://cran.r-project.org/web/packages/penalizedSVM/index.html) - penalizedSVM: Feature Selection SVM using penalty functions.\n* [quantregForest](http://cran.r-project.org/web/packages/quantregForest/index.html) - quantregForest: Quantile Regression Forests.\n* [randomForest](http://cran.r-project.org/web/packages/randomForest/index.html) - randomForest: Breiman and Cutler's random forests for classification and regression.\n* [randomForestSRC](http://cran.r-project.org/web/packages/randomForestSRC/index.html) - randomForestSRC: Random Forests for Survival, Regression and Classification (RF-SRC).\n* [rattle](http://cran.r-project.org/web/packages/rattle/index.html) - rattle: Graphical user interface for data mining in R.\n* [rda](http://cran.r-project.org/web/packages/rda/index.html) - rda: Shrunken Centroids Regularized Discriminant Analysis.\n* [rdetools](http://cran.r-project.org/web/packages/rdetools/index.html) - rdetools: Relevant Dimension Estimation (RDE) in Feature Spaces.\n* [REEMtree](http://cran.r-project.org/web/packages/REEMtree/index.html) - REEMtree: Regression Trees with Random Effects for Longitudinal (Panel) Data.\n* [relaxo](http://cran.r-project.org/web/packages/relaxo/index.html) - relaxo: Relaxed Lasso.\n* [rgenoud](http://cran.r-project.org/web/packages/rgenoud/index.html) - rgenoud: R version of GENetic Optimization Using Derivatives\n* [rgp](http://cran.r-project.org/web/packages/rgp/index.html) - rgp: R genetic programming framework.\n* [Rmalschains](http://cran.r-project.org/web/packages/Rmalschains/index.html) - Rmalschains: Continuous Optimization using Memetic Algorithms with Local Search Chains (MA-LS-Chains) in R.\n* [rminer](http://cran.r-project.org/web/packages/rminer/index.html) - rminer: Simpler use of data mining methods (e.g. NN and SVM) in classification and regression.\n* [ROCR](http://cran.r-project.org/web/packages/ROCR/index.html) - ROCR: Visualizing the performance of scoring classifiers.\n* [RoughSets](http://cran.r-project.org/web/packages/RoughSets/index.html) - RoughSets: Data Analysis Using Rough Set and Fuzzy Rough Set Theories.\n* [rpart](http://cran.r-project.org/web/packages/rpart/index.html) - rpart: Recursive Partitioning and Regression Trees.\n* [RPMM](http://cran.r-project.org/web/packages/RPMM/index.html) - RPMM: Recursively Partitioned Mixture Model.\n* [RSNNS](http://cran.r-project.org/web/packages/RSNNS/index.html) - RSNNS: Neural Networks in R using the Stuttgart Neural Network Simulator (SNNS).\n* [RWeka](http://cran.r-project.org/web/packages/RWeka/index.html) - RWeka: R/Weka interface.\n* [RXshrink](http://cran.r-project.org/web/packages/RXshrink/index.html) - RXshrink: Maximum Likelihood Shrinkage via Generalized Ridge or Least Angle Regression.\n* [sda](http://cran.r-project.org/web/packages/sda/index.html) - sda: Shrinkage Discriminant Analysis and CAT Score Variable Selection.\n* [SDDA](http://cran.r-project.org/web/packages/SDDA/index.html) - SDDA: Stepwise Diagonal Discriminant Analysis.\n* [SuperLearner](https://github.com/ecpolley/SuperLearner) and [subsemble](http://cran.r-project.org/web/packages/subsemble/index.html) - Multi-algorithm ensemble learning packages.\n* [svmpath](http://cran.r-project.org/web/packages/svmpath/index.html) - svmpath: svmpath: the SVM Path algorithm.\n* [tgp](http://cran.r-project.org/web/packages/tgp/index.html) - tgp: Bayesian treed Gaussian process models.\n* [tree](http://cran.r-project.org/web/packages/tree/index.html) - tree: Classification and regression trees.\n* [varSelRF](http://cran.r-project.org/web/packages/varSelRF/index.html) - varSelRF: Variable selection using random forests.\n* [XGBoost.R](https://github.com/tqchen/xgboost/tree/master/R-package) - R binding for eXtreme Gradient Boosting (Tree) Library.\n* [Optunity](http://optunity.readthedocs.io/en/latest/) - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search. Optunity is written in Python but interfaces seamlessly to R.\n* [igraph](http://igraph.org/r/) - binding to igraph library - General purpose graph library.\n* [MXNet](https://github.com/dmlc/mxnet) - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.\n* [TDSP-Utilities](https://github.com/Azure/Azure-TDSP-Utilities) - Two data science utilities in R from Microsoft: 1) Interactive Data Exploration, Analysis, and Reporting (IDEAR) ; 2) Automated Modeling and Reporting (AMR).\n\n\u003ca name=\"r-data-analysis\"\u003e\u003c/a\u003e\n#### Data Analysis / Data Visualization\n\n* [ggplot2](http://ggplot2.org/) - A data visualization package based on the grammar of graphics.\n* [tmap](https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html) for visualizing geospatial data with static maps and [leaflet](https://rstudio.github.io/leaflet/) for interactive maps\n* [tm](https://www.rdocumentation.org/packages/tm/) and [quanteda](https://quanteda.io/) are the main packages for managing,  analyzing, and visualizing textual data.\n* [shiny](https://shiny.rstudio.com/) is the basis for truly interactive  displays and dashboards in R. However, some measure of interactivity can be achieved with [htmlwidgets](https://www.htmlwidgets.org/) bringing javascript libraries to R. These include, [plotly](https://plot.ly/r/), [dygraphs](http://rstudio.github.io/dygraphs),  [highcharter](http://jkunst.com/highcharter/), and several others.\n\n\u003ca name=\"sas\"\u003e\u003c/a\u003e\n## SAS\n\n\u003ca name=\"sas-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n\n* [Visual Data Mining and Machine Learning](https://www.sas.com/vdmml) - Interactive, automated, and programmatic modeling with the latest machine learning algorithms in and end-to-end analytics environment, from data prep to deployment. Free trial available.\n* [Enterprise Miner](https://www.sas.com/en_us/software/enterprise-miner.html) - Data mining and machine learning that creates deployable models using a GUI or code.\n* [Factory Miner](https://www.sas.com/en_us/software/factory-miner.html) - Automatically creates deployable machine learning models across numerous market or customer segments using a GUI.\n\n\u003ca name=\"sas-data-analysis\"\u003e\u003c/a\u003e\n#### Data Analysis / Data Visualization\n\n* [SAS/STAT](https://www.sas.com/en_us/software/analytics/stat.html) - For conducting advanced statistical analysis.\n* [University Edition](https://www.sas.com/en_us/software/university-edition.html) - FREE! Includes all SAS packages necessary for data analysis and visualization, and includes online SAS courses.\n\n\u003ca name=\"sas-mpp\"\u003e\u003c/a\u003e\n#### High Performance Machine Learning\n\n* [High Performance Data Mining](https://www.sas.com/en_us/software/analytics/high-performance-data-mining.html) - Data mining and machine learning that creates deployable models using a GUI or code in an MPP environment, including Hadoop.\n* [High Performance Text Mining](https://www.sas.com/en_us/software/analytics/high-performance-text-mining.html) - Text mining using a GUI or code in an MPP environment, including Hadoop.\n\n\u003ca name=\"sas-nlp\"\u003e\u003c/a\u003e\n#### Natural Language Processing\n\n* [Contextual Analysis](https://www.sas.com/en_us/software/analytics/contextual-analysis.html) - Add structure to unstructured text using a GUI.\n* [Sentiment Analysis](https://www.sas.com/en_us/software/analytics/sentiment-analysis.html) - Extract sentiment from text using a GUI.\n* [Text Miner](https://www.sas.com/en_us/software/analytics/text-miner.html) - Text mining using a GUI or code.\n\n\u003ca name=\"sas-demos\"\u003e\u003c/a\u003e\n#### Demos and Scripts\n\n* [ML_Tables](https://github.com/sassoftware/enlighten-apply/tree/master/ML_tables) - Concise cheat sheets containing machine learning best practices.\n* [enlighten-apply](https://github.com/sassoftware/enlighten-apply) - Example code and materials that illustrate applications of SAS machine learning techniques.\n* [enlighten-integration](https://github.com/sassoftware/enlighten-integration) - Example code and materials that illustrate techniques for integrating SAS with other analytics technologies in Java, PMML, Python and R.\n* [enlighten-deep](https://github.com/sassoftware/enlighten-deep) - Example code and materials that illustrate using neural networks with several hidden layers in SAS.\n* [dm-flow](https://github.com/sassoftware/dm-flow) - Library of SAS Enterprise Miner process flow diagrams to help you learn by example about specific data mining topics.\n\n\n\u003ca name=\"scala\"\u003e\u003c/a\u003e\n## Scala\n\n\u003ca name=\"scala-nlp\"\u003e\u003c/a\u003e\n#### Natural Language Processing\n\n* [ScalaNLP](http://www.scalanlp.org/) - ScalaNLP is a suite of machine learning and numerical computing libraries.\n* [Breeze](https://github.com/scalanlp/breeze) - Breeze is a numerical processing library for Scala.\n* [Chalk](https://github.com/scalanlp/chalk) - Chalk is a natural language processing library.\n* [FACTORIE](https://github.com/factorie/factorie) - FACTORIE is a toolkit for deployable probabilistic modeling, implemented as a software library in Scala. It provides its users with a succinct language for creating relational factor graphs, estimating parameters and performing inference.\n* [Montague](https://github.com/Workday/upshot-montague) - Montague is a semantic parsing library for Scala with an easy-to-use DSL.\n\n\u003ca name=\"scala-data-analysis\"\u003e\u003c/a\u003e\n#### Data Analysis / Data Visualization\n\n* [MLlib in Apache Spark](http://spark.apache.org/docs/latest/mllib-guide.html) - Distributed machine learning library in Spark\n* [Hydrosphere Mist](https://github.com/Hydrospheredata/mist) - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.\n* [Scalding](https://github.com/twitter/scalding) - A Scala API for Cascading.\n* [Summing Bird](https://github.com/twitter/summingbird) - Streaming MapReduce with Scalding and Storm.\n* [Algebird](https://github.com/twitter/algebird) - Abstract Algebra for Scala.\n* [xerial](https://github.com/xerial/xerial) - Data management utilities for Scala.\n* [PredictionIO](https://github.com/apache/incubator-predictionio) - PredictionIO, a machine learning server for software developers and data engineers.\n* [BIDMat](https://github.com/BIDData/BIDMat) - CPU and GPU-accelerated matrix library intended to support large-scale exploratory data analysis.\n* [Flink](http://flink.apache.org/) - Open source platform for distributed stream and batch data processing.\n* [Spark Notebook](http://spark-notebook.io) - Interactive and Reactive Data Science using Scala and Spark.\n\n\u003ca name=\"scala-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n\n* [DeepLearning.scala](http://deeplearning.thoughtworks.school/) - Creating statically typed dynamic neural networks from object-oriented \u0026 functional programming constructs.\n* [Conjecture](https://github.com/etsy/Conjecture) - Scalable Machine Learning in Scalding.\n* [brushfire](https://github.com/stripe/brushfire) - Distributed decision tree ensemble learning in Scala.\n* [ganitha](https://github.com/tresata/ganitha) - Scalding powered machine learning.\n* [adam](https://github.com/bigdatagenomics/adam) - A genomics processing engine and specialized file format built using Apache Avro, Apache Spark and Parquet. Apache 2 licensed.\n* [bioscala](https://github.com/bioscala/bioscala) - Bioinformatics for the Scala programming language\n* [BIDMach](https://github.com/BIDData/BIDMach) - CPU and GPU-accelerated Machine Learning Library.\n* [Figaro](https://github.com/p2t2/figaro) - a Scala library for constructing probabilistic models.\n* [H2O Sparkling Water](https://github.com/h2oai/sparkling-water) - H2O and Spark interoperability.\n* [FlinkML in Apache Flink](https://ci.apache.org/projects/flink/flink-docs-master/apis/batch/libs/ml/index.html) - Distributed machine learning library in Flink.\n* [DynaML](https://github.com/transcendent-ai-labs/DynaML) - Scala Library/REPL for Machine Learning Research.\n* [Saul](https://github.com/IllinoisCogComp/saul/) - Flexible Declarative Learning-Based Programming.\n* [SwiftLearner](https://github.com/valdanylchuk/swiftlearner/) - Simply written algorithms to help study ML or write your own implementations.\n* [Smile](http://haifengl.github.io/smile/) - Statistical Machine Intelligence and Learning Engine.\n* [doddle-model](https://github.com/picnicml/doddle-model) - An in-memory machine learning library built on top of Breeze. It provides immutable objects and exposes its functionality through a scikit-learn-like API.\n\n\u003ca name=\"swift\"\u003e\u003c/a\u003e\n## Swift\n\n\u003ca name=\"swift-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n\n* [Bender](https://github.com/xmartlabs/Bender) - Fast Neural Networks framework built on top of Metal. Supports TensorFlow models.\n* [Swift AI](https://github.com/collinhundley/Swift-AI) - Highly optimized artificial intelligence and machine learning library written in Swift.\n* [BrainCore](https://github.com/aleph7/BrainCore) - The iOS and OS X neural network framework.\n* [swix](https://github.com/stsievert/swix) - A bare bones library that\n  includes a general matrix language and wraps some OpenCV for iOS development.\n* [DeepLearningKit](http://deeplearningkit.org/) an Open Source Deep Learning Framework for Apple’s iOS, OS X and tvOS.\n  It currently allows using deep convolutional neural network models trained in Caffe on Apple operating systems.\n* [AIToolbox](https://github.com/KevinCoble/AIToolbox) - A toolbox framework of AI modules written in Swift:  Graphs/Trees, Linear Regression, Support Vector Machines, Neural Networks, PCA, KMeans, Genetic Algorithms, MDP, Mixture of Gaussians.\n* [MLKit](https://github.com/Somnibyte/MLKit) - A simple Machine Learning Framework written in Swift. Currently features Simple Linear Regression, Polynomial Regression, and Ridge Regression.\n* [Swift Brain](https://github.com/vlall/Swift-Brain) - The first neural network / machine learning library written in Swift. This is a project for AI algorithms in Swift for iOS and OS X development. This project includes algorithms focused on Bayes theorem, neural networks, SVMs, Matrices, etc...\n* [Perfect TensorFlow](https://github.com/PerfectlySoft/Perfect-TensorFlow) - Swift Language Bindings of TensorFlow. Using native TensorFlow models on both macOS / Linux.\n* [PredictionBuilder](https://github.com/denissimon/prediction-builder-swift) - A library for machine learning that builds predictions using a linear regression.\n* [Awesome CoreML](https://github.com/NilStack/awesome-CoreML-models) - A curated list of pretrained CoreML models.\n* [Awesome Core ML Models](https://github.com/likedan/Awesome-CoreML-Models) - A curated list of machine learning models in CoreML format.\n\n\u003ca name=\"tensor\"\u003e\u003c/a\u003e\n## TensorFlow\n\n\u003ca name=\"tensor-general-purpose\"\u003e\u003c/a\u003e\n#### General-Purpose Machine Learning\n* [Awesome TensorFlow](https://github.com/jtoy/awesome-tensorflow) - A list of all things related to TensorFlow.\n* [Golden TensorFlow](https://golden.com/wiki/TensorFlow) - A page of content on TensorFlow, including academic papers and links to related topics.\n\n\u003ca name=\"tools\"\u003e\u003c/a\u003e\n## Tools\n\n\u003ca name=\"tools-misc\"\u003e\u003c/a\u003e\n#### Misc\n* [Notebooks](https://github.com/rlan/notebooks) - A starter kit for Jupyter notebooks and machine learning. Companion docker images consist of all combinations of python versions, machine learning frameworks (Keras, PyTorch and Tensorflow) and CPU/CUDA versions.\n* [DVC](https://github.com/iterative/dvc) - Data Science Version Control is an open-source version control system for machine learning projects with pipelines support. It makes ML projects reproducible and shareable.\n\n\u003ca name=\"credits\"\u003e\u003c/a\u003e\n## Credits\n\n* Some of the python libraries were cut-and-pasted from [vinta](https://github.com/vinta/awesome-python)\n* The few go reference I found where pulled from [this page](https://github.com/golang/go/wiki/Projects)\n"
  },
  {
    "repo": "keras-team/keras",
    "content": "﻿# Keras: Deep Learning for humans\n\n![Keras logo](https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png)\n\n[![Build Status](https://travis-ci.org/keras-team/keras.svg?branch=master)](https://travis-ci.org/keras-team/keras)\n[![license](https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000)](https://github.com/keras-team/keras/blob/master/LICENSE)\n\n## You have just found Keras.\n\nKeras is a high-level neural networks API, written in Python and capable of running on top of [TensorFlow](https://github.com/tensorflow/tensorflow), [CNTK](https://github.com/Microsoft/cntk), or [Theano](https://github.com/Theano/Theano). It was developed with a focus on enabling fast experimentation. *Being able to go from idea to result with the least possible delay is key to doing good research.*\n\nUse Keras if you need a deep learning library that:\n\n- Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n- Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n- Runs seamlessly on CPU and GPU.\n\nRead the documentation at [Keras.io](https://keras.io).\n\nKeras is compatible with: __Python 2.7-3.6__.\n\n\n------------------\n\n\n## Guiding principles\n\n- __User friendliness.__ Keras is an API designed for human beings, not machines. It puts user experience front and center. Keras follows best practices for reducing cognitive load: it offers consistent \u0026 simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear and actionable feedback upon user error.\n\n- __Modularity.__ A model is understood as a sequence or a graph of standalone, fully-configurable modules that can be plugged together with as few restrictions as possible. In particular, neural layers, cost functions, optimizers, initialization schemes, activation functions, regularization schemes are all standalone modules that you can combine to create new models.\n\n- __Easy extensibility.__ New modules are simple to add (as new classes and functions), and existing modules provide ample examples. To be able to easily create new modules allows for total expressiveness, making Keras suitable for advanced research.\n\n- __Work with Python__. No separate models configuration files in a declarative format. Models are described in Python code, which is compact, easier to debug, and allows for ease of extensibility.\n\n\n------------------\n\n\n## Getting started: 30 seconds to Keras\n\nThe core data structure of Keras is a __model__, a way to organize layers. The simplest type of model is the [`Sequential`](https://keras.io/getting-started/sequential-model-guide) model, a linear stack of layers. For more complex architectures, you should use the [Keras functional API](https://keras.io/getting-started/functional-api-guide), which allows to build arbitrary graphs of layers.\n\nHere is the `Sequential` model:\n\n```python\nfrom keras.models import Sequential\n\nmodel = Sequential()\n```\n\nStacking layers is as easy as `.add()`:\n\n```python\nfrom keras.layers import Dense\n\nmodel.add(Dense(units=64, activation='relu', input_dim=100))\nmodel.add(Dense(units=10, activation='softmax'))\n```\n\nOnce your model looks good, configure its learning process with `.compile()`:\n\n```python\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='sgd',\n              metrics=['accuracy'])\n```\n\nIf you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code).\n```python\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True))\n```\n\nYou can now iterate on your training data in batches:\n\n```python\n# x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\nmodel.fit(x_train, y_train, epochs=5, batch_size=32)\n```\n\nAlternatively, you can feed batches to your model manually:\n\n```python\nmodel.train_on_batch(x_batch, y_batch)\n```\n\nEvaluate your performance in one line:\n\n```python\nloss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n```\n\nOr generate predictions on new data:\n\n```python\nclasses = model.predict(x_test, batch_size=128)\n```\n\nBuilding a question answering system, an image classification model, a Neural Turing Machine, or any other model is just as fast. The ideas behind deep learning are simple, so why should their implementation be painful?\n\nFor a more in-depth tutorial about Keras, you can check out:\n\n- [Getting started with the Sequential model](https://keras.io/getting-started/sequential-model-guide)\n- [Getting started with the functional API](https://keras.io/getting-started/functional-api-guide)\n\nIn the [examples folder](https://github.com/keras-team/keras/tree/master/examples) of the repository, you will find more advanced models: question-answering with memory networks, text generation with stacked LSTMs, etc.\n\n\n------------------\n\n\n## Installation\n\nBefore installing Keras, please install one of its backend engines: TensorFlow, Theano, or CNTK. We recommend the TensorFlow backend.\n\n- [TensorFlow installation instructions](https://www.tensorflow.org/install/).\n- [Theano installation instructions](http://deeplearning.net/software/theano/install.html#install).\n- [CNTK installation instructions](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-cntk-on-your-machine).\n\nYou may also consider installing the following **optional dependencies**:\n\n- [cuDNN](https://docs.nvidia.com/deeplearning/sdk/cudnn-install/) (recommended if you plan on running Keras on GPU).\n- HDF5 and [h5py](http://docs.h5py.org/en/latest/build.html) (required if you plan on saving Keras models to disk).\n- [graphviz](https://graphviz.gitlab.io/download/) and [pydot](https://github.com/erocarrera/pydot) (used by [visualization utilities](https://keras.io/visualization/) to plot model graphs).\n\nThen, you can install Keras itself. There are two ways to install Keras:\n\n- **Install Keras from PyPI (recommended):**\n\n```sh\nsudo pip install keras\n```\n\nIf you are using a virtualenv, you may want to avoid using sudo:\n\n```sh\npip install keras\n```\n\n- **Alternatively: install Keras from the GitHub source:**\n\nFirst, clone Keras using `git`:\n\n```sh\ngit clone https://github.com/keras-team/keras.git\n```\n\n Then, `cd` to the Keras folder and run the install command:\n```sh\ncd keras\nsudo python setup.py install\n```\n\n------------------\n\n\n## Configuring your Keras backend\n\nBy default, Keras will use TensorFlow as its tensor manipulation library. [Follow these instructions](https://keras.io/backend/) to configure the Keras backend.\n\n------------------\n\n\n## Support\n\nYou can ask questions and join the development discussion:\n\n- On the [Keras Google group](https://groups.google.com/forum/#!forum/keras-users).\n- On the [Keras Slack channel](https://kerasteam.slack.com). Use [this link](https://keras-slack-autojoin.herokuapp.com/) to request an invitation to the channel.\n\nYou can also post **bug reports and feature requests** (only) in [GitHub issues](https://github.com/keras-team/keras/issues). Make sure to read [our guidelines](https://github.com/keras-team/keras/blob/master/CONTRIBUTING.md) first.\n\n\n------------------\n\n\n## Why this name, Keras?\n\nKeras (κέρας) means _horn_ in Greek. It is a reference to a literary image from ancient Greek and Latin literature, first found in the _Odyssey_, where dream spirits (_Oneiroi_, singular _Oneiros_) are divided between those who deceive men with false visions, who arrive to Earth through a gate of ivory, and those who announce a future that will come to pass, who arrive through a gate of horn. It's a play on the words κέρας (horn) / κραίνω (fulfill), and ἐλέφας (ivory) / ἐλεφαίρομαι (deceive).\n\nKeras was initially developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).\n\n\u003e_\"Oneiroi are beyond our unravelling --who can be sure what tale they tell? Not all that men look for comes to pass. Two gates there are that give passage to fleeting Oneiroi; one is made of horn, one of ivory. The Oneiroi that pass through sawn ivory are deceitful, bearing a message that will not be fulfilled; those that come out through polished horn have truth behind them, to be accomplished for men who see them.\"_ Homer, Odyssey 19. 562 ff (Shewring translation).\n\n------------------\n"
  },
  {
    "repo": "requests/requests",
    "content": "Requests: HTTP for Humans™\n==========================\n\n[![image](https://img.shields.io/pypi/v/requests.svg)](https://pypi.org/project/requests/)\n[![image](https://img.shields.io/pypi/l/requests.svg)](https://pypi.org/project/requests/)\n[![image](https://img.shields.io/pypi/pyversions/requests.svg)](https://pypi.org/project/requests/)\n[![codecov.io](https://codecov.io/github/requests/requests/coverage.svg?branch=master)](https://codecov.io/github/requests/requests)\n[![image](https://img.shields.io/github/contributors/requests/requests.svg)](https://github.com/requests/requests/graphs/contributors)\n[![image](https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg)](https://saythanks.io/to/kennethreitz)\n\n**If you're interested in financially supporting Kenneth Reitz open source, consider [visiting this link](https://cash.me/$KennethReitz). Your support helps tremendously with sustainability of motivation, as Open Source is no longer part of my day job.**\n\nRequests is the only *Non-GMO* HTTP library for Python, safe for human\nconsumption.\n\n![image](https://farm5.staticflickr.com/4317/35198386374_1939af3de6_k_d.jpg)\n\nBehold, the power of Requests:\n\n``` {.sourceCode .python}\n\u003e\u003e\u003e r = requests.get('https://api.github.com/user', auth=('user', 'pass'))\n\u003e\u003e\u003e r.status_code\n200\n\u003e\u003e\u003e r.headers['content-type']\n'application/json; charset=utf8'\n\u003e\u003e\u003e r.encoding\n'utf-8'\n\u003e\u003e\u003e r.text\nu'{\"type\":\"User\"...'\n\u003e\u003e\u003e r.json()\n{u'disk_usage': 368627, u'private_gists': 484, ...}\n```\n\nSee [the similar code, sans Requests](https://gist.github.com/973705).\n\n[![image](https://raw.githubusercontent.com/requests/requests/master/docs/_static/requests-logo-small.png)](http://docs.python-requests.org/)\n\nRequests allows you to send *organic, grass-fed* HTTP/1.1 requests,\nwithout the need for manual labor. There's no need to manually add query\nstrings to your URLs, or to form-encode your POST data. Keep-alive and\nHTTP connection pooling are 100% automatic, thanks to\n[urllib3](https://github.com/shazow/urllib3).\n\nBesides, all the cool kids are doing it. Requests is one of the most\ndownloaded Python packages of all time, pulling in over 11,000,000\ndownloads every month. You don't want to be left out!\n\nFeature Support\n---------------\n\nRequests is ready for today's web.\n\n-   International Domains and URLs\n-   Keep-Alive \u0026 Connection Pooling\n-   Sessions with Cookie Persistence\n-   Browser-style SSL Verification\n-   Basic/Digest Authentication\n-   Elegant Key/Value Cookies\n-   Automatic Decompression\n-   Automatic Content Decoding\n-   Unicode Response Bodies\n-   Multipart File Uploads\n-   HTTP(S) Proxy Support\n-   Connection Timeouts\n-   Streaming Downloads\n-   `.netrc` Support\n-   Chunked Requests\n\nRequests officially supports Python 2.7 \u0026 3.4–3.7, and runs great on\nPyPy.\n\nInstallation\n------------\n\nTo install Requests, simply use [pipenv](http://pipenv.org/) (or pip, of\ncourse):\n\n``` {.sourceCode .bash}\n$ pipenv install requests\n✨🍰✨\n```\n\nSatisfaction guaranteed.\n\nDocumentation\n-------------\n\nFantastic documentation is available at\n\u003chttp://docs.python-requests.org/\u003e, for a limited time only.\n\nHow to Contribute\n-----------------\n\n1.  Check for open issues or open a fresh issue to start a discussion\n    around a feature idea or a bug. There is a [Contributor\n    Friendly](https://github.com/requests/requests/issues?direction=desc\u0026labels=Contributor+Friendly\u0026page=1\u0026sort=updated\u0026state=open)\n    tag for issues that should be ideal for people who are not very\n    familiar with the codebase yet.\n2.  Fork [the repository](https://github.com/requests/requests) on\n    GitHub to start making your changes to the **master** branch (or\n    branch off of it).\n3.  Write a test which shows that the bug was fixed or that the feature\n    works as expected.\n4.  Send a pull request and bug the maintainer until it gets merged and\n    published. :) Make sure to add yourself to\n    [AUTHORS](https://github.com/requests/requests/blob/master/AUTHORS.rst).\n\n"
  },
  {
    "repo": "ansible/ansible",
    "content": "|PyPI version| |Docs badge| |Chat badge| |Build Status| |Code Of Conduct| |Mailing Lists| |License|\n\n*******\nAnsible\n*******\n\nAnsible is a radically simple IT automation system. It handles\nconfiguration-management, application deployment, cloud provisioning,\nad-hoc task-execution, and multinode orchestration -- including\ntrivializing things like zero-downtime rolling updates with load\nbalancers.\n\nRead the documentation and more at https://ansible.com/\n\nYou can find installation instructions\n`here \u003chttps://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html\u003e`_ for a\nvariety of platforms.\n\nMost users should probably install a released version of Ansible from ``pip``, a package manager or\nour `release repository \u003chttps://releases.ansible.com/ansible/\u003e`_. `Officially supported\n\u003chttps://www.ansible.com/ansible-engine\u003e`_ builds of Ansible are also available. Some power users\nrun directly from the development branch - while significant efforts are made to ensure that\n``devel`` is reasonably stable, you're more likely to encounter breaking changes when running\nAnsible this way.\n\nDesign Principles\n=================\n\n*  Have a dead simple setup process and a minimal learning curve.\n*  Manage machines very quickly and in parallel.\n*  Avoid custom-agents and additional open ports, be agentless by\n   leveraging the existing SSH daemon.\n*  Describe infrastructure in a language that is both machine and human\n   friendly.\n*  Focus on security and easy auditability/review/rewriting of content.\n*  Manage new remote machines instantly, without bootstrapping any\n   software.\n*  Allow module development in any dynamic language, not just Python.\n*  Be usable as non-root.\n*  Be the easiest IT automation system to use, ever.\n\nGet Involved\n============\n\n*  Read `Community\n   Information \u003chttps://docs.ansible.com/ansible/latest/community\u003e`_ for all\n   kinds of ways to contribute to and interact with the project,\n   including mailing list information and how to submit bug reports and\n   code to Ansible.\n*  All code submissions are done through pull requests to the ``devel`` branch.\n*  Feel free to talk to us before making larger changes\n   to avoid duplicate efforts. This not only helps everyone\n   know what's going on, it also helps save time and effort if we decide\n   some changes are needed.\n*  Users list:\n   `ansible-project \u003chttps://groups.google.com/group/ansible-project\u003e`_\n*  Development list:\n   `ansible-devel \u003chttps://groups.google.com/group/ansible-devel\u003e`_\n*  Announcement list:\n   `ansible-announce \u003chttps://groups.google.com/group/ansible-announce\u003e`_\n   -- read only\n*  irc.freenode.net: #ansible\n*  For the full list of Email Lists, IRC channels see the\n   `Communication page \u003chttps://docs.ansible.com/ansible/latest/community/communication.html\u003e`_\n\nBranch Info\n===========\n\n*  Releases are named after Led Zeppelin songs. (Releases prior to 2.0\n   were named after Van Halen songs.)\n*  The ``devel`` branch corresponds to the release actively under\n   development.\n*  The ``stable-2.x`` branches exist for current releases.\n*  Various release-X.Y branches exist for previous releases.\n*  For information about the active branches see the\n   `Ansible release and maintenance \u003chttps://docs.ansible.com/ansible/latest/reference_appendices/release_and_maintenance.html\u003e`_ page.\n*  We'd love to have your contributions, read the `Community\n   Guide \u003chttps://docs.ansible.com/ansible/latest/community\u003e`_ for notes on\n   how to get started.\n\nRoadmap\n=======\n\nBased on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.0, 2.1).\nSubminor versions will generally not have roadmaps published.\n\nThe `Ansible Roadmap page \u003chttps://docs.ansible.com/ansible/devel/roadmap/\u003e`_ details what is planned and how to influence the roadmap.\n\nAuthors\n=======\n\nAnsible was created by `Michael DeHaan \u003chttps://github.com/mpdehaan\u003e`_\n(michael.dehaan/gmail/com) and has contributions from over 3700 users\n(and growing). Thanks everyone!\n\n`Ansible \u003chttps://www.ansible.com\u003e`_ is sponsored by `Red Hat, Inc.\n\u003chttps://www.redhat.com\u003e`_\n\nLicense\n=======\n\nGNU General Public License v3.0\n\nSee `COPYING \u003cCOPYING\u003e`_ to see the full text.\n\n.. |PyPI version| image:: https://img.shields.io/pypi/v/ansible.svg\n   :target: https://pypi.org/project/ansible\n.. |Docs badge| image:: https://img.shields.io/badge/docs-latest-brightgreen.svg\n   :target: https://docs.ansible.com/ansible/latest/\n.. |Build Status| image:: https://api.shippable.com/projects/573f79d02a8192902e20e34b/badge?branch=devel\n   :target: https://app.shippable.com/projects/573f79d02a8192902e20e34b\n.. |Chat badge| image:: https://img.shields.io/badge/chat-IRC-brightgreen.svg\n   :target: https://docs.ansible.com/ansible/latest/community/communication.html\n.. |Code Of Conduct| image:: https://img.shields.io/badge/code%20of%20conduct-Ansible-silver.svg\n   :target: https://docs.ansible.com/ansible/latest/community/code_of_conduct.html\n   :alt: Ansible Code of Conduct\n.. |Mailing Lists| image:: https://img.shields.io/badge/mailing%20lists-Ansible-orange.svg\n   :target: https://docs.ansible.com/ansible/latest/community/communication.html#mailing-list-information\n   :alt: Ansible mailing lists\n.. |License| image:: https://img.shields.io/badge/license-GPL%20v3.0-brightgreen.svg\n   :target: COPYING\n   :alt: Repository License\n"
  },
  {
    "repo": "scikit-learn/scikit-learn",
    "content": ".. -*- mode: rst -*-\n\n|Travis|_ |AppVeyor|_ |Codecov|_ |CircleCI|_ |Python35|_ |PyPi|_ |DOI|_\n\n.. |Travis| image:: https://api.travis-ci.org/scikit-learn/scikit-learn.svg?branch=master\n.. _Travis: https://travis-ci.org/scikit-learn/scikit-learn\n\n.. |AppVeyor| image:: https://ci.appveyor.com/api/projects/status/github/scikit-learn/scikit-learn?branch=master\u0026svg=true\n.. _AppVeyor: https://ci.appveyor.com/project/sklearn-ci/scikit-learn/history\n\n.. |Codecov| image:: https://codecov.io/github/scikit-learn/scikit-learn/badge.svg?branch=master\u0026service=github\n.. _Codecov: https://codecov.io/github/scikit-learn/scikit-learn?branch=master\n\n.. |CircleCI| image:: https://circleci.com/gh/scikit-learn/scikit-learn/tree/master.svg?style=shield\u0026circle-token=:circle-token\n.. _CircleCI: https://circleci.com/gh/scikit-learn/scikit-learn\n\n.. |Python35| image:: https://img.shields.io/badge/python-3.5-blue.svg\n.. _Python35: https://badge.fury.io/py/scikit-learn\n\n.. |PyPi| image:: https://badge.fury.io/py/scikit-learn.svg\n.. _PyPi: https://badge.fury.io/py/scikit-learn\n\n.. |DOI| image:: https://zenodo.org/badge/21369/scikit-learn/scikit-learn.svg\n.. _DOI: https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn\n\nscikit-learn\n============\n\nscikit-learn is a Python module for machine learning built on top of\nSciPy and distributed under the 3-Clause BSD license.\n\nThe project was started in 2007 by David Cournapeau as a Google Summer\nof Code project, and since then many volunteers have contributed. See\nthe `About us \u003chttp://scikit-learn.org/dev/about.html#authors\u003e`_ page\nfor a list of core contributors.\n\nIt is currently maintained by a team of volunteers.\n\nWebsite: http://scikit-learn.org\n\n\nInstallation\n------------\n\nDependencies\n~~~~~~~~~~~~\n\nscikit-learn requires:\n\n- Python (\u003e= 3.5)\n- NumPy (\u003e= 1.11.0)\n- SciPy (\u003e= 0.17.0)\n\n**Scikit-learn 0.20 was the last version to support Python2.7.**\nScikit-learn 0.21 and later require Python 3.5 or newer.\n\nFor running the examples Matplotlib \u003e= 1.5.1 is required. A few examples\nrequire scikit-image \u003e= 0.12.3, a few examples require pandas \u003e= 0.18.0\nand a few example require joblib \u003e= 0.11.\n\nscikit-learn also uses CBLAS, the C interface to the Basic Linear Algebra\nSubprograms library. scikit-learn comes with a reference implementation, but\nthe system CBLAS will be detected by the build system and used if present.\nCBLAS exists in many implementations; see `Linear algebra libraries\n\u003chttp://scikit-learn.org/stable/modules/computing#linear-algebra-libraries\u003e`_\nfor known issues.\n\nUser installation\n~~~~~~~~~~~~~~~~~\n\nIf you already have a working installation of numpy and scipy,\nthe easiest way to install scikit-learn is using ``pip`` ::\n\n    pip install -U scikit-learn\n\nor ``conda``::\n\n    conda install scikit-learn\n\nThe documentation includes more detailed `installation instructions \u003chttp://scikit-learn.org/stable/install.html\u003e`_.\n\n\nChangelog\n---------\n\nSee the `changelog \u003chttp://scikit-learn.org/dev/whats_new.html\u003e`__\nfor a history of notable changes to scikit-learn.\n\nDevelopment\n-----------\n\nWe welcome new contributors of all experience levels. The scikit-learn\ncommunity goals are to be helpful, welcoming, and effective. The\n`Development Guide \u003chttp://scikit-learn.org/stable/developers/index.html\u003e`_\nhas detailed information about contributing code, documentation, tests, and\nmore. We've included some basic information in this README.\n\nImportant links\n~~~~~~~~~~~~~~~\n\n- Official source code repo: https://github.com/scikit-learn/scikit-learn\n- Download releases: https://pypi.org/project/scikit-learn/\n- Issue tracker: https://github.com/scikit-learn/scikit-learn/issues\n\nSource code\n~~~~~~~~~~~\n\nYou can check the latest sources with the command::\n\n    git clone https://github.com/scikit-learn/scikit-learn.git\n\nSetting up a development environment\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nQuick tutorial on how to go about setting up your environment to\ncontribute to scikit-learn: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md\n\nTesting\n~~~~~~~\n\nAfter installation, you can launch the test suite from outside the\nsource directory (you will need to have ``pytest`` \u003e= 3.3.0 installed)::\n\n    pytest sklearn\n\nSee the web page http://scikit-learn.org/dev/developers/advanced_installation.html#testing\nfor more information.\n\n    Random number generation can be controlled during testing by setting\n    the ``SKLEARN_SEED`` environment variable.\n\nSubmitting a Pull Request\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBefore opening a Pull Request, have a look at the\nfull Contributing page to make sure your code complies\nwith our guidelines: http://scikit-learn.org/stable/developers/index.html\n\n\nProject History\n---------------\n\nThe project was started in 2007 by David Cournapeau as a Google Summer\nof Code project, and since then many volunteers have contributed. See\nthe  `About us \u003chttp://scikit-learn.org/dev/about.html#authors\u003e`_ page\nfor a list of core contributors.\n\nThe project is currently maintained by a team of volunteers.\n\n**Note**: `scikit-learn` was previously referred to as `scikits.learn`.\n\n\nHelp and Support\n----------------\n\nDocumentation\n~~~~~~~~~~~~~\n\n- HTML documentation (stable release): http://scikit-learn.org\n- HTML documentation (development version): http://scikit-learn.org/dev/\n- FAQ: http://scikit-learn.org/stable/faq.html\n\nCommunication\n~~~~~~~~~~~~~\n\n- Mailing list: https://mail.python.org/mailman/listinfo/scikit-learn\n- IRC channel: ``#scikit-learn`` at ``webchat.freenode.net``\n- Stack Overflow: https://stackoverflow.com/questions/tagged/scikit-learn\n- Website: http://scikit-learn.org\n\nCitation\n~~~~~~~~\n\nIf you use scikit-learn in a scientific publication, we would appreciate citations: http://scikit-learn.org/stable/about.html#citing-scikit-learn\n"
  },
  {
    "repo": "minimaxir/big-list-of-naughty-strings",
    "content": "# Big List of Naughty Strings\nThe Big List of Naughty Strings is an evolving list of strings which have a high probability of causing issues when used as user-input data. This is intended for use in helping both automated and manual QA testing; useful for whenever your QA engineer [walks into a bar](http://www.sempf.net/post/On-Testing1).\n\n## Why Test Naughty Strings?\n\nEven multi-billion dollar companies with huge amounts of automated testing can't find every bad input. For example, look at what happens when you try to Tweet a [zero-width space](https://en.wikipedia.org/wiki/Zero-width_space) (U+200B) on Twitter:\n\n![](http://i.imgur.com/HyDg2eV.gif)\n\nAlthough this is not a malicious error, and typical users aren't Tweeting weird unicode, an \"internal server error\" for unexpected input is never a positive experience for the user, and may in fact be a symptom of deeper string-validation issues. The Big List of Naughty Strings is intended to help reveal such issues.\n\n## Usage\n\n`blns.txt` consists of newline-delimited strings and comments which are preceded with `#`. The comments divide the strings into sections for easy manual reading and copy/pasting into input forms. For those who want to access the strings programmatically, a `blns.json` file is provided containing an array with all the comments stripped out (the `scripts` folder contains a Python script used to generate the `blns.json`).\n\n## Contributions\n\nFeel free to send a pull request to add more strings, or additional sections. However, please do not send pull requests with very-long strings (255+ characters), as that makes the list much more difficult to view.\n\nLikewise, please do not send pull requests which compromise *manual usability of the file*. This includes the [EICAR test string](https://en.wikipedia.org/wiki/EICAR_test_file), which can cause the file to be flagged by antivirus scanners, and files which alter the encoding of `blns.txt`. Also, do not send a null character (U+0000) string, as it [changes the file format on GitHub to binary](http://stackoverflow.com/a/19723302) and renders it unreadable in pull requests. Finally, when adding or removing a string please update all files when you perform a pull request.\n\n## Disclaimer\n\nThe Big List of Naughty Strings is intended to be used *for software you own and manage*. Some of the Naughty Strings can indicate security vulnerabilities, and as a result using such strings with third-party software may be a crime. The maintainer is not responsible for any negative actions that result from the use of the list.\n\nAdditionally, the Big List of Naughty Strings is not a fully-comprehensive substitute for formal security/penetration testing for your service.\n\n## Maintainer/Creator\n\nMax Woolf ([@minimaxir](https://twitter.com/minimaxir))\n\n## Social Media Discussions\n\n* June 10, 2015 [Hacker News]: [Show HN: Big List of Naughty Strings for testing user-input data](https://news.ycombinator.com/item?id=10035008)\n* August 17, 2015 [Reddit]: [Big list of naughty strings.](https://www.reddit.com/r/programming/comments/3hdxqx/big_list_of_naughty_strings/)\n* February 9, 2016 [Reddit]: [Big List of Naughty Strings](https://www.reddit.com/r/webdev/comments/44wc5b/big_list_of_naughty_strings/)\n* January 15, 2017 [Hacker News]: [Naughty Strings: A list of strings likely to cause issues as user-input data](https://news.ycombinator.com/item?id=13406119)\n* January 16, 2017 [Reddit]: [Naughty Strings: A list of strings likely to cause issues as user-input data](https://www.reddit.com/r/programming/comments/5o9inb/naughty_strings_a_list_of_strings_likely_to_cause/)\n\n## License\n\nMIT\n"
  },
  {
    "repo": "scrapy/scrapy",
    "content": "======\nScrapy\n======\n\n.. image:: https://img.shields.io/pypi/v/Scrapy.svg\n   :target: https://pypi.python.org/pypi/Scrapy\n   :alt: PyPI Version\n\n.. image:: https://img.shields.io/pypi/pyversions/Scrapy.svg\n   :target: https://pypi.python.org/pypi/Scrapy\n   :alt: Supported Python Versions\n\n.. image:: https://img.shields.io/travis/scrapy/scrapy/master.svg\n   :target: https://travis-ci.org/scrapy/scrapy\n   :alt: Build Status\n\n.. image:: https://img.shields.io/badge/wheel-yes-brightgreen.svg\n   :target: https://pypi.python.org/pypi/Scrapy\n   :alt: Wheel Status\n\n.. image:: https://img.shields.io/codecov/c/github/scrapy/scrapy/master.svg\n   :target: https://codecov.io/github/scrapy/scrapy?branch=master\n   :alt: Coverage report\n\n.. image:: https://anaconda.org/conda-forge/scrapy/badges/version.svg\n   :target: https://anaconda.org/conda-forge/scrapy\n   :alt: Conda Version\n\n\nOverview\n========\n\nScrapy is a fast high-level web crawling and web scraping framework, used to\ncrawl websites and extract structured data from their pages. It can be used for\na wide range of purposes, from data mining to monitoring and automated testing.\n\nFor more information including a list of features check the Scrapy homepage at:\nhttps://scrapy.org\n\nRequirements\n============\n\n* Python 2.7 or Python 3.4+\n* Works on Linux, Windows, Mac OSX, BSD\n\nInstall\n=======\n\nThe quick way::\n\n    pip install scrapy\n\nFor more details see the install section in the documentation:\nhttps://doc.scrapy.org/en/latest/intro/install.html\n\nDocumentation\n=============\n\nDocumentation is available online at https://doc.scrapy.org/ and in the ``docs``\ndirectory.\n\nReleases\n========\n\nYou can find release notes at https://doc.scrapy.org/en/latest/news.html\n\nCommunity (blog, twitter, mail list, IRC)\n=========================================\n\nSee https://scrapy.org/community/\n\nContributing\n============\n\nSee https://doc.scrapy.org/en/master/contributing.html\n\nCode of Conduct\n---------------\n\nPlease note that this project is released with a Contributor Code of Conduct\n(see https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md).\n\nBy participating in this project you agree to abide by its terms.\nPlease report unacceptable behavior to opensource@scrapinghub.com.\n\nCompanies using Scrapy\n======================\n\nSee https://scrapy.org/companies/\n\nCommercial Support\n==================\n\nSee https://scrapy.org/support/\n"
  },
  {
    "repo": "shadowsocks/shadowsocks",
    "content": "Removed according to regulations.\n"
  },
  {
    "repo": "XX-net/XX-Net",
    "content": "# XX-Net\n\n###### [中文文档](https://github.com/XX-net/XX-Net/wiki/%E4%B8%AD%E6%96%87%E6%96%87%E6%A1%A3) \u0026nbsp; \u0026nbsp; \u0026nbsp;[English](https://github.com/XX-net/XX-Net/wiki/English-Home-Page) \u0026nbsp; \u0026nbsp; \u0026nbsp;[فارسی صفحه اصلی](https://github.com/XX-net/XX-Net/wiki/Persian-home-page) \n\n* 易用的翻墙工具  \n* 包含的GAE_proxy和X-Tunnel：  \n\n\n| 模块        | GAE_proxy   | X-Tunnel  |  \n| ------------- |:-------------:| :-----:| \n| 联通性| 依赖IPv6 | 更多通道 |\n| 速度 | 流畅 | 下载快速，稍微延迟 | \n| 安全性| Google可看到通信内容 | 支持完整https加密 |  \n| 易用 | 需开启Ipv6，部署服务端，导入证书 | 简单  |\n| 兼容性| 部分网站不支持 | 无问题 |\n| 收费  | 免费 | 付费 |  \n\n\u003cbr\u003e\n\n### [__下载页面__](https://github.com/XX-net/XX-Net/blob/master/code/default/download.md)\n\u003cbr\u003e\n\n\n### 最新状态：\n 2018-06-23\n* Google 更新服务器证书，请更新到3.12.2 以上。\n\n* 最近反馈国产安全软件/QQ会影响使用，遇到问题请试试关闭安全软件或QQ\n  \n\n* GAE_Proxy 请开启IPv6，参考:  \n  [如何开启IPv6](https://github.com/XX-net/XX-Net/wiki/%E5%A6%82%E4%BD%95%E5%BC%80%E5%90%AFIPv6)\n\n    \n  \n\u003cbr\u003e\n\n#### 提示：  \n* 有问题请先看[Wiki文档](https://github.com/XX-net/XX-Net/wiki/%E4%B8%AD%E6%96%87%E6%96%87%E6%A1%A3)\n* [提问](https://github.com/XX-net/XX-Net/issues)前，请先看[最近讨论主题](https://github.com/XX-net/XX-Net/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc)，避免重复发问。  \n"
  },
  {
    "repo": "certbot/certbot",
    "content": ".. This file contains a series of comments that are used to include sections of this README in other files. Do not modify these comments unless you know what you are doing. tag:intro-begin\n\nCertbot is part of EFF’s effort to encrypt the entire Internet. Secure communication over the Web relies on HTTPS, which requires the use of a digital certificate that lets browsers verify the identity of web servers (e.g., is that really google.com?). Web servers obtain their certificates from trusted third parties called certificate authorities (CAs). Certbot is an easy-to-use client that fetches a certificate from Let’s Encrypt—an open certificate authority launched by the EFF, Mozilla, and others—and deploys it to a web server.\n\nAnyone who has gone through the trouble of setting up a secure website knows what a hassle getting and maintaining a certificate is. Certbot and Let’s Encrypt can automate away the pain and let you turn on and manage HTTPS with simple commands. Using Certbot and Let's Encrypt is free, so there’s no need to arrange payment.\n\nHow you use Certbot depends on the configuration of your web server. The best way to get started is to use our `interactive guide \u003chttps://certbot.eff.org\u003e`_. It generates instructions based on your configuration settings. In most cases, you’ll need `root or administrator access \u003chttps://certbot.eff.org/faq/#does-certbot-require-root-administrator-privileges\u003e`_ to your web server to run Certbot.\n\nCertbot is meant to be run directly on your web server, not on your personal computer. If you’re using a hosted service and don’t have direct access to your web server, you might not be able to use Certbot. Check with your hosting provider for documentation about uploading certificates or using certificates issued by Let’s Encrypt.\n\nCertbot is a fully-featured, extensible client for the Let's\nEncrypt CA (or any other CA that speaks the `ACME\n\u003chttps://github.com/ietf-wg-acme/acme/blob/master/draft-ietf-acme-acme.md\u003e`_\nprotocol) that can automate the tasks of obtaining certificates and\nconfiguring webservers to use them. This client runs on Unix-based operating\nsystems.\n\nTo see the changes made to Certbot between versions please refer to our\n`changelog \u003chttps://github.com/certbot/certbot/blob/master/CHANGELOG.md\u003e`_.\n\nUntil May 2016, Certbot was named simply ``letsencrypt`` or ``letsencrypt-auto``,\ndepending on install method. Instructions on the Internet, and some pieces of the\nsoftware, may still refer to this older name.\n\nContributing\n------------\n\nIf you'd like to contribute to this project please read `Developer Guide\n\u003chttps://certbot.eff.org/docs/contributing.html\u003e`_.\n\n.. _installation:\n\nInstallation\n------------\n\nThe easiest way to install Certbot is by visiting `certbot.eff.org`_, where you can\nfind the correct installation instructions for many web server and OS combinations.\nFor more information, see `Get Certbot \u003chttps://certbot.eff.org/docs/install.html\u003e`_.\n\n.. _certbot.eff.org: https://certbot.eff.org/\n\nHow to run the client\n---------------------\n\nIn many cases, you can just run ``certbot-auto`` or ``certbot``, and the\nclient will guide you through the process of obtaining and installing certs\ninteractively.\n\nFor full command line help, you can type::\n\n  ./certbot-auto --help all\n\n\nYou can also tell it exactly what you want it to do from the command line.\nFor instance, if you want to obtain a cert for ``example.com``,\n``www.example.com``, and ``other.example.net``, using the Apache plugin to both\nobtain and install the certs, you could do this::\n\n  ./certbot-auto --apache -d example.com -d www.example.com -d other.example.net\n\n(The first time you run the command, it will make an account, and ask for an\nemail and agreement to the Let's Encrypt Subscriber Agreement; you can\nautomate those with ``--email`` and ``--agree-tos``)\n\nIf you want to use a webserver that doesn't have full plugin support yet, you\ncan still use \"standalone\" or \"webroot\" plugins to obtain a certificate::\n\n  ./certbot-auto certonly --standalone --email admin@example.com -d example.com -d www.example.com -d other.example.net\n\n\nUnderstanding the client in more depth\n--------------------------------------\n\nTo understand what the client is doing in detail, it's important to\nunderstand the way it uses plugins.  Please see the `explanation of\nplugins \u003chttps://certbot.eff.org/docs/using.html#plugins\u003e`_ in\nthe User Guide.\n\nLinks\n=====\n\n.. Do not modify this comment unless you know what you're doing. tag:links-begin\n\nDocumentation: https://certbot.eff.org/docs\n\nSoftware project: https://github.com/certbot/certbot\n\nNotes for developers: https://certbot.eff.org/docs/contributing.html\n\nMain Website: https://certbot.eff.org\n\nLet's Encrypt Website: https://letsencrypt.org\n\nCommunity: https://community.letsencrypt.org\n\nACME spec: http://ietf-wg-acme.github.io/acme/\n\nACME working area in github: https://github.com/ietf-wg-acme/acme\n\n|build-status| |coverage| |docs| |container|\n\n.. |build-status| image:: https://travis-ci.org/certbot/certbot.svg?branch=master\n   :target: https://travis-ci.org/certbot/certbot\n   :alt: Travis CI status\n\n.. |coverage| image:: https://codecov.io/gh/certbot/certbot/branch/master/graph/badge.svg\n   :target: https://codecov.io/gh/certbot/certbot\n   :alt: Coverage status\n\n.. |docs| image:: https://readthedocs.org/projects/letsencrypt/badge/\n   :target: https://readthedocs.org/projects/letsencrypt/\n   :alt: Documentation status\n\n.. |container| image:: https://quay.io/repository/letsencrypt/letsencrypt/status\n   :target: https://quay.io/repository/letsencrypt/letsencrypt\n   :alt: Docker Repository on Quay.io\n\n.. Do not modify this comment unless you know what you're doing. tag:links-end\n\nSystem Requirements\n===================\n\nSee https://certbot.eff.org/docs/install.html#system-requirements.\n\n.. Do not modify this comment unless you know what you're doing. tag:intro-end\n\n.. Do not modify this comment unless you know what you're doing. tag:features-begin\n\nCurrent Features\n=====================\n\n* Supports multiple web servers:\n\n  - apache/2.x\n  - nginx/0.8.48+\n  - webroot (adds files to webroot directories in order to prove control of\n    domains and obtain certs)\n  - standalone (runs its own simple webserver to prove you control a domain)\n  - other server software via `third party plugins \u003chttps://certbot.eff.org/docs/using.html#third-party-plugins\u003e`_\n\n* The private key is generated locally on your system.\n* Can talk to the Let's Encrypt CA or optionally to other ACME\n  compliant services.\n* Can get domain-validated (DV) certificates.\n* Can revoke certificates.\n* Adjustable RSA key bit-length (2048 (default), 4096, ...).\n* Can optionally install a http -\u003e https redirect, so your site effectively\n  runs https only (Apache only)\n* Fully automated.\n* Configuration changes are logged and can be reverted.\n* Supports an interactive text UI, or can be driven entirely from the\n  command line.\n* Free and Open Source Software, made with Python.\n\n.. Do not modify this comment unless you know what you're doing. tag:features-end\n\nFor extensive documentation on using and contributing to Certbot, go to https://certbot.eff.org/docs. If you would like to contribute to the project or run the latest code from git, you should read our `developer guide \u003chttps://certbot.eff.org/docs/contributing.html\u003e`_.\n"
  },
  {
    "repo": "python/cpython",
    "content": "This is Python version 3.8.0 alpha 0\n====================================\n\n.. image:: https://travis-ci.org/python/cpython.svg?branch=master\n   :alt: CPython build status on Travis CI\n   :target: https://travis-ci.org/python/cpython\n\n.. image:: https://ci.appveyor.com/api/projects/status/4mew1a93xdkbf5ua/branch/master?svg=true\n   :alt: CPython build status on Appveyor\n   :target: https://ci.appveyor.com/project/python/cpython/branch/master\n\n.. image:: https://dev.azure.com/python/cpython/_apis/build/status/Azure%20Pipelines%20CI?branchName=master\n   :alt: CPython build status on Azure DevOps\n   :target: https://dev.azure.com/python/cpython/_build/latest?definitionId=4\u0026branchName=master\n\n.. image:: https://codecov.io/gh/python/cpython/branch/master/graph/badge.svg\n   :alt: CPython code coverage on Codecov\n   :target: https://codecov.io/gh/python/cpython\n\n.. image:: https://img.shields.io/badge/zulip-join_chat-brightgreen.svg\n   :alt: Python Zulip chat\n   :target: https://python.zulipchat.com\n\n\nCopyright (c) 2001-2018 Python Software Foundation.  All rights reserved.\n\nSee the end of this file for further copyright and license information.\n\n.. contents::\n\nGeneral Information\n-------------------\n\n- Website: https://www.python.org\n- Source code: https://github.com/python/cpython\n- Issue tracker: https://bugs.python.org\n- Documentation: https://docs.python.org\n- Developer's Guide: https://devguide.python.org/\n\nContributing to CPython\n-----------------------\n\nFor more complete instructions on contributing to CPython development,\nsee the `Developer Guide`_.\n\n.. _Developer Guide: https://devguide.python.org/\n\nUsing Python\n------------\n\nInstallable Python kits, and information about using Python, are available at\n`python.org`_.\n\n.. _python.org: https://www.python.org/\n\nBuild Instructions\n------------------\n\nOn Unix, Linux, BSD, macOS, and Cygwin::\n\n    ./configure\n    make\n    make test\n    sudo make install\n\nThis will install Python as python3.\n\nYou can pass many options to the configure script; run ``./configure --help``\nto find out more.  On macOS and Cygwin, the executable is called ``python.exe``;\nelsewhere it's just ``python``.\n\nIf you are running on macOS with the latest updates installed, make sure to install\nopenSSL or some other SSL software along with Homebrew or another package manager.\nIf issues persist, see https://devguide.python.org/setup/#macos-and-os-x for more \ninformation. \n\nOn macOS, if you have configured Python with ``--enable-framework``, you\nshould use ``make frameworkinstall`` to do the installation.  Note that this\ninstalls the Python executable in a place that is not normally on your PATH,\nyou may want to set up a symlink in ``/usr/local/bin``.\n\nOn Windows, see `PCbuild/readme.txt\n\u003chttps://github.com/python/cpython/blob/master/PCbuild/readme.txt\u003e`_.\n\nIf you wish, you can create a subdirectory and invoke configure from there.\nFor example::\n\n    mkdir debug\n    cd debug\n    ../configure --with-pydebug\n    make\n    make test\n\n(This will fail if you *also* built at the top-level directory.  You should do\na ``make clean`` at the toplevel first.)\n\nTo get an optimized build of Python, ``configure --enable-optimizations``\nbefore you run ``make``.  This sets the default make targets up to enable\nProfile Guided Optimization (PGO) and may be used to auto-enable Link Time\nOptimization (LTO) on some platforms.  For more details, see the sections\nbelow.\n\n\nProfile Guided Optimization\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nPGO takes advantage of recent versions of the GCC or Clang compilers.  If used,\neither via ``configure --enable-optimizations`` or by manually running\n``make profile-opt`` regardless of configure flags, the optimized build\nprocess will perform the following steps:\n\nThe entire Python directory is cleaned of temporary files that may have\nresulted from a previous compilation.\n\nAn instrumented version of the interpreter is built, using suitable compiler\nflags for each flavour. Note that this is just an intermediary step.  The\nbinary resulting from this step is not good for real life workloads as it has\nprofiling instructions embedded inside.\n\nAfter the instrumented interpreter is built, the Makefile will run a training\nworkload.  This is necessary in order to profile the interpreter execution.\nNote also that any output, both stdout and stderr, that may appear at this step\nis suppressed.\n\nThe final step is to build the actual interpreter, using the information\ncollected from the instrumented one.  The end result will be a Python binary\nthat is optimized; suitable for distribution or production installation.\n\n\nLink Time Optimization\n^^^^^^^^^^^^^^^^^^^^^^\n\nEnabled via configure's ``--with-lto`` flag.  LTO takes advantage of the\nability of recent compiler toolchains to optimize across the otherwise\narbitrary ``.o`` file boundary when building final executables or shared\nlibraries for additional performance gains.\n\n\nWhat's New\n----------\n\nWe have a comprehensive overview of the changes in the `What's New in Python\n3.8 \u003chttps://docs.python.org/3.8/whatsnew/3.8.html\u003e`_ document.  For a more\ndetailed change log, read `Misc/NEWS\n\u003chttps://github.com/python/cpython/blob/master/Misc/NEWS.d\u003e`_, but a full\naccounting of changes can only be gleaned from the `commit history\n\u003chttps://github.com/python/cpython/commits/master\u003e`_.\n\nIf you want to install multiple versions of Python see the section below\nentitled \"Installing multiple versions\".\n\n\nDocumentation\n-------------\n\n`Documentation for Python 3.8 \u003chttps://docs.python.org/3.8/\u003e`_ is online,\nupdated daily.\n\nIt can also be downloaded in many formats for faster access.  The documentation\nis downloadable in HTML, PDF, and reStructuredText formats; the latter version\nis primarily for documentation authors, translators, and people with special\nformatting requirements.\n\nFor information about building Python's documentation, refer to `Doc/README.rst\n\u003chttps://github.com/python/cpython/blob/master/Doc/README.rst\u003e`_.\n\n\nConverting From Python 2.x to 3.x\n---------------------------------\n\nSignificant backward incompatible changes were made for the release of Python\n3.0, which may cause programs written for Python 2 to fail when run with Python\n3.  For more information about porting your code from Python 2 to Python 3, see\nthe `Porting HOWTO \u003chttps://docs.python.org/3/howto/pyporting.html\u003e`_.\n\n\nTesting\n-------\n\nTo test the interpreter, type ``make test`` in the top-level directory.  The\ntest set produces some output.  You can generally ignore the messages about\nskipped tests due to optional features which can't be imported.  If a message\nis printed about a failed test or a traceback or core dump is produced,\nsomething is wrong.\n\nBy default, tests are prevented from overusing resources like disk space and\nmemory.  To enable these tests, run ``make testall``.\n\nIf any tests fail, you can re-run the failing test(s) in verbose mode.  For\nexample, if ``test_os`` and ``test_gdb`` failed, you can run::\n\n    make test TESTOPTS=\"-v test_os test_gdb\"\n\nIf the failure persists and appears to be a problem with Python rather than\nyour environment, you can `file a bug report \u003chttps://bugs.python.org\u003e`_ and\ninclude relevant output from that command to show the issue.\n\nSee `Running \u0026 Writing Tests \u003chttps://devguide.python.org/runtests/\u003e`_\nfor more on running tests.\n\nInstalling multiple versions\n----------------------------\n\nOn Unix and Mac systems if you intend to install multiple versions of Python\nusing the same installation prefix (``--prefix`` argument to the configure\nscript) you must take care that your primary python executable is not\noverwritten by the installation of a different version.  All files and\ndirectories installed using ``make altinstall`` contain the major and minor\nversion and can thus live side-by-side.  ``make install`` also creates\n``${prefix}/bin/python3`` which refers to ``${prefix}/bin/pythonX.Y``.  If you\nintend to install multiple versions using the same prefix you must decide which\nversion (if any) is your \"primary\" version.  Install that version using ``make\ninstall``.  Install all other versions using ``make altinstall``.\n\nFor example, if you want to install Python 2.7, 3.6, and 3.8 with 3.8 being the\nprimary version, you would execute ``make install`` in your 3.8 build directory\nand ``make altinstall`` in the others.\n\n\nIssue Tracker and Mailing List\n------------------------------\n\nBug reports are welcome!  You can use the `issue tracker\n\u003chttps://bugs.python.org\u003e`_ to report bugs, and/or submit pull requests `on\nGitHub \u003chttps://github.com/python/cpython\u003e`_.\n\nYou can also follow development discussion on the `python-dev mailing list\n\u003chttps://mail.python.org/mailman/listinfo/python-dev/\u003e`_.\n\n\nProposals for enhancement\n-------------------------\n\nIf you have a proposal to change Python, you may want to send an email to the\ncomp.lang.python or `python-ideas`_ mailing lists for initial feedback.  A\nPython Enhancement Proposal (PEP) may be submitted if your idea gains ground.\nAll current PEPs, as well as guidelines for submitting a new PEP, are listed at\n`python.org/dev/peps/ \u003chttps://www.python.org/dev/peps/\u003e`_.\n\n.. _python-ideas: https://mail.python.org/mailman/listinfo/python-ideas/\n\n\nRelease Schedule\n----------------\n\nSee :pep:`569` for Python 3.8 release details.\n\n\nCopyright and License Information\n---------------------------------\n\nCopyright (c) 2001-2018 Python Software Foundation.  All rights reserved.\n\nCopyright (c) 2000 BeOpen.com.  All rights reserved.\n\nCopyright (c) 1995-2001 Corporation for National Research Initiatives.  All\nrights reserved.\n\nCopyright (c) 1991-1995 Stichting Mathematisch Centrum.  All rights reserved.\n\nSee the file \"LICENSE\" for information on the history of this software, terms \u0026\nconditions for usage, and a DISCLAIMER OF ALL WARRANTIES.\n\nThis Python distribution contains *no* GNU General Public License (GPL) code,\nso it may be used in proprietary projects.  There are interfaces to some GNU\ncode but these are entirely optional.\n\nAll trademarks referenced herein are property of their respective holders.\n"
  },
  {
    "repo": "soimort/you-get",
    "content": "# You-Get\n\n[![PyPI version](https://img.shields.io/pypi/v/you-get.svg)](https://pypi.python.org/pypi/you-get/)\n[![Build Status](https://travis-ci.org/soimort/you-get.svg)](https://travis-ci.org/soimort/you-get)\n[![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/soimort/you-get?utm_source=badge\u0026utm_medium=badge\u0026utm_campaign=pr-badge\u0026utm_content=badge)\n\n[You-Get](https://you-get.org/) is a tiny command-line utility to download media contents (videos, audios, images) from the Web, in case there is no other handy way to do it.\n\nHere's how you use `you-get` to download a video from [YouTube](https://www.youtube.com/watch?v=jNQXAC9IVRw):\n\n```console\n$ you-get 'https://www.youtube.com/watch?v=jNQXAC9IVRw'\nsite:                YouTube\ntitle:               Me at the zoo\nstream:\n    - itag:          43\n      container:     webm\n      quality:       medium\n      size:          0.5 MiB (564215 bytes)\n    # download-with: you-get --itag=43 [URL]\n\nDownloading Me at the zoo.webm ...\n 100% (  0.5/  0.5MB) ├██████████████████████████████████┤[1/1]    6 MB/s\n\nSaving Me at the zoo.en.srt ... Done.\n```\n\nAnd here's why you might want to use it:\n\n* You enjoyed something on the Internet, and just want to download them for your own pleasure.\n* You watch your favorite videos online from your computer, but you are prohibited from saving them. You feel that you have no control over your own computer. (And it's not how an open Web is supposed to work.)\n* You want to get rid of any closed-source technology or proprietary JavaScript code, and disallow things like Flash running on your computer.\n* You are an adherent of hacker culture and free software.\n\nWhat `you-get` can do for you:\n\n* Download videos / audios from popular websites such as YouTube, Youku, Niconico, and a bunch more. (See the [full list of supported sites](#supported-sites))\n* Stream an online video in your media player. No web browser, no more ads.\n* Download images (of interest) by scraping a web page.\n* Download arbitrary non-HTML contents, i.e., binary files.\n\nInterested? [Install it](#installation) now and [get started by examples](#getting-started).\n\nAre you a Python programmer? Then check out [the source](https://github.com/soimort/you-get) and fork it!\n\n![](https://i.imgur.com/GfthFAz.png)\n\n## Installation\n\n### Prerequisites\n\nThe following dependencies are required and must be installed separately, unless you are using a pre-built package or chocolatey on Windows:\n\n* **[Python 3](https://www.python.org/downloads/)**\n* **[FFmpeg](https://www.ffmpeg.org/)** (strongly recommended) or [Libav](https://libav.org/)\n* (Optional) [RTMPDump](https://rtmpdump.mplayerhq.hu/)\n\n### Option 1: Install via pip\n\nThe official release of `you-get` is distributed on [PyPI](https://pypi.python.org/pypi/you-get), and can be installed easily from a PyPI mirror via the [pip](https://en.wikipedia.org/wiki/Pip_\\(package_manager\\)) package manager. Note that you must use the Python 3 version of `pip`:\n\n    $ pip3 install you-get\n\n### Option 2: Install via [Antigen](https://github.com/zsh-users/antigen)\n\nAdd the following line to your `.zshrc`:\n\n    antigen bundle soimort/you-get\n\n### Option 3: Use a pre-built package (Windows only)\n\nDownload the `exe` (standalone) or `7z` (all dependencies included) from: \u003chttps://github.com/soimort/you-get/releases/latest\u003e.\n\n### Option 4: Download from GitHub\n\nYou may either download the [stable](https://github.com/soimort/you-get/archive/master.zip) (identical with the latest release on PyPI) or the [develop](https://github.com/soimort/you-get/archive/develop.zip) (more hotfixes, unstable features) branch of `you-get`. Unzip it, and put the directory containing the `you-get` script into your `PATH`.\n\nAlternatively, run\n\n```\n$ [sudo] python3 setup.py install\n```\n\nOr\n\n```\n$ python3 setup.py install --user\n```\n\nto install `you-get` to a permanent path.\n\n### Option 5: Git clone\n\nThis is the recommended way for all developers, even if you don't often code in Python.\n\n```\n$ git clone git://github.com/soimort/you-get.git\n```\n\nThen put the cloned directory into your `PATH`, or run `./setup.py install` to install `you-get` to a permanent path.\n\n### Option 6: Using [Chocolatey](https://chocolatey.org/) (Windows only)\n\n```\n\u003e choco install you-get\n```\n\n### Option 7: Homebrew (Mac only)\n\nYou can install `you-get` easily via:\n\n```\n$ brew install you-get\n```\n\n### Option 8: pkg (FreeBSD only)\n\nYou can install `you-get` easily via:\n\n```\n# pkg install you-get\n```\n\n### Shell completion\n\nCompletion definitions for Bash, Fish and Zsh can be found in [`contrib/completion`](https://github.com/soimort/you-get/tree/develop/contrib/completion). Please consult your shell's manual for how to take advantage of them.\n\n## Upgrading\n\nBased on which option you chose to install `you-get`, you may upgrade it via:\n\n```\n$ pip3 install --upgrade you-get\n```\n\nor download the latest release via:\n\n```\n$ you-get https://github.com/soimort/you-get/archive/master.zip\n```\n\nor use [chocolatey package manager](https://chocolatey.org):\n\n```\n\u003e choco upgrade you-get\n```\n\nIn order to get the latest ```develop``` branch without messing up the PIP, you can try:\n\n```\n$ pip3 install --upgrade git+https://github.com/soimort/you-get@develop\n```\n\n## Getting Started\n\n### Download a video\n\nWhen you get a video of interest, you might want to use the `--info`/`-i` option to see all available quality and formats:\n\n```\n$ you-get -i 'https://www.youtube.com/watch?v=jNQXAC9IVRw'\nsite:                YouTube\ntitle:               Me at the zoo\nstreams:             # Available quality and codecs\n    [ DEFAULT ] _________________________________\n    - itag:          43\n      container:     webm\n      quality:       medium\n      size:          0.5 MiB (564215 bytes)\n    # download-with: you-get --itag=43 [URL]\n\n    - itag:          18\n      container:     mp4\n      quality:       medium\n    # download-with: you-get --itag=18 [URL]\n\n    - itag:          5\n      container:     flv\n      quality:       small\n    # download-with: you-get --itag=5 [URL]\n\n    - itag:          36\n      container:     3gp\n      quality:       small\n    # download-with: you-get --itag=36 [URL]\n\n    - itag:          17\n      container:     3gp\n      quality:       small\n    # download-with: you-get --itag=17 [URL]\n```\n\nThe format marked with `DEFAULT` is the one you will get by default. If that looks cool to you, download it:\n\n```\n$ you-get 'https://www.youtube.com/watch?v=jNQXAC9IVRw'\nsite:                YouTube\ntitle:               Me at the zoo\nstream:\n    - itag:          43\n      container:     webm\n      quality:       medium\n      size:          0.5 MiB (564215 bytes)\n    # download-with: you-get --itag=43 [URL]\n\nDownloading zoo.webm ...\n100.0% (  0.5/0.5  MB) ├████████████████████████████████████████┤[1/1]    7 MB/s\n\nSaving Me at the zoo.en.srt ...Done.\n```\n\n(If a YouTube video has any closed captions, they will be downloaded together with the video file, in SubRip subtitle format.)\n\nOr, if you prefer another format (mp4), just use whatever the option `you-get` shows to you:\n\n```\n$ you-get --itag=18 'https://www.youtube.com/watch?v=jNQXAC9IVRw'\n```\n\n**Note:**\n\n* At this point, format selection has not been generally implemented for most of our supported sites; in that case, the default format to download is the one with the highest quality.\n* `ffmpeg` is a required dependency, for downloading and joining videos streamed in multiple parts (e.g. on some sites like Youku), and for YouTube videos of 1080p or high resolution.\n* If you don't want `you-get` to join video parts after downloading them, use the `--no-merge`/`-n` option.\n\n### Download anything else\n\nIf you already have the URL of the exact resource you want, you can download it directly with:\n\n```\n$ you-get https://stallman.org/rms.jpg\nSite:       stallman.org\nTitle:      rms\nType:       JPEG Image (image/jpeg)\nSize:       0.06 MiB (66482 Bytes)\n\nDownloading rms.jpg ...\n100.0% (  0.1/0.1  MB) ├████████████████████████████████████████┤[1/1]  127 kB/s\n```\n\nOtherwise, `you-get` will scrape the web page and try to figure out if there's anything interesting to you:\n\n```\n$ you-get http://kopasas.tumblr.com/post/69361932517\nSite:       Tumblr.com\nTitle:      kopasas\nType:       Unknown type (None)\nSize:       0.51 MiB (536583 Bytes)\n\nSite:       Tumblr.com\nTitle:      tumblr_mxhg13jx4n1sftq6do1_1280\nType:       Portable Network Graphics (image/png)\nSize:       0.51 MiB (536583 Bytes)\n\nDownloading tumblr_mxhg13jx4n1sftq6do1_1280.png ...\n100.0% (  0.5/0.5  MB) ├████████████████████████████████████████┤[1/1]   22 MB/s\n```\n\n**Note:**\n\n* This feature is an experimental one and far from perfect. It works best on scraping large-sized images from popular websites like Tumblr and Blogger, but there is really no universal pattern that can apply to any site on the Internet.\n\n### Search on Google Videos and download\n\nYou can pass literally anything to `you-get`. If it isn't a valid URL, `you-get` will do a Google search and download the most relevant video for you. (It might not be exactly the thing you wish to see, but still very likely.)\n\n```\n$ you-get \"Richard Stallman eats\"\n```\n\n### Pause and resume a download\n\nYou may use \u003ckbd\u003eCtrl\u003c/kbd\u003e+\u003ckbd\u003eC\u003c/kbd\u003e to interrupt a download.\n\nA temporary `.download` file is kept in the output directory. Next time you run `you-get` with the same arguments, the download progress will resume from the last session. In case the file is completely downloaded (the temporary `.download` extension is gone), `you-get` will just skip the download.\n\nTo enforce re-downloading, use the `--force`/`-f` option. (**Warning:** doing so will overwrite any existing file or temporary file with the same name!)\n\n### Set the path and name of downloaded file\n\nUse the `--output-dir`/`-o` option to set the path, and `--output-filename`/`-O` to set the name of the downloaded file:\n\n```\n$ you-get -o ~/Videos -O zoo.webm 'https://www.youtube.com/watch?v=jNQXAC9IVRw'\n```\n\n**Tips:**\n\n* These options are helpful if you encounter problems with the default video titles, which may contain special characters that do not play well with your current shell / operating system / filesystem.\n* These options are also helpful if you write a script to batch download files and put them into designated folders with designated names.\n\n### Proxy settings\n\nYou may specify an HTTP proxy for `you-get` to use, via the `--http-proxy`/`-x` option:\n\n```\n$ you-get -x 127.0.0.1:8087 'https://www.youtube.com/watch?v=jNQXAC9IVRw'\n```\n\nHowever, the system proxy setting (i.e. the environment variable `http_proxy`) is applied by default. To disable any proxy, use the `--no-proxy` option.\n\n**Tips:**\n\n* If you need to use proxies a lot (in case your network is blocking certain sites), you might want to use `you-get` with [proxychains](https://github.com/rofl0r/proxychains-ng) and set `alias you-get=\"proxychains -q you-get\"` (in Bash).\n* For some websites (e.g. Youku), if you need access to some videos that are only available in mainland China, there is an option of using a specific proxy to extract video information from the site: `--extractor-proxy`/`-y`.\n\n### Watch a video\n\nUse the `--player`/`-p` option to feed the video into your media player of choice, e.g. `mplayer` or `vlc`, instead of downloading it:\n\n```\n$ you-get -p vlc 'https://www.youtube.com/watch?v=jNQXAC9IVRw'\n```\n\nOr, if you prefer to watch the video in a browser, just without ads or comment section:\n\n```\n$ you-get -p chromium 'https://www.youtube.com/watch?v=jNQXAC9IVRw'\n```\n\n**Tips:**\n\n* It is possible to use the `-p` option to start another download manager, e.g., `you-get -p uget-gtk 'https://www.youtube.com/watch?v=jNQXAC9IVRw'`, though they may not play together very well.\n\n### Load cookies\n\nNot all videos are publicly available to anyone. If you need to log in your account to access something (e.g., a private video), it would be unavoidable to feed the browser cookies to `you-get` via the `--cookies`/`-c` option.\n\n**Note:**\n\n* As of now, we are supporting two formats of browser cookies: Mozilla `cookies.sqlite` and Netscape `cookies.txt`.\n\n### Reuse extracted data\n\nUse `--url`/`-u` to get a list of downloadable resource URLs extracted from the page. Use `--json` to get an abstract of extracted data in the JSON format.\n\n**Warning:**\n\n* For the time being, this feature has **NOT** been stabilized and the JSON schema may have breaking changes in the future.\n\n## Supported Sites\n\n| Site | URL | Videos? | Images? | Audios? |\n| :--: | :-- | :-----: | :-----: | :-----: |\n| **YouTube** | \u003chttps://www.youtube.com/\u003e    |✓| | |\n| **Twitter** | \u003chttps://twitter.com/\u003e        |✓|✓| |\n| VK          | \u003chttp://vk.com/\u003e              |✓|✓| |\n| Vine        | \u003chttps://vine.co/\u003e            |✓| | |\n| Vimeo       | \u003chttps://vimeo.com/\u003e          |✓| | |\n| Vidto       | \u003chttp://vidto.me/\u003e            |✓| | |\n| Videomega   | \u003chttp://videomega.tv/\u003e        |✓| | |\n| Veoh        | \u003chttp://www.veoh.com/\u003e        |✓| | |\n| **Tumblr**  | \u003chttps://www.tumblr.com/\u003e     |✓|✓|✓|\n| TED         | \u003chttp://www.ted.com/\u003e         |✓| | |\n| SoundCloud  | \u003chttps://soundcloud.com/\u003e     | | |✓|\n| SHOWROOM    | \u003chttps://www.showroom-live.com/\u003e |✓| | |\n| Pinterest   | \u003chttps://www.pinterest.com/\u003e  | |✓| |\n| MusicPlayOn | \u003chttp://en.musicplayon.com/\u003e  |✓| | |\n| MTV81       | \u003chttp://www.mtv81.com/\u003e       |✓| | |\n| Mixcloud    | \u003chttps://www.mixcloud.com/\u003e   | | |✓|\n| Metacafe    | \u003chttp://www.metacafe.com/\u003e    |✓| | |\n| Magisto     | \u003chttp://www.magisto.com/\u003e     |✓| | |\n| Khan Academy | \u003chttps://www.khanacademy.org/\u003e |✓| | |\n| Internet Archive | \u003chttps://archive.org/\u003e   |✓| | |\n| **Instagram** | \u003chttps://instagram.com/\u003e    |✓|✓| |\n| InfoQ       | \u003chttp://www.infoq.com/presentations/\u003e |✓| | |\n| Imgur       | \u003chttp://imgur.com/\u003e           | |✓| |\n| Heavy Music Archive | \u003chttp://www.heavy-music.ru/\u003e | | |✓|\n| **Google+** | \u003chttps://plus.google.com/\u003e    |✓|✓| |\n| Freesound   | \u003chttp://www.freesound.org/\u003e   | | |✓|\n| Flickr      | \u003chttps://www.flickr.com/\u003e     |✓|✓| |\n| FC2 Video   | \u003chttp://video.fc2.com/\u003e       |✓| | |\n| Facebook    | \u003chttps://www.facebook.com/\u003e   |✓| | |\n| eHow        | \u003chttp://www.ehow.com/\u003e        |✓| | |\n| Dailymotion | \u003chttp://www.dailymotion.com/\u003e |✓| | |\n| Coub        | \u003chttp://coub.com/\u003e            |✓| | |\n| CBS         | \u003chttp://www.cbs.com/\u003e         |✓| | |\n| Bandcamp    | \u003chttp://bandcamp.com/\u003e        | | |✓|\n| AliveThai   | \u003chttp://alive.in.th/\u003e         |✓| | |\n| interest.me | \u003chttp://ch.interest.me/tvn\u003e   |✓| | |\n| **755\u003cbr/\u003eナナゴーゴー** | \u003chttp://7gogo.jp/\u003e |✓|✓| |\n| **niconico\u003cbr/\u003eニコニコ動画** | \u003chttp://www.nicovideo.jp/\u003e |✓| | |\n| **163\u003cbr/\u003e网易视频\u003cbr/\u003e网易云音乐** | \u003chttp://v.163.com/\u003e\u003cbr/\u003e\u003chttp://music.163.com/\u003e |✓| |✓|\n| 56网     | \u003chttp://www.56.com/\u003e           |✓| | |\n| **AcFun** | \u003chttp://www.acfun.tv/\u003e        |✓| | |\n| **Baidu\u003cbr/\u003e百度贴吧** | \u003chttp://tieba.baidu.com/\u003e |✓|✓| |\n| 爆米花网 | \u003chttp://www.baomihua.com/\u003e     |✓| | |\n| **bilibili\u003cbr/\u003e哔哩哔哩** | \u003chttp://www.bilibili.com/\u003e |✓| | |\n| Dilidili | \u003chttp://www.dilidili.com/\u003e     |✓| | |\n| 豆瓣     | \u003chttp://www.douban.com/\u003e       |✓| |✓|\n| 斗鱼     | \u003chttp://www.douyutv.com/\u003e      |✓| | |\n| Panda\u003cbr/\u003e熊猫 | \u003chttp://www.panda.tv/\u003e      |✓| | |\n| 凤凰视频 | \u003chttp://v.ifeng.com/\u003e          |✓| | |\n| 风行网   | \u003chttp://www.fun.tv/\u003e           |✓| | |\n| iQIYI\u003cbr/\u003e爱奇艺 | \u003chttp://www.iqiyi.com/\u003e |✓| | |\n| 激动网   | \u003chttp://www.joy.cn/\u003e           |✓| | |\n| 酷6网    | \u003chttp://www.ku6.com/\u003e          |✓| | |\n| 酷狗音乐 | \u003chttp://www.kugou.com/\u003e        | | |✓|\n| 酷我音乐 | \u003chttp://www.kuwo.cn/\u003e          | | |✓|\n| 乐视网   | \u003chttp://www.le.com/\u003e           |✓| | |\n| 荔枝FM   | \u003chttp://www.lizhi.fm/\u003e         | | |✓|\n| 秒拍     | \u003chttp://www.miaopai.com/\u003e      |✓| | |\n| MioMio弹幕网 | \u003chttp://www.miomio.tv/\u003e    |✓| | |\n| 痞客邦   | \u003chttps://www.pixnet.net/\u003e      |✓| | |\n| PPTV聚力 | \u003chttp://www.pptv.com/\u003e         |✓| | |\n| 齐鲁网   | \u003chttp://v.iqilu.com/\u003e          |✓| | |\n| QQ\u003cbr/\u003e腾讯视频 | \u003chttp://v.qq.com/\u003e      |✓| | |\n| 企鹅直播 | \u003chttp://live.qq.com/\u003e          |✓| | |\n| Sina\u003cbr/\u003e新浪视频\u003cbr/\u003e微博秒拍视频 | \u003chttp://video.sina.com.cn/\u003e\u003cbr/\u003e\u003chttp://video.weibo.com/\u003e |✓| | |\n| Sohu\u003cbr/\u003e搜狐视频 | \u003chttp://tv.sohu.com/\u003e |✓| | |\n| **Tudou\u003cbr/\u003e土豆** | \u003chttp://www.tudou.com/\u003e |✓| | |\n| 虾米     | \u003chttp://www.xiami.com/\u003e        |✓| |✓|\n| 阳光卫视 | \u003chttp://www.isuntv.com/\u003e       |✓| | |\n| **音悦Tai** | \u003chttp://www.yinyuetai.com/\u003e |✓| | |\n| **Youku\u003cbr/\u003e优酷** | \u003chttp://www.youku.com/\u003e |✓| | |\n| 战旗TV   | \u003chttp://www.zhanqi.tv/lives\u003e   |✓| | |\n| 央视网   | \u003chttp://www.cntv.cn/\u003e          |✓| | |\n| 花瓣     | \u003chttp://huaban.com/\u003e           | |✓| |\n| Naver\u003cbr/\u003e네이버 | \u003chttp://tvcast.naver.com/\u003e     |✓| | |\n| 芒果TV   | \u003chttp://www.mgtv.com/\u003e         |✓| | |\n| 火猫TV   | \u003chttp://www.huomao.com/\u003e       |✓| | |\n| 全民直播 | \u003chttp://www.quanmin.tv/\u003e       |✓| | |\n| 阳光宽频网 | \u003chttp://www.365yg.com/\u003e      |✓| | |\n| 西瓜视频 | \u003chttps://www.ixigua.com/\u003e      |✓| | |\n| 快手 | \u003chttps://www.kuaishou.com/\u003e      |✓|✓| |\n| 抖音 | \u003chttps://www.douyin.com/\u003e      |✓| | |\n| TikTok | \u003chttps://www.tiktok.com/\u003e      |✓| | |\n| 中国体育(TV) | \u003chttp://v.zhibo.tv/\u003e \u003c/br\u003e\u003chttp://video.zhibo.tv/\u003e    |✓| | |\n| 知乎 | \u003chttps://www.zhihu.com/\u003e      |✓| | |\n\nFor all other sites not on the list, the universal extractor will take care of finding and downloading interesting resources from the page.\n\n### Known bugs\n\nIf something is broken and `you-get` can't get you things you want, don't panic. (Yes, this happens all the time!)\n\nCheck if it's already a known problem on \u003chttps://github.com/soimort/you-get/wiki/Known-Bugs\u003e. If not, follow the guidelines on [how to report a broken extractor](https://github.com/soimort/you-get/blob/develop/CONTRIBUTING.md#report-a-broken-extractor).\n\n## Getting Involved\n\nYou can reach us on the Gitter channel [#soimort/you-get](https://gitter.im/soimort/you-get) (here's how you [set up your IRC client](http://irc.gitter.im) for Gitter). If you have a quick question regarding `you-get`, ask it there.\n\nIf you are seeking to report an issue or contribute, please make sure to read [the guidelines](https://github.com/soimort/you-get/blob/develop/CONTRIBUTING.md) first.\n\n## Legal Issues\n\nThis software is distributed under the [MIT license](https://raw.github.com/soimort/you-get/master/LICENSE.txt).\n\nIn particular, please be aware that\n\n\u003e THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\nTranslated to human words:\n\n*In case your use of the software forms the basis of copyright infringement, or you use the software for any other illegal purposes, the authors cannot take any responsibility for you.*\n\nWe only ship the code here, and how you are going to use it is left to your own discretion.\n\n## Authors\n\nMade by [@soimort](https://github.com/soimort), who is in turn powered by :coffee:, :beer: and :ramen:.\n\nYou can find the [list of all contributors](https://github.com/soimort/you-get/graphs/contributors) here.\n"
  },
  {
    "repo": "isocpp/CppCoreGuidelines",
    "content": "[![C++ Core Guidelines](cpp_core_guidelines_logo_text.png)](http://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines)\n\n\u003e\"Within C++ is a smaller, simpler, safer language struggling to get out.\"\n\u003e-- \u003ccite\u003eBjarne Stroustrup\u003c/cite\u003e\n\nThe [C++ Core Guidelines](CppCoreGuidelines.md) are a collaborative effort led by Bjarne Stroustrup, much like the C++ language itself. They are the result of many\nperson-years of discussion and design across a number of organizations. Their design encourages general applicability and broad adoption but\nthey can be freely copied and modified to meet your organization's needs.\n\n## Getting started\n\nThe guidelines themselves are found at [CppCoreGuidelines](CppCoreGuidelines.md). The document is in [GH-flavored MarkDown](https://github.github.com/gfm/). It is intentionally kept simple, mostly in ASCII, to allow automatic post-processing such as language translation and reformatting. The editors maintain one\n[version formatted for browsing](http://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines). Note that it is manually integrated and can be slightly older than the version in the master branch.\n\nThe Guidelines are a constantly evolving document without a strict \"release\" cadence. Bjarne Stroustrup periodically reviews the document and increments the version number in the introduction. [Checkins that increment the version number](https://github.com/isocpp/CppCoreGuidelines/releases) are tagged in git. \n\nMany of the guidelines make use of the header-only Guidelines Support Library. One implementation is available at [GSL: Guidelines Support Library](https://github.com/Microsoft/GSL).\n\n## Background and scope\n\nThe aim of the guidelines is to help people to use modern C++ effectively. By \"modern C++\" we mean C++11, C++14, and C++17. In other\nwords, what would you like your code to look like in 5 years' time, given that you can start now? In 10 years' time?\n\nThe guidelines are focused on relatively higher-level issues, such as interfaces, resource management, memory management, and concurrency. Such\nrules affect application architecture and library design. Following the rules will lead to code that is statically type-safe, has no resource\nleaks, and catches many more programming logic errors than is common in code today. And it will run fast -- you can afford to do things right.\n\nWe are less concerned with low-level issues, such as naming conventions and indentation style. However, no topic that can help a programmer is\nout of bounds.\n\nOur initial set of rules emphasizes safety (of various forms) and simplicity. They may very well be too strict. We expect to have to introduce\nmore exceptions to better accommodate real-world needs. We also need more rules.\n\nYou will find some of the rules contrary to your expectations or even contrary to your experience. If we haven't suggested that you change your\ncoding style in any way, we have failed! Please try to verify or disprove rules! In particular, we'd really like to have some of our rules\nbacked up with measurements or better examples.\n\nYou will find some of the rules obvious or even trivial. Please remember that one purpose of a guideline is to help someone who is less\nexperienced or coming from a different background or language to get up to speed.\n\nThe rules are designed to be supported by an analysis tool. Violations of rules will be flagged with references (or links) to the relevant rule.\nWe do not expect you to memorize all the rules before trying to write code.\n\nThe rules are meant for gradual introduction into a code base. We plan to build tools for that and hope others will too.\n\n## Contributions and LICENSE\n\nComments and suggestions for improvements are most welcome. We plan to modify and extend this document as our understanding improves and the\nlanguage and the set of available libraries improve. More details are found at [CONTRIBUTING](./CONTRIBUTING.md) and [LICENSE](./LICENSE) .\n"
  },
  {
    "repo": "floodsung/Deep-Learning-Papers-Reading-Roadmap",
    "content": "# Deep Learning Papers Reading Roadmap\n\n\u003eIf you are a newcomer to the Deep Learning area, the first question you may have is \"Which paper should I start reading from?\"\n\n\u003eHere is a reading roadmap of Deep Learning papers!\n\nThe roadmap is constructed in accordance with the following four guidelines:\n\n- From outline to detail\n- From old to state-of-the-art\n- from generic to specific areas\n- focus on state-of-the-art\n\nYou will find many papers that are quite new but really worth reading.\n\nI would continue adding papers to this roadmap.\n\n\n---------------------------------------\n\n# 1 Deep Learning History and Basics\n\n## 1.0 Book\n\n**[0]** Bengio, Yoshua, Ian J. Goodfellow, and Aaron Courville. \"**Deep learning**.\" An MIT Press book. (2015). [[html]](http://www.deeplearningbook.org/) **(Deep Learning Bible, you can read this book while reading following papers.)** :star::star::star::star::star:\n\n## 1.1 Survey\n\n**[1]** LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. \"**Deep learning**.\" Nature 521.7553 (2015): 436-444. [[pdf]](http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf) **(Three Giants' Survey)** :star::star::star::star::star:\n\n## 1.2 Deep Belief Network(DBN)(Milestone of Deep Learning Eve)\n\n**[2]** Hinton, Geoffrey E., Simon Osindero, and Yee-Whye Teh. \"**A fast learning algorithm for deep belief nets**.\" Neural computation 18.7 (2006): 1527-1554. [[pdf]](http://www.cs.toronto.edu/~hinton/absps/ncfast.pdf)**(Deep Learning Eve)** :star::star::star:\n\n**[3]** Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. \"**Reducing the dimensionality of data with neural networks**.\" Science 313.5786 (2006): 504-507. [[pdf]](http://www.cs.toronto.edu/~hinton/science.pdf) **(Milestone, Show the promise of deep learning)** :star::star::star:\n\n## 1.3 ImageNet Evolution（Deep Learning broke out from here）\n\n**[4]** Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"**Imagenet classification with deep convolutional neural networks**.\" Advances in neural information processing systems. 2012. [[pdf]](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) **(AlexNet, Deep Learning Breakthrough)** :star::star::star::star::star:\n\n**[5]** Simonyan, Karen, and Andrew Zisserman. \"**Very deep convolutional networks for large-scale image recognition**.\" arXiv preprint arXiv:1409.1556 (2014). [[pdf]](https://arxiv.org/pdf/1409.1556.pdf) **(VGGNet,Neural Networks become very deep!)** :star::star::star:\n\n**[6]** Szegedy, Christian, et al. \"**Going deeper with convolutions**.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf) **(GoogLeNet)** :star::star::star:\n\n**[7]** He, Kaiming, et al. \"**Deep residual learning for image recognition**.\" arXiv preprint arXiv:1512.03385 (2015). [[pdf]](https://arxiv.org/pdf/1512.03385.pdf) **(ResNet,Very very deep networks, CVPR best paper)** :star::star::star::star::star:\n\n## 1.4 Speech Recognition Evolution\n\n**[8]** Hinton, Geoffrey, et al. \"**Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups**.\" IEEE Signal Processing Magazine 29.6 (2012): 82-97. [[pdf]](http://cs224d.stanford.edu/papers/maas_paper.pdf) **(Breakthrough in speech recognition)**:star::star::star::star:\n\n**[9]** Graves, Alex, Abdel-rahman Mohamed, and Geoffrey Hinton. \"**Speech recognition with deep recurrent neural networks**.\" 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. [[pdf]](http://arxiv.org/pdf/1303.5778.pdf) **(RNN)**:star::star::star:\n\n**[10]** Graves, Alex, and Navdeep Jaitly. \"**Towards End-To-End Speech Recognition with Recurrent Neural Networks**.\" ICML. Vol. 14. 2014. [[pdf]](http://www.jmlr.org/proceedings/papers/v32/graves14.pdf):star::star::star:\n\n**[11]** Sak, Haşim, et al. \"**Fast and accurate recurrent neural network acoustic models for speech recognition**.\" arXiv preprint arXiv:1507.06947 (2015). [[pdf]](http://arxiv.org/pdf/1507.06947) **(Google Speech Recognition System)** :star::star::star:\n\n**[12]** Amodei, Dario, et al. \"**Deep speech 2: End-to-end speech recognition in english and mandarin**.\" arXiv preprint arXiv:1512.02595 (2015). [[pdf]](https://arxiv.org/pdf/1512.02595.pdf) **(Baidu Speech Recognition System)** :star::star::star::star:\n\n**[13]** W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, G. Zweig \"**Achieving Human Parity in Conversational Speech Recognition**.\" arXiv preprint arXiv:1610.05256 (2016). [[pdf]](https://arxiv.org/pdf/1610.05256v1) **(State-of-the-art in speech recognition, Microsoft)** :star::star::star::star:\n\n\u003eAfter reading above papers, you will have a basic understanding of the Deep Learning history, the basic architectures of Deep Learning model(including CNN, RNN, LSTM) and how deep learning can be applied to image and speech recognition issues. The following papers will take you in-depth understanding of the Deep Learning method, Deep Learning in different areas of application and the frontiers. I suggest that you can choose the following papers based on your interests and research direction.\n\n#2 Deep Learning Method\n\n## 2.1 Model\n\n**[14]** Hinton, Geoffrey E., et al. \"**Improving neural networks by preventing co-adaptation of feature detectors**.\" arXiv preprint arXiv:1207.0580 (2012). [[pdf]](https://arxiv.org/pdf/1207.0580.pdf) **(Dropout)** :star::star::star:\n\n**[15]** Srivastava, Nitish, et al. \"**Dropout: a simple way to prevent neural networks from overfitting**.\" Journal of Machine Learning Research 15.1 (2014): 1929-1958. [[pdf]](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) :star::star::star:\n\n**[16]** Ioffe, Sergey, and Christian Szegedy. \"**Batch normalization: Accelerating deep network training by reducing internal covariate shift**.\" arXiv preprint arXiv:1502.03167 (2015). [[pdf]](http://arxiv.org/pdf/1502.03167) **(An outstanding Work in 2015)** :star::star::star::star:\n\n**[17]** Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. \"**Layer normalization**.\" arXiv preprint arXiv:1607.06450 (2016). [[pdf]](https://arxiv.org/pdf/1607.06450.pdf?utm_source=sciontist.com\u0026utm_medium=refer\u0026utm_campaign=promote) **(Update of Batch Normalization)** :star::star::star::star:\n\n**[18]** Courbariaux, Matthieu, et al. \"**Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 or−1**.\" [[pdf]](https://pdfs.semanticscholar.org/f832/b16cb367802609d91d400085eb87d630212a.pdf) **(New Model,Fast)**  :star::star::star:\n\n**[19]** Jaderberg, Max, et al. \"**Decoupled neural interfaces using synthetic gradients**.\" arXiv preprint arXiv:1608.05343 (2016). [[pdf]](https://arxiv.org/pdf/1608.05343) **(Innovation of Training Method,Amazing Work)** :star::star::star::star::star:\n\n**[20]** Chen, Tianqi, Ian Goodfellow, and Jonathon Shlens. \"Net2net: Accelerating learning via knowledge transfer.\" arXiv preprint arXiv:1511.05641 (2015). [[pdf]](https://arxiv.org/abs/1511.05641) **(Modify previously trained network to reduce training epochs)** :star::star::star:\n\n**[21]** Wei, Tao, et al. \"Network Morphism.\" arXiv preprint arXiv:1603.01670 (2016). [[pdf]](https://arxiv.org/abs/1603.01670) **(Modify previously trained network to reduce training epochs)** :star::star::star:\n\n## 2.2 Optimization\n\n**[22]** Sutskever, Ilya, et al. \"**On the importance of initialization and momentum in deep learning**.\" ICML (3) 28 (2013): 1139-1147. [[pdf]](http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf) **(Momentum optimizer)** :star::star:\n\n**[23]** Kingma, Diederik, and Jimmy Ba. \"**Adam: A method for stochastic optimization**.\" arXiv preprint arXiv:1412.6980 (2014). [[pdf]](http://arxiv.org/pdf/1412.6980) **(Maybe used most often currently)** :star::star::star:\n\n**[24]** Andrychowicz, Marcin, et al. \"**Learning to learn by gradient descent by gradient descent**.\" arXiv preprint arXiv:1606.04474 (2016). [[pdf]](https://arxiv.org/pdf/1606.04474) **(Neural Optimizer,Amazing Work)** :star::star::star::star::star:\n\n**[25]** Han, Song, Huizi Mao, and William J. Dally. \"**Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding**.\" CoRR, abs/1510.00149 2 (2015). [[pdf]](https://pdfs.semanticscholar.org/5b6c/9dda1d88095fa4aac1507348e498a1f2e863.pdf) **(ICLR best paper, new direction to make NN running fast,DeePhi Tech Startup)** :star::star::star::star::star:\n\n**[26]** Iandola, Forrest N., et al. \"**SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and\u003c 1MB model size**.\" arXiv preprint arXiv:1602.07360 (2016). [[pdf]](http://arxiv.org/pdf/1602.07360) **(Also a new direction to optimize NN,DeePhi Tech Startup)** :star::star::star::star:\n\n## 2.3 Unsupervised Learning / Deep Generative Model\n\n**[27]** Le, Quoc V. \"**Building high-level features using large scale unsupervised learning**.\" 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. [[pdf]](http://arxiv.org/pdf/1112.6209.pdf\u0026embed) **(Milestone, Andrew Ng, Google Brain Project, Cat)** :star::star::star::star:\n\n\n**[28]** Kingma, Diederik P., and Max Welling. \"**Auto-encoding variational bayes**.\" arXiv preprint arXiv:1312.6114 (2013). [[pdf]](http://arxiv.org/pdf/1312.6114) **(VAE)** :star::star::star::star:\n\n**[29]** Goodfellow, Ian, et al. \"**Generative adversarial nets**.\" Advances in Neural Information Processing Systems. 2014. [[pdf]](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) **(GAN,super cool idea)** :star::star::star::star::star:\n\n**[30]** Radford, Alec, Luke Metz, and Soumith Chintala. \"**Unsupervised representation learning with deep convolutional generative adversarial networks**.\" arXiv preprint arXiv:1511.06434 (2015). [[pdf]](http://arxiv.org/pdf/1511.06434) **(DCGAN)** :star::star::star::star:\n\n**[31]** Gregor, Karol, et al. \"**DRAW: A recurrent neural network for image generation**.\" arXiv preprint arXiv:1502.04623 (2015). [[pdf]](http://jmlr.org/proceedings/papers/v37/gregor15.pdf) **(VAE with attention, outstanding work)** :star::star::star::star::star:\n\n**[32]** Oord, Aaron van den, Nal Kalchbrenner, and Koray Kavukcuoglu. \"**Pixel recurrent neural networks**.\" arXiv preprint arXiv:1601.06759 (2016). [[pdf]](http://arxiv.org/pdf/1601.06759) **(PixelRNN)** :star::star::star::star:\n\n**[33]** Oord, Aaron van den, et al. \"Conditional image generation with PixelCNN decoders.\" arXiv preprint arXiv:1606.05328 (2016). [[pdf]](https://arxiv.org/pdf/1606.05328) **(PixelCNN)** :star::star::star::star:\n\n## 2.4 RNN / Sequence-to-Sequence Model\n\n**[34]** Graves, Alex. \"**Generating sequences with recurrent neural networks**.\" arXiv preprint arXiv:1308.0850 (2013). [[pdf]](http://arxiv.org/pdf/1308.0850) **(LSTM, very nice generating result, show the power of RNN)** :star::star::star::star:\n\n**[35]** Cho, Kyunghyun, et al. \"**Learning phrase representations using RNN encoder-decoder for statistical machine translation**.\" arXiv preprint arXiv:1406.1078 (2014). [[pdf]](http://arxiv.org/pdf/1406.1078) **(First Seq-to-Seq Paper)** :star::star::star::star:\n\n**[36]** Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. \"**Sequence to sequence learning with neural networks**.\" Advances in neural information processing systems. 2014. [[pdf]](https://arxiv.org/pdf/1409.3215.pdf) **(Outstanding Work)** :star::star::star::star::star:\n\n**[37]** Bahdanau, Dzmitry, KyungHyun Cho, and Yoshua Bengio. \"**Neural Machine Translation by Jointly Learning to Align and Translate**.\" arXiv preprint arXiv:1409.0473 (2014). [[pdf]](https://arxiv.org/pdf/1409.0473v7.pdf) :star::star::star::star:\n\n**[38]** Vinyals, Oriol, and Quoc Le. \"**A neural conversational model**.\" arXiv preprint arXiv:1506.05869 (2015). [[pdf]](http://arxiv.org/pdf/1506.05869.pdf%20(http://arxiv.org/pdf/1506.05869.pdf)) **(Seq-to-Seq on Chatbot)** :star::star::star:\n\n## 2.5 Neural Turing Machine\n\n**[39]** Graves, Alex, Greg Wayne, and Ivo Danihelka. \"**Neural turing machines**.\" arXiv preprint arXiv:1410.5401 (2014). [[pdf]](http://arxiv.org/pdf/1410.5401.pdf) **(Basic Prototype of Future Computer)** :star::star::star::star::star:\n\n**[40]** Zaremba, Wojciech, and Ilya Sutskever. \"**Reinforcement learning neural Turing machines**.\" arXiv preprint arXiv:1505.00521 362 (2015). [[pdf]](https://pdfs.semanticscholar.org/f10e/071292d593fef939e6ef4a59baf0bb3a6c2b.pdf) :star::star::star:\n\n**[41]** Weston, Jason, Sumit Chopra, and Antoine Bordes. \"**Memory networks**.\" arXiv preprint arXiv:1410.3916 (2014). [[pdf]](http://arxiv.org/pdf/1410.3916) :star::star::star:\n\n\n**[42]** Sukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. \"**End-to-end memory networks**.\" Advances in neural information processing systems. 2015. [[pdf]](http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf) :star::star::star::star:\n\n**[43]** Vinyals, Oriol, Meire Fortunato, and Navdeep Jaitly. \"**Pointer networks**.\" Advances in Neural Information Processing Systems. 2015. [[pdf]](http://papers.nips.cc/paper/5866-pointer-networks.pdf) :star::star::star::star:\n\n**[44]** Graves, Alex, et al. \"**Hybrid computing using a neural network with dynamic external memory**.\" Nature (2016). [[pdf]](https://www.dropbox.com/s/0a40xi702grx3dq/2016-graves.pdf) **(Milestone,combine above papers' ideas)** :star::star::star::star::star:\n\n## 2.6 Deep Reinforcement Learning\n\n**[45]** Mnih, Volodymyr, et al. \"**Playing atari with deep reinforcement learning**.\" arXiv preprint arXiv:1312.5602 (2013). [[pdf]](http://arxiv.org/pdf/1312.5602.pdf)) **(First Paper named deep reinforcement learning)** :star::star::star::star:\n\n**[46]** Mnih, Volodymyr, et al. \"**Human-level control through deep reinforcement learning**.\" Nature 518.7540 (2015): 529-533. [[pdf]](https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf) **(Milestone)** :star::star::star::star::star:\n\n**[47]** Wang, Ziyu, Nando de Freitas, and Marc Lanctot. \"**Dueling network architectures for deep reinforcement learning**.\" arXiv preprint arXiv:1511.06581 (2015). [[pdf]](http://arxiv.org/pdf/1511.06581) **(ICLR best paper,great idea)**  :star::star::star::star:\n\n**[48]** Mnih, Volodymyr, et al. \"**Asynchronous methods for deep reinforcement learning**.\" arXiv preprint arXiv:1602.01783 (2016). [[pdf]](http://arxiv.org/pdf/1602.01783) **(State-of-the-art method)** :star::star::star::star::star:\n\n**[49]** Lillicrap, Timothy P., et al. \"**Continuous control with deep reinforcement learning**.\" arXiv preprint arXiv:1509.02971 (2015). [[pdf]](http://arxiv.org/pdf/1509.02971) **(DDPG)** :star::star::star::star:\n\n**[50]** Gu, Shixiang, et al. \"**Continuous Deep Q-Learning with Model-based Acceleration**.\" arXiv preprint arXiv:1603.00748 (2016). [[pdf]](http://arxiv.org/pdf/1603.00748) **(NAF)** :star::star::star::star:\n\n**[51]** Schulman, John, et al. \"**Trust region policy optimization**.\" CoRR, abs/1502.05477 (2015). [[pdf]](http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf) **(TRPO)** :star::star::star::star:\n\n**[52]** Silver, David, et al. \"**Mastering the game of Go with deep neural networks and tree search**.\" Nature 529.7587 (2016): 484-489. [[pdf]](http://willamette.edu/~levenick/cs448/goNature.pdf) **(AlphaGo)** :star::star::star::star::star:\n\n## 2.7 Deep Transfer Learning / Lifelong Learning / especially for RL\n\n**[53]** Bengio, Yoshua. \"**Deep Learning of Representations for Unsupervised and Transfer Learning**.\" ICML Unsupervised and Transfer Learning 27 (2012): 17-36. [[pdf]](http://www.jmlr.org/proceedings/papers/v27/bengio12a/bengio12a.pdf) **(A Tutorial)** :star::star::star:\n\n**[54]** Silver, Daniel L., Qiang Yang, and Lianghao Li. \"**Lifelong Machine Learning Systems: Beyond Learning Algorithms**.\" AAAI Spring Symposium: Lifelong Machine Learning. 2013. [[pdf]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.696.7800\u0026rep=rep1\u0026type=pdf) **(A brief discussion about lifelong learning)**  :star::star::star:\n\n**[55]** Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. \"**Distilling the knowledge in a neural network**.\" arXiv preprint arXiv:1503.02531 (2015). [[pdf]](http://arxiv.org/pdf/1503.02531) **(Godfather's Work)** :star::star::star::star:\n\n**[56]** Rusu, Andrei A., et al. \"**Policy distillation**.\" arXiv preprint arXiv:1511.06295 (2015). [[pdf]](http://arxiv.org/pdf/1511.06295) **(RL domain)** :star::star::star:\n\n**[57]** Parisotto, Emilio, Jimmy Lei Ba, and Ruslan Salakhutdinov. \"**Actor-mimic: Deep multitask and transfer reinforcement learning**.\" arXiv preprint arXiv:1511.06342 (2015). [[pdf]](http://arxiv.org/pdf/1511.06342) **(RL domain)** :star::star::star:\n\n**[58]** Rusu, Andrei A., et al. \"**Progressive neural networks**.\" arXiv preprint arXiv:1606.04671 (2016). [[pdf]](https://arxiv.org/pdf/1606.04671) **(Outstanding Work, A novel idea)** :star::star::star::star::star:\n\n\n## 2.8 One Shot Deep Learning\n\n**[59]** Lake, Brenden M., Ruslan Salakhutdinov, and Joshua B. Tenenbaum. \"**Human-level concept learning through probabilistic program induction**.\" Science 350.6266 (2015): 1332-1338. [[pdf]](http://clm.utexas.edu/compjclub/wp-content/uploads/2016/02/lake2015.pdf) **(No Deep Learning,but worth reading)** :star::star::star::star::star:\n\n**[60]** Koch, Gregory, Richard Zemel, and Ruslan Salakhutdinov. \"**Siamese Neural Networks for One-shot Image Recognition**.\"(2015) [[pdf]](http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf) :star::star::star:\n\n**[61]** Santoro, Adam, et al. \"**One-shot Learning with Memory-Augmented Neural Networks**.\" arXiv preprint arXiv:1605.06065 (2016). [[pdf]](http://arxiv.org/pdf/1605.06065) **(A basic step to one shot learning)** :star::star::star::star:\n\n**[62]** Vinyals, Oriol, et al. \"**Matching Networks for One Shot Learning**.\" arXiv preprint arXiv:1606.04080 (2016). [[pdf]](https://arxiv.org/pdf/1606.04080) :star::star::star:\n\n**[63]** Hariharan, Bharath, and Ross Girshick. \"**Low-shot visual object recognition**.\" arXiv preprint arXiv:1606.02819 (2016). [[pdf]](http://arxiv.org/pdf/1606.02819) **(A step to large data)** :star::star::star::star:\n\n\n# 3 Applications\n\n## 3.1 NLP(Natural Language Processing)\n\n**[1]** Antoine Bordes, et al. \"**Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing**.\" AISTATS(2012) [[pdf]](https://www.hds.utc.fr/~bordesan/dokuwiki/lib/exe/fetch.php?id=en%3Apubli\u0026cache=cache\u0026media=en:bordes12aistats.pdf) :star::star::star::star:\n\n**[2]** Mikolov, et al. \"**Distributed representations of words and phrases and their compositionality**.\" ANIPS(2013): 3111-3119 [[pdf]](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) **(word2vec)** :star::star::star:\n\n**[3]** Sutskever, et al. \"**“Sequence to sequence learning with neural networks**.\" ANIPS(2014) [[pdf]](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) :star::star::star:\n\n**[4]** Ankit Kumar, et al. \"**“Ask Me Anything: Dynamic Memory Networks for Natural Language Processing**.\" arXiv preprint arXiv:1506.07285(2015) [[pdf]](https://arxiv.org/abs/1506.07285) :star::star::star::star:\n\n**[5]** Yoon Kim, et al. \"**Character-Aware Neural Language Models**.\" NIPS(2015) arXiv preprint arXiv:1508.06615(2015) [[pdf]](https://arxiv.org/abs/1508.06615) :star::star::star::star:\n\n**[6]** Jason Weston, et al. \"**Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks**.\" arXiv preprint arXiv:1502.05698(2015) [[pdf]](https://arxiv.org/abs/1502.05698) **(bAbI tasks)** :star::star::star:\n\n**[7]** Karl Moritz Hermann, et al. \"**Teaching Machines to Read and Comprehend**.\" arXiv preprint arXiv:1506.03340(2015) [[pdf]](https://arxiv.org/abs/1506.03340) **(CNN/DailyMail cloze style questions)** :star::star:\n\n**[8]** Alexis Conneau, et al. \"**Very Deep Convolutional Networks for Natural Language Processing**.\" arXiv preprint arXiv:1606.01781(2016) [[pdf]](https://arxiv.org/abs/1606.01781) **(state-of-the-art in text classification)** :star::star::star:\n\n**[9]** Armand Joulin, et al. \"**Bag of Tricks for Efficient Text Classification**.\" arXiv preprint arXiv:1607.01759(2016) [[pdf]](https://arxiv.org/abs/1607.01759) **(slightly worse than state-of-the-art, but a lot faster)** :star::star::star:\n\n## 3.2 Object Detection\n\n**[1]** Szegedy, Christian, Alexander Toshev, and Dumitru Erhan. \"**Deep neural networks for object detection**.\" Advances in Neural Information Processing Systems. 2013. [[pdf]](http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf) :star::star::star:\n\n**[2]** Girshick, Ross, et al. \"**Rich feature hierarchies for accurate object detection and semantic segmentation**.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2014. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf) **(RCNN)** :star::star::star::star::star:\n\n**[3]** He, Kaiming, et al. \"**Spatial pyramid pooling in deep convolutional networks for visual recognition**.\" European Conference on Computer Vision. Springer International Publishing, 2014. [[pdf]](http://arxiv.org/pdf/1406.4729) **(SPPNet)** :star::star::star::star:\n\n**[4]** Girshick, Ross. \"**Fast r-cnn**.\" Proceedings of the IEEE International Conference on Computer Vision. 2015. [[pdf]](https://pdfs.semanticscholar.org/8f67/64a59f0d17081f2a2a9d06f4ed1cdea1a0ad.pdf) :star::star::star::star:\n\n**[5]** Ren, Shaoqing, et al. \"**Faster R-CNN: Towards real-time object detection with region proposal networks**.\" Advances in neural information processing systems. 2015. [[pdf]](https://arxiv.org/pdf/1506.01497.pdf) :star::star::star::star:\n\n**[6]** Redmon, Joseph, et al. \"**You only look once: Unified, real-time object detection**.\" arXiv preprint arXiv:1506.02640 (2015). [[pdf]](http://homes.cs.washington.edu/~ali/papers/YOLO.pdf) **(YOLO,Oustanding Work, really practical)** :star::star::star::star::star:\n\n**[7]** Liu, Wei, et al. \"**SSD: Single Shot MultiBox Detector**.\" arXiv preprint arXiv:1512.02325 (2015). [[pdf]](http://arxiv.org/pdf/1512.02325) :star::star::star:\n\n**[8]** Dai, Jifeng, et al. \"**R-FCN: Object Detection via\nRegion-based Fully Convolutional Networks**.\" arXiv preprint arXiv:1605.06409 (2016). [[pdf]](https://arxiv.org/abs/1605.06409) :star::star::star::star:\n\n**[9]** He, Gkioxari, et al. \"**Mask R-CNN**\" arXiv preprint arXiv:1703.06870 (2017). [[pdf]](https://arxiv.org/abs/1703.06870) :star::star::star::star:\n## 3.3 Visual Tracking\n\n**[1]** Wang, Naiyan, and Dit-Yan Yeung. \"**Learning a deep compact image representation for visual tracking**.\" Advances in neural information processing systems. 2013. [[pdf]](http://papers.nips.cc/paper/5192-learning-a-deep-compact-image-representation-for-visual-tracking.pdf) **(First Paper to do visual tracking using Deep Learning,DLT Tracker)** :star::star::star:\n\n**[2]** Wang, Naiyan, et al. \"**Transferring rich feature hierarchies for robust visual tracking**.\" arXiv preprint arXiv:1501.04587 (2015). [[pdf]](http://arxiv.org/pdf/1501.04587) **(SO-DLT)** :star::star::star::star:\n\n**[3]** Wang, Lijun, et al. \"**Visual tracking with fully convolutional networks**.\" Proceedings of the IEEE International Conference on Computer Vision. 2015. [[pdf]](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Wang_Visual_Tracking_With_ICCV_2015_paper.pdf) **(FCNT)** :star::star::star::star:\n\n**[4]** Held, David, Sebastian Thrun, and Silvio Savarese. \"**Learning to Track at 100 FPS with Deep Regression Networks**.\" arXiv preprint arXiv:1604.01802 (2016). [[pdf]](http://arxiv.org/pdf/1604.01802) **(GOTURN,Really fast as a deep learning method,but still far behind un-deep-learning methods)** :star::star::star::star:\n\n**[5]** Bertinetto, Luca, et al. \"**Fully-Convolutional Siamese Networks for Object Tracking**.\" arXiv preprint arXiv:1606.09549 (2016). [[pdf]](https://arxiv.org/pdf/1606.09549) **(SiameseFC,New state-of-the-art for real-time object tracking)** :star::star::star::star:\n\n**[6]** Martin Danelljan, Andreas Robinson, Fahad Khan, Michael Felsberg. \"**Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking**.\" ECCV (2016) [[pdf]](http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/C-COT_ECCV16.pdf) **(C-COT)** :star::star::star::star:\n\n**[7]** Nam, Hyeonseob, Mooyeol Baek, and Bohyung Han. \"**Modeling and Propagating CNNs in a Tree Structure for Visual Tracking**.\" arXiv preprint arXiv:1608.07242 (2016). [[pdf]](https://arxiv.org/pdf/1608.07242) **(VOT2016 Winner,TCNN)** :star::star::star::star:\n\n## 3.4 Image Caption\n**[1]** Farhadi,Ali,etal. \"**Every picture tells a story: Generating sentences from images**\". In Computer VisionECCV 2010. Springer Berlin Heidelberg:15-29, 2010. [[pdf]](https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf) :star::star::star:\n\n**[2]** Kulkarni, Girish, et al. \"**Baby talk: Understanding and generating image descriptions**\". In Proceedings of the 24th CVPR, 2011. [[pdf]](http://tamaraberg.com/papers/generation_cvpr11.pdf):star::star::star::star:\n\n**[3]** Vinyals, Oriol, et al. \"**Show and tell: A neural image caption generator**\". In arXiv preprint arXiv:1411.4555, 2014. [[pdf]](https://arxiv.org/pdf/1411.4555.pdf):star::star::star:\n\n**[4]** Donahue, Jeff, et al. \"**Long-term recurrent convolutional networks for visual recognition and description**\". In arXiv preprint arXiv:1411.4389 ,2014. [[pdf]](https://arxiv.org/pdf/1411.4389.pdf)\n\n**[5]** Karpathy, Andrej, and Li Fei-Fei. \"**Deep visual-semantic alignments for generating image descriptions**\". In arXiv preprint arXiv:1412.2306, 2014. [[pdf]](https://cs.stanford.edu/people/karpathy/cvpr2015.pdf):star::star::star::star::star:\n\n**[6]** Karpathy, Andrej, Armand Joulin, and Fei Fei F. Li. \"**Deep fragment embeddings for bidirectional image sentence mapping**\". In Advances in neural information processing systems, 2014. [[pdf]](https://arxiv.org/pdf/1406.5679v1.pdf):star::star::star::star:\n\n**[7]** Fang, Hao, et al. \"**From captions to visual concepts and back**\". In arXiv preprint arXiv:1411.4952, 2014. [[pdf]](https://arxiv.org/pdf/1411.4952v3.pdf):star::star::star::star::star:\n\n**[8]** Chen, Xinlei, and C. Lawrence Zitnick. \"**Learning a recurrent visual representation for image caption generation**\". In arXiv preprint arXiv:1411.5654, 2014. [[pdf]](https://arxiv.org/pdf/1411.5654v1.pdf):star::star::star::star:\n\n**[9]** Mao, Junhua, et al. \"**Deep captioning with multimodal recurrent neural networks (m-rnn)**\". In arXiv preprint arXiv:1412.6632, 2014. [[pdf]](https://arxiv.org/pdf/1412.6632v5.pdf):star::star::star:\n\n**[10]** Xu, Kelvin, et al. \"**Show, attend and tell: Neural image caption generation with visual attention**\". In arXiv preprint arXiv:1502.03044, 2015. [[pdf]](https://arxiv.org/pdf/1502.03044v3.pdf):star::star::star::star::star:\n\n## 3.5 Machine Translation\n\n\u003e Some milestone papers are listed in RNN / Seq-to-Seq topic.\n\n**[1]** Luong, Minh-Thang, et al. \"**Addressing the rare word problem in neural machine translation**.\" arXiv preprint arXiv:1410.8206 (2014). [[pdf]](http://arxiv.org/pdf/1410.8206) :star::star::star::star:\n\n\n**[2]** Sennrich, et al. \"**Neural Machine Translation of Rare Words with Subword Units**\". In arXiv preprint arXiv:1508.07909, 2015. [[pdf]](https://arxiv.org/pdf/1508.07909.pdf):star::star::star:\n\n**[3]** Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. \"**Effective approaches to attention-based neural machine translation**.\" arXiv preprint arXiv:1508.04025 (2015). [[pdf]](http://arxiv.org/pdf/1508.04025) :star::star::star::star:\n\n**[4]** Chung, et al. \"**A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation**\". In arXiv preprint arXiv:1603.06147, 2016. [[pdf]](https://arxiv.org/pdf/1603.06147.pdf):star::star:\n\n**[5]** Lee, et al. \"**Fully Character-Level Neural Machine Translation without Explicit Segmentation**\". In arXiv preprint arXiv:1610.03017, 2016. [[pdf]](https://arxiv.org/pdf/1610.03017.pdf):star::star::star::star::star:\n\n**[6]** Wu, Schuster, Chen, Le, et al. \"**Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation**\". In arXiv preprint arXiv:1609.08144v2, 2016. [[pdf]](https://arxiv.org/pdf/1609.08144v2.pdf) **(Milestone)** :star::star::star::star:\n\n## 3.6 Robotics\n\n**[1]** Koutník, Jan, et al. \"**Evolving large-scale neural networks for vision-based reinforcement learning**.\" Proceedings of the 15th annual conference on Genetic and evolutionary computation. ACM, 2013. [[pdf]](http://repository.supsi.ch/4550/1/koutnik2013gecco.pdf) :star::star::star:\n\n**[2]** Levine, Sergey, et al. \"**End-to-end training of deep visuomotor policies**.\" Journal of Machine Learning Research 17.39 (2016): 1-40. [[pdf]](http://www.jmlr.org/papers/volume17/15-522/15-522.pdf) :star::star::star::star::star:\n\n**[3]** Pinto, Lerrel, and Abhinav Gupta. \"**Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours**.\" arXiv preprint arXiv:1509.06825 (2015). [[pdf]](http://arxiv.org/pdf/1509.06825) :star::star::star:\n\n**[4]** Levine, Sergey, et al. \"**Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection**.\" arXiv preprint arXiv:1603.02199 (2016). [[pdf]](http://arxiv.org/pdf/1603.02199) :star::star::star::star:\n\n**[5]** Zhu, Yuke, et al. \"**Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning**.\" arXiv preprint arXiv:1609.05143 (2016). [[pdf]](https://arxiv.org/pdf/1609.05143) :star::star::star::star:\n\n**[6]** Yahya, Ali, et al. \"**Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search**.\" arXiv preprint arXiv:1610.00673 (2016). [[pdf]](https://arxiv.org/pdf/1610.00673) :star::star::star::star:\n\n**[7]** Gu, Shixiang, et al. \"**Deep Reinforcement Learning for Robotic Manipulation**.\" arXiv preprint arXiv:1610.00633 (2016). [[pdf]](https://arxiv.org/pdf/1610.00633) :star::star::star::star:\n\n**[8]** A Rusu, M Vecerik, Thomas Rothörl, N Heess, R Pascanu, R Hadsell.\"**Sim-to-Real Robot Learning from Pixels with Progressive Nets**.\" arXiv preprint arXiv:1610.04286 (2016). [[pdf]](https://arxiv.org/pdf/1610.04286.pdf) :star::star::star::star:\n\n**[9]** Mirowski, Piotr, et al. \"**Learning to navigate in complex environments**.\" arXiv preprint arXiv:1611.03673 (2016). [[pdf]](https://arxiv.org/pdf/1611.03673) :star::star::star::star:\n\n## 3.7 Art\n\n**[1]** Mordvintsev, Alexander; Olah, Christopher; Tyka, Mike (2015). \"**Inceptionism: Going Deeper into Neural Networks**\". Google Research. [[html]](https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html) **(Deep Dream)**\n:star::star::star::star:\n\n**[2]** Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. \"**A neural algorithm of artistic style**.\" arXiv preprint arXiv:1508.06576 (2015). [[pdf]](http://arxiv.org/pdf/1508.06576) **(Outstanding Work, most successful method currently)** :star::star::star::star::star:\n\n**[3]** Zhu, Jun-Yan, et al. \"**Generative Visual Manipulation on the Natural Image Manifold**.\" European Conference on Computer Vision. Springer International Publishing, 2016. [[pdf]](https://arxiv.org/pdf/1609.03552) **(iGAN)** :star::star::star::star:\n\n**[4]** Champandard, Alex J. \"**Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks**.\" arXiv preprint arXiv:1603.01768 (2016). [[pdf]](http://arxiv.org/pdf/1603.01768) **(Neural Doodle)** :star::star::star::star:\n\n**[5]** Zhang, Richard, Phillip Isola, and Alexei A. Efros. \"**Colorful Image Colorization**.\" arXiv preprint arXiv:1603.08511 (2016). [[pdf]](http://arxiv.org/pdf/1603.08511) :star::star::star::star:\n\n**[6]** Johnson, Justin, Alexandre Alahi, and Li Fei-Fei. \"**Perceptual losses for real-time style transfer and super-resolution**.\" arXiv preprint arXiv:1603.08155 (2016). [[pdf]](https://arxiv.org/pdf/1603.08155.pdf) :star::star::star::star:\n\n**[7]** Vincent Dumoulin, Jonathon Shlens and Manjunath Kudlur. \"**A learned representation for artistic style**.\" arXiv preprint arXiv:1610.07629 (2016). [[pdf]](https://arxiv.org/pdf/1610.07629v1.pdf) :star::star::star::star:\n\n**[8]** Gatys, Leon and Ecker, et al.\"**Controlling Perceptual Factors in Neural Style Transfer**.\" arXiv preprint arXiv:1611.07865 (2016). [[pdf]](https://arxiv.org/pdf/1611.07865.pdf) **(control style transfer over spatial location,colour information and across spatial scale)**:star::star::star::star:\n\n**[9]** Ulyanov, Dmitry and Lebedev, Vadim, et al. \"**Texture Networks: Feed-forward Synthesis of Textures and Stylized Images**.\" arXiv preprint arXiv:1603.03417(2016). [[pdf]](http://arxiv.org/abs/1603.03417) **(texture generation and style transfer)** :star::star::star::star:\n\n\n## 3.8 Object Segmentation\n\n**[1]** J. Long, E. Shelhamer, and T. Darrell, “**Fully convolutional networks for semantic segmentation**.” in CVPR, 2015. [[pdf]](https://arxiv.org/pdf/1411.4038v2.pdf) :star::star::star::star::star:\n\n**[2]** L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. \"**Semantic image segmentation with deep convolutional nets and fully connected crfs**.\" In ICLR, 2015. [[pdf]](https://arxiv.org/pdf/1606.00915v1.pdf) :star::star::star::star::star:\n\n**[3]** Pinheiro, P.O., Collobert, R., Dollar, P. \"**Learning to segment object candidates.**\" In: NIPS. 2015. [[pdf]](https://arxiv.org/pdf/1506.06204v2.pdf) :star::star::star::star:\n\n**[4]** Dai, J., He, K., Sun, J. \"**Instance-aware semantic segmentation via multi-task network cascades**.\" in CVPR. 2016 [[pdf]](https://arxiv.org/pdf/1512.04412v1.pdf) :star::star::star:\n\n**[5]** Dai, J., He, K., Sun, J. \"**Instance-sensitive Fully Convolutional Networks**.\" arXiv preprint arXiv:1603.08678 (2016). [[pdf]](https://arxiv.org/pdf/1603.08678v1.pdf) :star::star::star:\n\n\n"
  },
  {
    "repo": "tldr-pages/tldr",
    "content": "# tldr\n\n[![Build status][travis-image]][travis-url]\n[![Gitter chat][gitter-image]][gitter-url]\n[![Merged PRs][prs-merged-image]][prs-merged-url]\n[![Issue stats][issuestats-image]][issuestats-url]\n[![GitHub contributors][contributors-image]][contributors-url]\n[![Number of files][tokei-image]][tokei-url]\n[![license][license-image]][license-url]\n\n[travis-url]: https://travis-ci.org/tldr-pages/tldr/builds\n[travis-image]: https://travis-ci.org/tldr-pages/tldr.svg?branch=master\n[gitter-url]: https://gitter.im/tldr-pages/tldr\n[gitter-image]: https://badges.gitter.im/tldr-pages/tldr.svg\n[prs-merged-url]: https://github.com/tldr-pages/tldr/pulls?q=is:pr+is:merged\n[prs-merged-image]: https://img.shields.io/github/issues-pr-closed-raw/tldr-pages/tldr.svg?label=merged+PRs\n[issuestats-url]: http://isitmaintained.com/project/tldr-pages/tldr\n[issuestats-image]: http://isitmaintained.com/badge/resolution/tldr-pages/tldr.svg\n[contributors-url]: https://github.com/tldr-pages/tldr/graphs/contributors\n[contributors-image]: https://img.shields.io/github/contributors/tldr-pages/tldr.svg\n[tokei-url]: https://github.com/tldr-pages/tldr/tree/master/pages\n[tokei-image]: https://tokei.rs/b1/github/tldr-pages/tldr?category=files\n[license-url]: https://github.com/tldr-pages/tldr/blob/master/LICENSE.md\n[license-image]: https://img.shields.io/github/license/tldr-pages/tldr.svg\n\nA collection of simplified and community-driven man pages.\n\nInstall it with `npm install -g tldr`\nor [try the web client](http://tldr.ostera.io).\n\n## What is tldr?\n\nNew to the command-line world? Or just a little rusty?\nOr perhaps you can't always remember the arguments to `lsof`, or `tar`?\n\nMaybe it doesn't help that the first option explained in `man tar` is:\n\n```\n-b blocksize\n   Specify the block size, in 512-byte records, for tape drive I/O.\n   As a rule, this argument is only needed when reading from or writing to tape drives,\n   and usually not even then as the default block size of 20 records (10240 bytes) is very common.\n```\n\nSurely people could benefit from simplified man pages\nfocused on practical examples.\nHow about:\n\n![tldr screenshot](screenshot.png)\n\nThis repository is just that: an ever-growing collection of examples\nfor the most common UNIX / Linux / macOS / SunOS commands.\n\n## Clients\n\nYou can access these pages on your computer using one of the following clients:\n\n- [Alfred Workflow](https://github.com/cs1707/tldr-alfred)\n- Android clients:\n  - [tldr-viewer](https://github.com/gianasista/tldr-viewer), available on\n    [Google Play](https://play.google.com/store/apps/details?id=de.gianasista.tldr_viewer)\n  - [tldroid](https://github.com/hidroh/tldroid), available on\n    [Google Play](https://play.google.com/store/apps/details?id=io.github.hidroh.tldroid)\n- Bash clients:\n  - [tldr](https://github.com/raylee/tldr)\n  - [tldr-bash-client](https://gitlab.com/pepa65/tldr-bash-client)\n- [C# client](https://github.com/principis/tldr-sharp)\n- [C++ client](https://github.com/tldr-pages/tldr-cpp-client):\n  `brew install tldr`\n- [Chrome Extension](https://github.com/hill/tldr-chrome) available on\n  [Chrome Web Store](https://chrome.google.com/webstore/detail/tldr-chrome/nnmlddkpgoecicoallmimonoboialpap)\n- [Crystal client](https://github.com/porras/tlcr):\n  `brew install porras/tap/tlcr`\n- [Dart client](https://github.com/hterkelsen/tldr):\n  `pub global activate tldr`\n- [Dash docset](https://github.com/Moddus/tldr-python-dash-docset):\n  Open `Preferences \u003e Downloads \u003e User Contributed` and find `tldr pages` in the list\n- Docker images:\n    - [tldr-docker](https://github.com/nutellinoit/tldr-docker)- Run the `tldr` command via a docker container: `alias tldr='docker run --rm -it -v ~/.tldr/:/root/.tldr/ nutellinoit/tldr'`\n- [Elixir client](https://github.com/edgurgel/tldr_elixir_client)\n  (binaries not yet available)\n- [Emacs client](https://github.com/kuanyui/tldr.el), available on\n  [MELPA](https://github.com/melpa/melpa)\n- Go clients:\n  - [github.com/pranavraja/tldr](https://github.com/pranavraja/tldr):\n    `go get github.com/pranavraja/tldr`\n    (or [platform binaries](https://github.com/pranavraja/tldr/releases))\n  - [4d63.com/tldr](https://4d63.com/tldr):\n    `go get 4d63.com/tldr` or `brew install 4d63/tldr/tldr`\n    (or [platform binaries](https://github.com/leighmcculloch/tldr/releases))\n- iOS clients:\n  - [tldr-man-page](https://github.com/freesuraj/TLDR), available on\n    [App Store](https://appsto.re/sg/IQ0-_.i)\n  - [tldr-pages](https://github.com/mflint/ios-tldr-viewer), available on\n    [App Store](https://itunes.apple.com/us/app/tldt-pages/id1071725095?ls=1\u0026mt=8)\n- [Haskell client](https://github.com/psibi/tldr-hs):\n  `stack install tldr`\n- [Node.js client](https://github.com/tldr-pages/tldr-node-client):\n  `npm install -g tldr`\n- [OCaml client](https://github.com/RosalesJ/tldr-ocaml): `opam install tldr`\n- [Perl5 client](https://github.com/shoichikaji/perl-tldr):\n  `cpanm App::tldr`\n- [PHP client](https://github.com/BrainMaestro/tldr-php):\n  `composer global require brainmaestro/tldr`\n- Python clients:\n  - [tldr-python-client](https://github.com/tldr-pages/tldr-python-client):\n    `pip install tldr`\n  - [tldr.py](https://github.com/lord63/tldr.py):\n    `pip install tldr.py`\n- [R client](https://github.com/kirillseva/tldrrr):\n  `devtools::install_github('kirillseva/tldrrr')`\n- [Ruby client](https://github.com/YellowApple/tldrb):\n  `gem install tldrb`\n- Rust clients:\n    - [rust-tldr](https://github.com/rilut/rust-tldr)\n      (thin client with online lookup):\n      `cargo install tldr`\n    - [tealdeer](https://github.com/dbrgn/tealdeer)\n      (fully featured client with offline cache):\n      `cargo install tealdeer`\n- Web clients:\n    - [tldr.jsx](https://github.com/ostera/tldr.jsx): http://tldr.ostera.io/\n    - [DistroWatch](https://distrowatch.com/dwres.php?resource=man-pages)\n\nThere is also a comprehensive\n[list of clients in our Wiki](https://github.com/tldr-pages/tldr/wiki/TLDR-clients).\n\n## Contributing\n\n- Your favourite command isn't covered?\n- You can think of more examples for an existing command?\n\nContributions are most welcome!\nWe strive to maintain a [welcoming and collaborative](GOVERNANCE.md) community.\nHave a look at the [contributing guidelines](CONTRIBUTING.md), and go ahead!\n\n## Similar projects\n\n- [Cheat](https://github.com/chrisallenlane/cheat)\n  allows you to create and view interactive cheatsheets on the command-line.\n  It was designed to help remind *nix system administrators of options\n  for commands that they use frequently, but not frequently enough to remember.\n\n- [Bro pages](http://bropages.org/)\n  are a highly readable supplement to man pages.\n  Bro pages show concise, common-case examples for Unix commands.\n  The examples are submitted by the user base, and can be voted up or down;\n  the best entries are what people see first when they look up a command.\n\n- [eg](https://github.com/srsudar/eg)\n  provides detailed examples with explanations on the command line.\n  Examples come from the repository, but `eg` supports displaying\n  custom examples and commands alongside the defaults.\n\n## What does \"tldr\" mean?\n\nTL;DR stands for \"Too Long; Didn't Read\".\nIt originates in Internet slang, where it is used to indicate that a long text\n(or parts of it) has been skipped as too lengthy.\nRead more in Wikipedia's [TL;DR article](https://en.wikipedia.org/wiki/TL;DR).\n"
  },
  {
    "repo": "TheAlgorithms/Python",
    "content": "# The Algorithms - Python \u003c!-- [![Build Status](https://travis-ci.org/TheAlgorithms/Python.svg)](https://travis-ci.org/TheAlgorithms/Python) --\u003e\n\n### All algorithms implemented in Python (for education)\n\nThese implementations are for demonstration purposes. They are less efficient than the implementations in the Python standard library.\n\n## Sorting Algorithms\n\n\n### Bubble Sort\n![alt text][bubble-image]\n\nFrom [Wikipedia][bubble-wiki]: **Bubble sort**, sometimes referred to as *sinking sort*, is a simple sorting algorithm that repeatedly steps through the list, compares adjacent pairs and swaps them if they are in the wrong order. The pass through the list is repeated until no swaps are needed, which indicates that the list is sorted.\n\n__Properties__\n* Worst case performance\tO(n\u003csup\u003e2\u003c/sup\u003e)\n* Best case performance\tO(n)\n* Average case performance\tO(n\u003csup\u003e2\u003c/sup\u003e)\n\n###### View the algorithm in [action][bubble-toptal]\n\n### Bucket Sort\n![alt text][bucket-image-1]\n![alt text][bucket-image-2]\n\nFrom [Wikipedia][bucket-wiki]: Bucket sort, or bin sort, is a sorting algorithm that distributes elements of an array into a number of buckets. Each bucket is then sorted individually, either using a different sorting algorithm, or by recursively applying the bucket sorting algorithm.\n\n__Properties__\n* Worst case performance\tO(n\u003csup\u003e2\u003c/sup\u003e)\n* Best case performance\tO(n+k)\n* Average case performance\tO(n+k)\n\n### Cocktail shaker\n![alt text][cocktail-shaker-image]\n\nFrom [Wikipedia][cocktail-shaker-wiki]: Cocktail shaker sort, also known as bidirectional bubble sort, cocktail sort, shaker sort (which can also refer to a variant of selection sort), ripple sort, shuffle sort, or shuttle sort, is a variation of bubble sort that is both a stable sorting algorithm and a comparison sort. The algorithm differs from a bubble sort in that it sorts in both directions on each pass through the list.\n\n__Properties__\n* Worst case performance\tO(n\u003csup\u003e2\u003c/sup\u003e)\n* Best case performance\tO(n)\n* Average case performance\tO(n\u003csup\u003e2\u003c/sup\u003e)\n\n\n### Insertion Sort\n![alt text][insertion-image]\n\nFrom [Wikipedia][insertion-wiki]: **Insertion sort** is a simple sorting algorithm that builds the final sorted array (or list) one item at a time. It is much less efficient on *large* lists than more advanced algorithms such as quicksort, heapsort, or merge sort.\n\n__Properties__\n* Worst case performance\tO(n\u003csup\u003e2\u003c/sup\u003e)\n* Best case performance\tO(n)\n* Average case performance\tO(n\u003csup\u003e2\u003c/sup\u003e)\n\n###### View the algorithm in [action][insertion-toptal]\n\n\n### Merge Sort\n![alt text][merge-image]\n\nFrom [Wikipedia][merge-wiki]: **Merge sort** (also commonly spelled *mergesort*) is an efficient, general-purpose, comparison-based sorting algorithm. Most implementations produce a stable sort, which means the order of equal items is the same in the input and output. Mergesort is a divide and conquer algorithm that was invented by John von Neumann in 1945.\n\n__Properties__\n* Worst case performance\tO(n log n)\n* Best case performance\tO(n log n)\n* Average case performance\tO(n log n)\n\n\n###### View the algorithm in [action][merge-toptal]\n\n### Quick Sort\n![alt text][quick-image]\n\nFrom [Wikipedia][quick-wiki]: **Quicksort** (sometimes called *partition-exchange sort*) is an efficient sorting algorithm, serving as a systematic method for placing the elements of an array in order.\n\n__Properties__\n* Worst case performance\tO(n\u003csup\u003e2\u003c/sup\u003e)\n* Best case performance\tO(*n* log *n*) or O(n) with three-way partition\n* Average case performance\tO(*n* log *n*)\n\n###### View the algorithm in [action][quick-toptal]\n\n\n### Heap\n\nFrom [Wikipedia](https://en.wikipedia.org/wiki/Heapsort): Heapsort is a comparison-based sorting algorithm. It can be thought of as an improved selection sort. It divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element and moving that to the sorted region\n\n__Properties__\n* Worst case performance\tO(*n* log *n*)\n* Best case performance\tO(*n* log *n*)\n* Average case performance\tO(*n* log *n*)\n\n\n\n###### View the algorithm in [action](https://www.toptal.com/developers/sorting-algorithms/heap-sort)\n\n### Radix\n\nFrom [Wikipedia][radix-wiki]: Radix sort is a non-comparative integer sorting algorithm that sorts data with integer keys by grouping keys by the individual digits which share the same significant position and value.\n\n__Properties__\n* Worst case performance\tO(wn)\n* Best case performance\tO(wn)\n* Average case performance\tO(wn)\n\n### Selection\n![alt text][selection-image]\n\nFrom [Wikipedia][selection-wiki]: **Selection sort** is an algorithm that divides the input list into two parts: the sublist of items already sorted, which is built up from left to right at the front (left) of the list, and the sublist of items remaining to be sorted that occupy the rest of the list. Initially, the sorted sublist is empty and the unsorted sublist is the entire input list. The algorithm proceeds by finding the smallest (or largest, depending on sorting order) element in the unsorted sublist, exchanging (swapping) it with the leftmost unsorted element (putting it in sorted order), and moving the sublist boundaries one element to the right.\n\n__Properties__\n* Worst case performance\tO(n\u003csup\u003e2\u003c/sup\u003e)\n* Best case performance\tO(n\u003csup\u003e2\u003c/sup\u003e)\n* Average case performance\tO(n\u003csup\u003e2\u003c/sup\u003e)\n\n###### View the algorithm in [action][selection-toptal]\n\n### Shell\n![alt text][shell-image]\n\nFrom [Wikipedia][shell-wiki]: **Shellsort** is a generalization of *insertion sort* that allows the exchange of items that are far apart.  The idea is to arrange the list of elements so that, starting anywhere, considering every nth element gives a sorted list.  Such a list is said to be h-sorted.  Equivalently, it can be thought of as h interleaved lists, each individually sorted.\n\n__Properties__\n* Worst case performance O(*n*log\u003csup\u003e2\u003c/sup\u003e*n*)\n* Best case performance O(*n* log *n*)\n* Average case performance depends on gap sequence\n\n###### View the algorithm in [action][shell-toptal]\n\n### Topological\n\nFrom [Wikipedia][topological-wiki]: A topological sort or topological ordering of a directed graph is a linear ordering of its vertices such that for every directed edge uv from vertex u to vertex v, u comes before v in the ordering. For instance, the vertices of the graph may represent tasks to be performed, and the edges may represent constraints that one task must be performed before another; in this application, a topological ordering is just a valid sequence for the tasks. A topological ordering is possible if and only if the graph has no directed cycles, that is, if it is a directed acyclic graph (DAG). Any DAG has at least one topological ordering, and algorithms are known for constructing a topological ordering of any DAG in linear time.\n\n### Time-Complexity Graphs\n\nComparing the complexity of sorting algorithms (*Bubble Sort*, *Insertion Sort*, *Selection Sort*)\n\n![Complexity Graphs](https://github.com/prateekiiest/Python/blob/master/sorts/sortinggraphs.png)\n\nSelecting a sort technique: Quicksort is a very fast algorithm but can be pretty tricky to implement while bubble sort is a slow algorithm which is very easy to implement. For a small datasets bubble sort may be a better option since it can be implemented quickly, but for larger datasets, the speedup from quicksort might be worth the trouble implementing the algorithm.\n\n\n\n----------------------------------------------------------------------------------\n\n## Search Algorithms\n\n### Linear\n![alt text][linear-image]\n\nFrom [Wikipedia][linear-wiki]: **Linear search** or *sequential search* is a method for finding an element in a list. It sequentially checks each element of the list until a match is found or all the elements have been searched.\n\n__Properties__\n* Worst case performance\tO(n)\n* Best case performance\tO(1)\n* Average case performance\tO(n)\n* Worst case space complexity\tO(1) iterative\n\n### Binary\n![alt text][binary-image]\n\nFrom [Wikipedia][binary-wiki]: **Binary search**, also known as *half-interval search* or *logarithmic search*, is a search algorithm that finds the position of a target value within a sorted array. It compares the target value to the middle element of the array; if they are unequal, the half in which the target cannot lie is eliminated and the search continues on the remaining half until it is successful.\n\n__Properties__\n* Worst case performance\tO(log n)\n* Best case performance\tO(1)\n* Average case performance\tO(log n)\n* Worst case space complexity\tO(1)\n\n## Interpolation\nInterpolation search is an algorithm for searching for a key in an array that has been ordered by numerical values assigned to the keys (key values). It was first described by W. W. Peterson in 1957. Interpolation search resembles the method by which people search a telephone directory for a name (the key value by which the book's entries are ordered): in each step the algorithm calculates where in the remaining search space the sought item might be, based on the key values at the bounds of the search space and the value of the sought key, usually via a linear interpolation. The key value actually found at this estimated position is then compared to the key value being sought. If it is not equal, then depending on the comparison, the remaining search space is reduced to the part before or after the estimated position. This method will only work if calculations on the size of differences between key values are sensible.\n\nBy comparison, binary search always chooses the middle of the remaining search space, discarding one half or the other, depending on the comparison between the key found at the estimated position and the key sought — it does not require numerical values for the keys, just a total order on them. The remaining search space is reduced to the part before or after the estimated position. The linear search uses equality only as it compares elements one-by-one from the start, ignoring any sorting.\n\nOn average the interpolation search makes about log(log(n)) comparisons (if the elements are uniformly distributed), where n is the number of elements to be searched. In the worst case (for instance where the numerical values of the keys increase exponentially) it can make up to O(n) comparisons.\n\nIn interpolation-sequential search, interpolation is used to find an item near the one being searched for, then linear search is used to find the exact item.\n###### Source: [Wikipedia](https://en.wikipedia.org/wiki/Interpolation_search)\n\n## Jump Search\n![alt text][JumpSearch-image]\nIn computer science, a jump search or block search refers to a search algorithm for ordered lists. It works by first checking all items L\u003csub\u003ekm\u003c/sub\u003e, where ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/2a5bc4b7383031ba693b7433198ead7170954c1d)  and *m* is the block size, until an item is found that is larger than the search key. To find the exact position of the search key in the list a linear search is performed on the sublist L\u003csub\u003e[(k-1)m, km]\u003c/sub\u003e.\n\nThe optimal value of m is √n, where n is the length of the list L. Because both steps of the algorithm look at, at most, √n items the algorithm runs in O(√n) time. This is better than a linear search, but worse than a binary search. The advantage over the latter is that a jump search only needs to jump backwards once, while a binary can jump backwards up to log n times. This can be important if a jumping backwards takes significantly more time than jumping forward.\n\nThe algorithm can be modified by performing multiple levels of jump search on the sublists, before finally performing the linear search. For an k-level jump search the optimum block size m\u003csub\u003el\u003c/sub\u003e for the l\u003csup\u003eth\u003c/sup\u003e level (counting from 1) is n\u003csup\u003e(k-l)/k\u003c/sup\u003e. The modified algorithm will perform *k* backward jumps and runs in O(kn\u003csup\u003e1/(k+1)\u003c/sup\u003e) time.\n###### Source: [Wikipedia](https://en.wikipedia.org/wiki/Jump_search)\n\n## Quick Select\n![alt text][QuickSelect-image]\nIn computer science, quickselect is a selection algorithm to find the kth smallest element in an unordered list. It is related to the quicksort sorting algorithm. Like quicksort, it was developed by Tony Hoare, and thus is also known as Hoare's selection algorithm. Like quicksort, it is efficient in practice and has good average-case performance, but has poor worst-case performance. Quickselect and its variants are the selection algorithms most often used in efficient real-world implementations.\n\nQuickselect uses the same overall approach as quicksort, choosing one element as a pivot and partitioning the data in two based on the pivot, accordingly as less than or greater than the pivot. However, instead of recursing into both sides, as in quicksort, quickselect only recurses into one side – the side with the element it is searching for. This reduces the average complexity from O(n log n) to O(n), with a worst case of O(n\u003csup\u003e2\u003c/sup\u003e).\n\nAs with quicksort, quickselect is generally implemented as an in-place algorithm, and beyond selecting the k'th element, it also partially sorts the data. See selection algorithm for further discussion of the connection with sorting.\n###### Source: [Wikipedia](https://en.wikipedia.org/wiki/Quickselect)\n\n## Tabu\nTabu search uses a local or neighborhood search procedure to iteratively move from one potential solution ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4) to an improved solution ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/0ac74959896052e160a5953102e4bc3850fe93b2) in the neighborhood of ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4), until some stopping criterion has been satisfied (generally, an attempt limit or a score threshold). Local search procedures often become stuck in poor-scoring areas or areas where scores plateau. In order to avoid these pitfalls and explore regions of the search space that would be left unexplored by other local search procedures, tabu search carefully explores the neighborhood of each solution as the search progresses. The solutions admitted to the new neighborhood, ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/4db1b4a2cfa6f356afe0738e999f0af2bed27f45), are determined through the use of memory structures. Using these memory structures, the search progresses by iteratively moving from the current solution ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4) to an improved solution ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/0ac74959896052e160a5953102e4bc3850fe93b2) in ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/4db1b4a2cfa6f356afe0738e999f0af2bed27f45).\n\nThese memory structures form what is known as the tabu list, a set of rules and banned solutions used to filter which solutions will be admitted to the neighborhood ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/4db1b4a2cfa6f356afe0738e999f0af2bed27f45) to be explored by the search. In its simplest form, a tabu list is a short-term set of the solutions that have been visited in the recent past (less than ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b) iterations ago, where ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b) is the number of previous solutions to be stored — is also called the tabu tenure). More commonly, a tabu list consists of solutions that have changed by the process of moving from one solution to another. It is convenient, for ease of description, to understand a “solution” to be coded and represented by such attributes.\n###### Source: [Wikipedia](https://en.wikipedia.org/wiki/Tabu_search)\n----------------------------------------------------------------------------------------------------------------------\n\n## Ciphers\n\n### Caesar\n![alt text][caesar]\u003cbr\u003e\n**Caesar cipher**, also known as Caesar's cipher, the shift cipher, Caesar's code or Caesar shift, is one of the simplest and most widely known encryption techniques.\u003cbr\u003e\nIt is **a type of substitution cipher** in which each letter in the plaintext is replaced by a letter some fixed number of positions down the alphabet. For example, with a left shift of 3, D would be replaced by A, E would become B, and so on. \u003cbr\u003e\nThe method is named after **Julius Caesar**, who used it in his private correspondence.\u003cbr\u003e\nThe encryption step performed by a Caesar cipher is often incorporated as part of more complex schemes, such as the Vigenère cipher, and still has modern application in the ROT13 system. As with all single-alphabet substitution ciphers, the Caesar cipher is easily broken and in modern practice offers essentially no communication security.\n###### Source: [Wikipedia](https://en.wikipedia.org/wiki/Caesar_cipher)\n\n### Vigenère\nThe **Vigenère cipher** is a method of encrypting alphabetic text by using a series of **interwoven Caesar ciphers** based on the letters of a keyword. It is **a form of polyalphabetic substitution**.\u003cbr\u003e\nThe Vigenère cipher has been reinvented many times. The method was originally described by Giovan Battista Bellaso in his 1553 book La cifra del. Sig. Giovan Battista Bellaso; however, the scheme was later misattributed to Blaise de Vigenère in the 19th century, and is now widely known as the \"Vigenère cipher\".\u003cbr\u003e\nThough the cipher is easy to understand and implement, for three centuries it resisted all attempts to break it; this earned it the description **le chiffre indéchiffrable**(French for 'the indecipherable cipher').\nMany people have tried to implement encryption schemes that are essentially Vigenère ciphers. Friedrich Kasiski was the first to publish a general method of deciphering a Vigenère cipher in 1863.\n###### Source: [Wikipedia](https://en.wikipedia.org/wiki/Vigen%C3%A8re_cipher)\n\n### Transposition\n\nIn cryptography, a **transposition cipher** is a method of encryption by which the positions held by units of plaintext (which are commonly characters or groups of characters) are shifted according to a regular system, so that the ciphertext constitutes a permutation of the plaintext. That is, the order of the units is changed (the plaintext is reordered).\u003cbr\u003e\n\nMathematically a bijective function is used on the characters' positions to encrypt and an inverse function to decrypt.\n###### Source: [Wikipedia](https://en.wikipedia.org/wiki/Transposition_cipher)\n\n### RSA (Rivest–Shamir–Adleman)\nRSA (Rivest–Shamir–Adleman) is one of the first public-key cryptosystems and is widely used for secure data transmission. In such a cryptosystem, the encryption key is public and it is different from the decryption key which is kept secret (private). In RSA, this asymmetry is based on the practical difficulty of the factorization of the product of two large prime numbers, the \"factoring problem\". The acronym RSA is made of the initial letters of the surnames of Ron Rivest, Adi Shamir, and Leonard Adleman, who first publicly described the algorithm in 1978. Clifford Cocks, an English mathematician working for the British intelligence agency Government Communications Headquarters (GCHQ), had developed an equivalent system in 1973, but this was not declassified until 1997.\n\nA user of RSA creates and then publishes a public key based on two large prime numbers, along with an auxiliary value. The prime numbers must be kept secret. Anyone can use the public key to encrypt a message, but with currently published methods, and if the public key is large enough, only someone with knowledge of the prime numbers can decode the message feasibly. Breaking RSA encryption is known as the RSA problem. Whether it is as difficult as the factoring problem remains an open question.\n###### Source: [Wikipedia](https://en.wikipedia.org/wiki/RSA_(cryptosystem))\n\n## ROT13\n![alt text][ROT13-image]\nROT13 (\"rotate by 13 places\", sometimes hyphenated ROT-13) is a simple letter substitution cipher that replaces a letter with the 13th letter after it, in the alphabet. ROT13 is a special case of the Caesar cipher which was developed in ancient Rome.\n\nBecause there are 26 letters (2×13) in the basic Latin alphabet, ROT13 is its own inverse; that is, to undo ROT13, the same algorithm is applied, so the same action can be used for encoding and decoding. The algorithm provides virtually no cryptographic security, and is often cited as a canonical example of weak encryption.\n###### Source: [Wikipedia](https://en.wikipedia.org/wiki/ROT13)\n\n## XOR\nIn cryptography, the simple XOR cipher is a type of additive cipher, an encryption algorithm that operates according to the principles:\n\nA ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/8b16e2bdaefee9eed86d866e6eba3ac47c710f60)  0 = A,\nA ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/8b16e2bdaefee9eed86d866e6eba3ac47c710f60)  A = 0,\n(A ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/8b16e2bdaefee9eed86d866e6eba3ac47c710f60)  B) ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/8b16e2bdaefee9eed86d866e6eba3ac47c710f60)  C = A ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/8b16e2bdaefee9eed86d866e6eba3ac47c710f60)  (B ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/8b16e2bdaefee9eed86d866e6eba3ac47c710f60)  C),\n(B ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/8b16e2bdaefee9eed86d866e6eba3ac47c710f60)  A) ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/8b16e2bdaefee9eed86d866e6eba3ac47c710f60)  A = B ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/8b16e2bdaefee9eed86d866e6eba3ac47c710f60)  0 = B,\nwhere ![](https://wikimedia.org/api/rest_v1/media/math/render/svg/8b16e2bdaefee9eed86d866e6eba3ac47c710f60)  denotes the exclusive disjunction (XOR) operation. This operation is sometimes called modulus 2 addition (or subtraction, which is identical). With this logic, a string of text can be encrypted by applying the bitwise XOR operator to every character using a given key. To decrypt the output, merely reapplying the XOR function with the key will remove the cipher.\n###### Source: [Wikipedia](https://en.wikipedia.org/wiki/XOR_cipher)\n\n[bubble-toptal]: https://www.toptal.com/developers/sorting-algorithms/bubble-sort\n[bubble-wiki]: https://en.wikipedia.org/wiki/Bubble_sort\n[bubble-image]: https://upload.wikimedia.org/wikipedia/commons/thumb/8/83/Bubblesort-edited-color.svg/220px-Bubblesort-edited-color.svg.png \"Bubble Sort\"\n\n[bucket-wiki]: https://en.wikipedia.org/wiki/Bucket_sort\n[bucket-image-1]: https://upload.wikimedia.org/wikipedia/commons/thumb/6/61/Bucket_sort_1.svg/311px-Bucket_sort_1.svg.png \"Bucket Sort\"\n[bucket-image-2]: https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Bucket_sort_2.svg/311px-Bucket_sort_2.svg.png \"Bucket Sort\"\n\n[cocktail-shaker-wiki]: https://en.wikipedia.org/wiki/Cocktail_shaker_sort\n[cocktail-shaker-image]: https://upload.wikimedia.org/wikipedia/commons/e/ef/Sorting_shaker_sort_anim.gif \"Cocktail Shaker Sort\"\n\n[insertion-toptal]: https://www.toptal.com/developers/sorting-algorithms/insertion-sort\n[insertion-wiki]: https://en.wikipedia.org/wiki/Insertion_sort\n[insertion-image]: https://upload.wikimedia.org/wikipedia/commons/7/7e/Insertionsort-edited.png \"Insertion Sort\"\n\n[quick-toptal]: https://www.toptal.com/developers/sorting-algorithms/quick-sort\n[quick-wiki]: https://en.wikipedia.org/wiki/Quicksort\n[quick-image]: https://upload.wikimedia.org/wikipedia/commons/6/6a/Sorting_quicksort_anim.gif \"Quick Sort\"\n\n[radix-wiki]: https://en.wikipedia.org/wiki/Radix_sort\n\n[merge-toptal]: https://www.toptal.com/developers/sorting-algorithms/merge-sort\n[merge-wiki]: https://en.wikipedia.org/wiki/Merge_sort\n[merge-image]: https://upload.wikimedia.org/wikipedia/commons/c/cc/Merge-sort-example-300px.gif \"Merge Sort\"\n\n[selection-toptal]: https://www.toptal.com/developers/sorting-algorithms/selection-sort\n[selection-wiki]: https://en.wikipedia.org/wiki/Selection_sort\n[selection-image]: https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Selection_sort_animation.gif/250px-Selection_sort_animation.gif \"Selection Sort Sort\"\n\n[shell-toptal]: https://www.toptal.com/developers/sorting-algorithms/shell-sort\n[shell-wiki]: https://en.wikipedia.org/wiki/Shellsort\n[shell-image]: https://upload.wikimedia.org/wikipedia/commons/d/d8/Sorting_shellsort_anim.gif \"Shell Sort\"\n\n[topological-wiki]: https://en.wikipedia.org/wiki/Topological_sorting\n\n[linear-wiki]: https://en.wikipedia.org/wiki/Linear_search\n[linear-image]: http://www.tutorialspoint.com/data_structures_algorithms/images/linear_search.gif \"Linear Search\"\n\n[binary-wiki]: https://en.wikipedia.org/wiki/Binary_search_algorithm\n[binary-image]: https://upload.wikimedia.org/wikipedia/commons/f/f7/Binary_search_into_array.png \"Binary Search\"\n\n\n[caesar]: https://upload.wikimedia.org/wikipedia/commons/4/4a/Caesar_cipher_left_shift_of_3.svg \"Caesar\"\n\n[ROT13-image]: https://upload.wikimedia.org/wikipedia/commons/3/33/ROT13_table_with_example.svg \"ROT13\"\n\n[JumpSearch-image]: https://i1.wp.com/theoryofprogramming.com/wp-content/uploads/2016/11/jump-search-1.jpg \"Jump Search\"\n\n[QuickSelect-image]: https://upload.wikimedia.org/wikipedia/commons/0/04/Selecting_quickselect_frames.gif \"Quick Select\"\n"
  },
  {
    "repo": "home-assistant/home-assistant",
    "content": "Home Assistant |Build Status| |Coverage Status| |Chat Status| |Reviewed by Hound|\n=================================================================================\n\nHome Assistant is a home automation platform running on Python 3. It is able to track and control all devices at home and offer a platform for automating control.\n\nTo get started:\n\n.. code:: bash\n\n    python3 -m pip install homeassistant\n    hass --open-ui\n\nCheck out `home-assistant.io \u003chttps://home-assistant.io\u003e`__ for `a\ndemo \u003chttps://home-assistant.io/demo/\u003e`__, `installation instructions \u003chttps://home-assistant.io/getting-started/\u003e`__,\n`tutorials \u003chttps://home-assistant.io/getting-started/automation-2/\u003e`__ and `documentation \u003chttps://home-assistant.io/docs/\u003e`__.\n\n|screenshot-states|\n\nFeatured integrations\n---------------------\n\n|screenshot-components|\n\nThe system is built using a modular approach so support for other devices or actions can be implemented easily. See also the `section on architecture \u003chttps://developers.home-assistant.io/docs/en/architecture_index.html\u003e`__ and the `section on creating your own\ncomponents \u003chttps://developers.home-assistant.io/docs/en/creating_component_index.html\u003e`__.\n\nIf you run into issues while using Home Assistant or during development\nof a component, check the `Home Assistant help section \u003chttps://home-assistant.io/help/\u003e`__ of our website for further help and information.\n\n.. |Build Status| image:: https://travis-ci.org/home-assistant/home-assistant.svg?branch=master\n   :target: https://travis-ci.org/home-assistant/home-assistant\n.. |Coverage Status| image:: https://img.shields.io/coveralls/home-assistant/home-assistant.svg\n   :target: https://coveralls.io/r/home-assistant/home-assistant?branch=master\n.. |Chat Status| image:: https://img.shields.io/discord/330944238910963714.svg\n   :target: https://discord.gg/c5DvZ4e\n.. |Reviewed by Hound| image:: https://img.shields.io/badge/Reviewed_by-Hound-8E64B0.svg\n   :target: https://houndci.com\n.. |screenshot-states| image:: https://raw.github.com/home-assistant/home-assistant/master/docs/screenshots.png\n   :target: https://home-assistant.io/demo/\n.. |screenshot-components| image:: https://raw.github.com/home-assistant/home-assistant/dev/docs/screenshot-components.png\n   :target: https://home-assistant.io/components/\n"
  },
  {
    "repo": "ageitgey/face_recognition",
    "content": "# Face Recognition\n\n_You can also read a translated version of this file [in Chinese 简体中文版](https://github.com/ageitgey/face_recognition/blob/master/README_Simplified_Chinese.md)._\n\nRecognize and manipulate faces from Python or from the command line with\nthe world's simplest face recognition library.\n\nBuilt using [dlib](http://dlib.net/)'s state-of-the-art face recognition\nbuilt with deep learning. The model has an accuracy of 99.38% on the\n[Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/) benchmark.\n\nThis also provides a simple `face_recognition` command line tool that lets\nyou do face recognition on a folder of images from the command line!\n\n\n[![PyPI](https://img.shields.io/pypi/v/face_recognition.svg)](https://pypi.python.org/pypi/face_recognition)\n[![Build Status](https://travis-ci.org/ageitgey/face_recognition.svg?branch=master)](https://travis-ci.org/ageitgey/face_recognition)\n[![Documentation Status](https://readthedocs.org/projects/face-recognition/badge/?version=latest)](http://face-recognition.readthedocs.io/en/latest/?badge=latest)\n\n## Features\n\n#### Find faces in pictures\n\nFind all the faces that appear in a picture:\n\n![](https://cloud.githubusercontent.com/assets/896692/23625227/42c65360-025d-11e7-94ea-b12f28cb34b4.png)\n\n```python\nimport face_recognition\nimage = face_recognition.load_image_file(\"your_file.jpg\")\nface_locations = face_recognition.face_locations(image)\n```\n\n#### Find and manipulate facial features in pictures\n\nGet the locations and outlines of each person's eyes, nose, mouth and chin.\n\n![](https://cloud.githubusercontent.com/assets/896692/23625282/7f2d79dc-025d-11e7-8728-d8924596f8fa.png)\n\n```python\nimport face_recognition\nimage = face_recognition.load_image_file(\"your_file.jpg\")\nface_landmarks_list = face_recognition.face_landmarks(image)\n```\n\nFinding facial features is super useful for lots of important stuff. But you can also use for really stupid stuff\nlike applying [digital make-up](https://github.com/ageitgey/face_recognition/blob/master/examples/digital_makeup.py) (think 'Meitu'):\n\n![](https://cloud.githubusercontent.com/assets/896692/23625283/80638760-025d-11e7-80a2-1d2779f7ccab.png)\n\n#### Identify faces in pictures\n\nRecognize who appears in each photo.\n\n![](https://cloud.githubusercontent.com/assets/896692/23625229/45e049b6-025d-11e7-89cc-8a71cf89e713.png)\n\n```python\nimport face_recognition\nknown_image = face_recognition.load_image_file(\"biden.jpg\")\nunknown_image = face_recognition.load_image_file(\"unknown.jpg\")\n\nbiden_encoding = face_recognition.face_encodings(known_image)[0]\nunknown_encoding = face_recognition.face_encodings(unknown_image)[0]\n\nresults = face_recognition.compare_faces([biden_encoding], unknown_encoding)\n```\n\nYou can even use this library with other Python libraries to do real-time face recognition:\n\n![](https://cloud.githubusercontent.com/assets/896692/24430398/36f0e3f0-13cb-11e7-8258-4d0c9ce1e419.gif)\n\nSee [this example](https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_from_webcam_faster.py) for the code.\n\n## Installation\n\n### Requirements\n\n  * Python 3.3+ or Python 2.7\n  * macOS or Linux (Windows not officially supported, but might work)\n\n### Installation Options:\n\n#### Installing on Mac or Linux\n\nFirst, make sure you have dlib already installed with Python bindings:\n\n  * [How to install dlib from source on macOS or Ubuntu](https://gist.github.com/ageitgey/629d75c1baac34dfa5ca2a1928a7aeaf)\n\nThen, install this module from pypi using `pip3` (or `pip2` for Python 2):\n\n```bash\npip3 install face_recognition\n```\n\nIf you are having trouble with installation, you can also try out a\n[pre-configured VM](https://medium.com/@ageitgey/try-deep-learning-in-python-now-with-a-fully-pre-configured-vm-1d97d4c3e9b).\n\n#### Installing on Raspberry Pi 2+\n\n  * [Raspberry Pi 2+ installation instructions](https://gist.github.com/ageitgey/1ac8dbe8572f3f533df6269dab35df65)\n\n#### Installing on Windows\n\nWhile Windows isn't officially supported, helpful users have posted instructions on how to install this library:\n\n  * [@masoudr's Windows 10 installation guide (dlib + face_recognition)](https://github.com/ageitgey/face_recognition/issues/175#issue-257710508)\n\n#### Installing a pre-configured Virtual Machine image\n\n  * [Download the pre-configured VM image](https://medium.com/@ageitgey/try-deep-learning-in-python-now-with-a-fully-pre-configured-vm-1d97d4c3e9b) (for VMware Player or VirtualBox).\n\n## Usage\n\n### Command-Line Interface\n\nWhen you install `face_recognition`, you get a two simple command-line \nprograms:\n\n* `face_recognition` - Recognize faces in a photograph or folder full for \n   photographs.\n* `face_detection` - Find faces in a photograph or folder full for photographs.\n\n#### `face_recognition` command line tool\n\nThe `face_recognition` command lets you recognize faces in a photograph or \nfolder full  for photographs.\n\nFirst, you need to provide a folder with one picture of each person you\nalready know. There should be one image file for each person with the\nfiles named according to who is in the picture:\n\n![known](https://cloud.githubusercontent.com/assets/896692/23582466/8324810e-00df-11e7-82cf-41515eba704d.png)\n\nNext, you need a second folder with the files you want to identify:\n\n![unknown](https://cloud.githubusercontent.com/assets/896692/23582465/81f422f8-00df-11e7-8b0d-75364f641f58.png)\n\nThen in you simply run the command `face_recognition`, passing in\nthe folder of known people and the folder (or single image) with unknown\npeople and it tells you who is in each image:\n\n```bash\n$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/\n\n/unknown_pictures/unknown.jpg,Barack Obama\n/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person\n```\n\nThere's one line in the output for each face. The data is comma-separated\nwith the filename and the name of the person found.\n\nAn `unknown_person` is a face in the image that didn't match anyone in\nyour folder of known people.\n\n#### `face_detection` command line tool\n\nThe `face_detection` command lets you find the location (pixel coordinatates) \nof any faces in an image.\n\nJust run the command `face_detection`, passing in a folder of images \nto check (or a single image):\n\n```bash\n$ face_detection  ./folder_with_pictures/\n\nexamples/image1.jpg,65,215,169,112\nexamples/image2.jpg,62,394,211,244\nexamples/image2.jpg,95,941,244,792\n```\n\nIt prints one line for each face that was detected. The coordinates\nreported are the top, right, bottom and left coordinates of the face (in pixels).\n \n##### Adjusting Tolerance / Sensitivity\n\nIf you are getting multiple matches for the same person, it might be that\nthe people in your photos look very similar and a lower tolerance value\nis needed to make face comparisons more strict.\n\nYou can do that with the `--tolerance` parameter. The default tolerance\nvalue is 0.6 and lower numbers make face comparisons more strict:\n\n```bash\n$ face_recognition --tolerance 0.54 ./pictures_of_people_i_know/ ./unknown_pictures/\n\n/unknown_pictures/unknown.jpg,Barack Obama\n/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person\n```\n\nIf you want to see the face distance calculated for each match in order\nto adjust the tolerance setting, you can use `--show-distance true`:\n\n```bash\n$ face_recognition --show-distance true ./pictures_of_people_i_know/ ./unknown_pictures/\n\n/unknown_pictures/unknown.jpg,Barack Obama,0.378542298956785\n/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person,None\n```\n\n##### More Examples\n\nIf you simply want to know the names of the people in each photograph but don't\ncare about file names, you could do this:\n\n```bash\n$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/ | cut -d ',' -f2\n\nBarack Obama\nunknown_person\n```\n\n##### Speeding up Face Recognition\n\nFace recognition can be done in parallel if you have a computer with\nmultiple CPU cores. For example if your system has 4 CPU cores, you can\nprocess about 4 times as many images in the same amount of time by using\nall your CPU cores in parallel.\n\nIf you are using Python 3.4 or newer, pass in a `--cpus \u003cnumber_of_cpu_cores_to_use\u003e` parameter:\n\n```bash\n$ face_recognition --cpus 4 ./pictures_of_people_i_know/ ./unknown_pictures/\n```\n\nYou can also pass in `--cpus -1` to use all CPU cores in your system.\n\n#### Python Module\n\nYou can import the `face_recognition` module and then easily manipulate\nfaces with just a couple of lines of code. It's super easy!\n\nAPI Docs: [https://face-recognition.readthedocs.io](https://face-recognition.readthedocs.io/en/latest/face_recognition.html).\n\n##### Automatically find all the faces in an image\n\n```python\nimport face_recognition\n\nimage = face_recognition.load_image_file(\"my_picture.jpg\")\nface_locations = face_recognition.face_locations(image)\n\n# face_locations is now an array listing the co-ordinates of each face!\n```\n\nSee [this example](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_picture.py)\n to try it out.\n\nYou can also opt-in to a somewhat more accurate deep-learning-based face detection model.\n\nNote: GPU acceleration (via nvidia's CUDA library) is required for good\nperformance with this model. You'll also want to enable CUDA support\nwhen compliling `dlib`.\n\n```python\nimport face_recognition\n\nimage = face_recognition.load_image_file(\"my_picture.jpg\")\nface_locations = face_recognition.face_locations(image, model=\"cnn\")\n\n# face_locations is now an array listing the co-ordinates of each face!\n```\n\nSee [this example](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_picture_cnn.py)\n to try it out.\n\nIf you have a lot of images and a GPU, you can also\n[find faces in batches](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_batches.py).\n\n##### Automatically locate the facial features of a person in an image\n\n```python\nimport face_recognition\n\nimage = face_recognition.load_image_file(\"my_picture.jpg\")\nface_landmarks_list = face_recognition.face_landmarks(image)\n\n# face_landmarks_list is now an array with the locations of each facial feature in each face.\n# face_landmarks_list[0]['left_eye'] would be the location and outline of the first person's left eye.\n```\n\nSee [this example](https://github.com/ageitgey/face_recognition/blob/master/examples/find_facial_features_in_picture.py)\n to try it out.\n\n##### Recognize faces in images and identify who they are\n\n```python\nimport face_recognition\n\npicture_of_me = face_recognition.load_image_file(\"me.jpg\")\nmy_face_encoding = face_recognition.face_encodings(picture_of_me)[0]\n\n# my_face_encoding now contains a universal 'encoding' of my facial features that can be compared to any other picture of a face!\n\nunknown_picture = face_recognition.load_image_file(\"unknown.jpg\")\nunknown_face_encoding = face_recognition.face_encodings(unknown_picture)[0]\n\n# Now we can see the two face encodings are of the same person with `compare_faces`!\n\nresults = face_recognition.compare_faces([my_face_encoding], unknown_face_encoding)\n\nif results[0] == True:\n    print(\"It's a picture of me!\")\nelse:\n    print(\"It's not a picture of me!\")\n```\n\nSee [this example](https://github.com/ageitgey/face_recognition/blob/master/examples/recognize_faces_in_pictures.py)\n to try it out.\n\n## Python Code Examples\n\nAll the examples are available [here](https://github.com/ageitgey/face_recognition/tree/master/examples).\n\n\n#### Face Detection\n\n* [Find faces in a photograph](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_picture.py)\n* [Find faces in a photograph (using deep learning)](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_picture_cnn.py)\n* [Find faces in batches of images w/ GPU (using deep learning)](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_batches.py)\n* [Blur all the faces in a live video using your webcam (Requires OpenCV to be installed)](https://github.com/ageitgey/face_recognition/blob/master/examples/blur_faces_on_webcam.py)\n\n#### Facial Features\n\n* [Identify specific facial features in a photograph](https://github.com/ageitgey/face_recognition/blob/master/examples/find_facial_features_in_picture.py)\n* [Apply (horribly ugly) digital make-up](https://github.com/ageitgey/face_recognition/blob/master/examples/digital_makeup.py)\n\n#### Facial Recognition\n\n* [Find and recognize unknown faces in a photograph based on photographs of known people](https://github.com/ageitgey/face_recognition/blob/master/examples/recognize_faces_in_pictures.py)\n* [Identify and draw boxes around each person in a photo](https://github.com/ageitgey/face_recognition/blob/master/examples/identify_and_draw_boxes_on_faces.py)\n* [Compare faces by numeric face distance instead of only True/False matches](https://github.com/ageitgey/face_recognition/blob/master/examples/face_distance.py)\n* [Recognize faces in live video using your webcam - Simple / Slower Version (Requires OpenCV to be installed)](https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_from_webcam.py)\n* [Recognize faces in live video using your webcam - Faster Version (Requires OpenCV to be installed)](https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_from_webcam_faster.py)\n* [Recognize faces in a video file and write out new video file (Requires OpenCV to be installed)](https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_from_video_file.py)\n* [Recognize faces on a Raspberry Pi w/ camera](https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_on_raspberry_pi.py)\n* [Run a web service to recognize faces via HTTP (Requires Flask to be installed)](https://github.com/ageitgey/face_recognition/blob/master/examples/web_service_example.py)\n* [Recognize faces with a K-nearest neighbors classifier](https://github.com/ageitgey/face_recognition/blob/master/examples/face_recognition_knn.py)\n\n## Articles and Guides that cover `face_recognition`\n\n- My article on how Face Recognition works: [Modern Face Recognition with Deep Learning](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78)\n  - Covers the algorithms and how they generally work\n- [Face recognition with OpenCV, Python, and deep learning](https://www.pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/) by Adrian Rosebrock\n  - Covers how to use face recognition in practice\n- [Raspberry Pi Face Recognition](https://www.pyimagesearch.com/2018/06/25/raspberry-pi-face-recognition/) by Adrian Rosebrock\n  - Covers how to use this on a Raspberry Pi\n- [Face clustering with Python](https://www.pyimagesearch.com/2018/07/09/face-clustering-with-python/) by Adrian Rosebrock\n  - Covers how to automatically cluster photos based on who appears in each photo using unsupervised learning\n\n## How Face Recognition Works\n\nIf you want to learn how face location and recognition work instead of\ndepending on a black box library, [read my article](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78).\n\n## Caveats\n\n* The face recognition model is trained on adults and does not work very well on children. It tends to mix\n  up children quite easy using the default comparison threshold of 0.6.\n* Accuracy may vary between ethnic groups. Please see [this wiki page](https://github.com/ageitgey/face_recognition/wiki/Face-Recognition-Accuracy-Problems#question-face-recognition-works-well-with-european-individuals-but-overall-accuracy-is-lower-with-asian-individuals) for more details.\n\n## Deployment to Cloud Hosts (Heroku, AWS, etc)\n\nSince `face_recognition` depends on `dlib` which is written in C++, it can be tricky to deploy an app\nusing it to a cloud hosting provider like Heroku or AWS.\n\nTo make things easier, there's an example Dockerfile in this repo that shows how to run an app built with\n`face_recognition` in a [Docker](https://www.docker.com/) container. With that, you should be able to deploy\nto any service that supports Docker images.\n\n## Having problems?\n\nIf you run into problems, please read the [Common Errors](https://github.com/ageitgey/face_recognition/wiki/Common-Errors) section of the wiki before filing a github issue.\n\n## Thanks\n\n* Many, many thanks to [Davis King](https://github.com/davisking) ([@nulhom](https://twitter.com/nulhom))\n  for creating dlib and for providing the trained facial feature detection and face encoding models\n  used in this library. For more information on the ResNet that powers the face encodings, check out\n  his [blog post](http://blog.dlib.net/2017/02/high-quality-face-recognition-with-deep.html).\n* Thanks to everyone who works on all the awesome Python data science libraries like numpy, scipy, scikit-image,\n  pillow, etc, etc that makes this kind of stuff so easy and fun in Python.\n* Thanks to [Cookiecutter](https://github.com/audreyr/cookiecutter) and the\n  [audreyr/cookiecutter-pypackage](https://github.com/audreyr/cookiecutter-pypackage) project template\n  for making Python project packaging way more tolerable.\n"
  },
  {
    "repo": "getsentry/sentry",
    "content": ".. raw:: html\n\n   \u003cp align=\"center\"\u003e\n     \u003cp align=\"center\"\u003e\n       \u003cimg src=\"https://sentry-brand.storage.googleapis.com/sentry-logo-black.png\" alt=\"Sentry\" height=\"72\"\n     \u003c/p\u003e\n     \u003cp align=\"center\"\u003e\n       Users and logs provide clues. Sentry provides answers.\n     \u003c/p\u003e\n   \u003c/p\u003e\n\nWhat's Sentry?\n--------------\n\nSentry fundamentally is a service that helps you monitor and fix crashes in realtime.\nThe server is in Python, but it contains a full API for sending events from any\nlanguage, in any application.\n\n.. raw:: html\n\n   \u003cp align=\"center\"\u003e\n     \u003cimg src=\"https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/sentry-product-issue-screenshot.png\" height=\"180\"\u003e\n     \u003cimg src=\"https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/sentry-product-project-screenshot.png\" height=\"180\"\u003e\n     \u003cimg src=\"https://github.com/getsentry/sentry/raw/master/src/sentry/static/sentry/images/sentry-product-releases-screenshot.png\" height=\"180\"\u003e\n   \u003c/p\u003e\n\nOfficial Sentry SDKs\n~~~~~~~~~~~~~~~~~~~~\n* `JavaScript \u003chttps://github.com/getsentry/sentry-javascript\u003e`_\n* `React-Native \u003chttps://github.com/getsentry/react-native-sentry\u003e`_\n* `Python \u003chttps://github.com/getsentry/sentry-python\u003e`_\n* `Ruby \u003chttps://github.com/getsentry/raven-ruby\u003e`_\n* `PHP \u003chttps://github.com/getsentry/sentry-php\u003e`_\n* `Go \u003chttps://github.com/getsentry/raven-go\u003e`_\n* `Java \u003chttps://github.com/getsentry/sentry-java\u003e`_\n* `Objective-C/Swift \u003chttps://github.com/getsentry/sentry-cocoa\u003e`_\n* `C# \u003chttps://github.com/getsentry/sentry-dotnet\u003e`_\n* `Perl \u003chttps://github.com/getsentry/perl-raven\u003e`_\n* `Elixir \u003chttps://github.com/getsentry/sentry-elixir\u003e`_\n* `Laravel \u003chttps://github.com/getsentry/sentry-laravel\u003e`_\n\nResources\n---------\n\n* `Documentation \u003chttps://docs.sentry.io/\u003e`_\n* `Community \u003chttps://forum.sentry.io/\u003e`_ (Bugs, feature requests, general questions)\n* `Contributing \u003chttps://docs.sentry.io/internal/contributing/\u003e`_\n* `Bug Tracker \u003chttps://github.com/getsentry/sentry/issues\u003e`_\n* `Code \u003chttps://github.com/getsentry/sentry\u003e`_\n* `IRC \u003circ://irc.freenode.net/sentry\u003e`_  (irc.freenode.net, #sentry)\n* `Transifex \u003chttps://www.transifex.com/getsentry/sentry/\u003e`_ (Translate Sentry!)\n"
  },
  {
    "repo": "0xAX/linux-insides",
    "content": "linux-insides\n===============\n\nA book-in-progress about the linux kernel and its insides.\n\n**The goal is simple** - to share my modest knowledge about the insides of the linux kernel and help people who are interested in linux kernel insides, and other low-level subject matter. Feel free to go through the book [Start here](https://github.com/0xAX/linux-insides/blob/master/SUMMARY.md)\n\n**Questions/Suggestions**: Feel free about any questions or suggestions by pinging me at twitter [@0xAX](https://twitter.com/0xAX), adding an [issue](https://github.com/0xAX/linux-insides/issues/new) or just drop me an [email](mailto:anotherworldofworld@gmail.com).\n\n# Mailing List\n\nWe have a Google Group mailing list for learning the kernel source code. Here are some instructions about how to use it.\n\n#### Join\n\nSend an email with any subject/content to `kernelhacking+subscribe@googlegroups.com`. Then you will receive a confirmation email. Reply it with any content and then you are done.\n\n\u003e If you have Google account, you can also open the [archive page](https://groups.google.com/forum/#!forum/kernelhacking) and click **Apply to join group**. You will be approved automatically.\n\n#### Send emails to mailing list\n\nJust send emails to `kernelhacking@googlegroups.com`. The basic usage is the same as other mailing lists powered by mailman.\n\n#### Archives\n\nhttps://groups.google.com/forum/#!forum/kernelhacking\n\nSupport\n-------\n\n**Support** If you like `linux-insides` you can support me with: \n\n[![Support with bitcoin](https://img.shields.io/badge/donate-bitcoin-green.svg)](https://www.coinbase.com/checkouts/0bfa452a41cf52c0b3f99500b4f31685) [![Join the chat at https://gitter.im/0xAX/linux-insides](https://badges.gitter.im/0xAX/linux-insides.svg)](https://gitter.im/0xAX/linux-insides?utm_source=badge\u0026utm_medium=badge\u0026utm_campaign=pr-badge\u0026utm_content=badge)\n\nOn other languages\n-------------------\n\n  * [Brazilian Portuguese](https://github.com/mauri870/linux-insides)\n  * [Chinese](https://github.com/MintCN/linux-insides-zh)\n  * [Japanese](https://github.com/tkmru/linux-insides-ja)\n  * [Russian](https://github.com/proninyaroslav/linux-insides-ru)\n  * [Spanish](https://github.com/leolas95/linux-insides)\n  * [Turkish](https://github.com/ayyucedemirbas/linux-insides_Turkish)\n\nContributions \n--------------\n\nFeel free to create issues or pull-requests if you have any problems.\n\n**Please read [CONTRIBUTING.md](https://github.com/0xAX/linux-insides/blob/master/CONTRIBUTING.md) before pushing any changes.**\n\n![image](http://oi58.tinypic.com/23upobq.jpg)\n\nAuthor\n---------------\n\n[@0xAX](https://twitter.com/0xAX)\n\nLICENSE\n-------------\n\nLicensed [BY-NC-SA Creative Commons](http://creativecommons.org/licenses/by-nc-sa/4.0/).\n"
  },
  {
    "repo": "faif/python-patterns",
    "content": "python-patterns\n===============\n\nA collection of design patterns and idioms in Python.\n\nCurrent Patterns\n----------------\n\n__Creational Patterns__:\n\n| Pattern | Description |\n|:-------:| ----------- |\n| [abstract_factory](creational/abstract_factory.py) | use a generic function with specific factories |\n| [borg](creational/borg.py) | a singleton with shared-state among instances |\n| [builder](creational/builder.py) | instead of using multiple constructors, builder object receives parameters and returns constructed objects |\n| [factory_method](creational/factory_method.py) | delegate a specialized function/method to create instances |\n| [lazy_evaluation](creational/lazy_evaluation.py) | lazily-evaluated property pattern in Python |\n| [pool](creational/pool.py) | preinstantiate and maintain a group of instances of the same type |\n| [prototype](creational/prototype.py) | use a factory and clones of a prototype for new instances (if instantiation is expensive) |\n\n__Structural Patterns__:\n\n| Pattern | Description |\n|:-------:| ----------- |\n| [3-tier](structural/3-tier.py) | data\u003c-\u003ebusiness logic\u003c-\u003epresentation separation (strict relationships) |\n| [adapter](structural/adapter.py) | adapt one interface to another using a white-list |\n| [bridge](structural/bridge.py) | a client-provider middleman to soften interface changes |\n| [composite](structural/composite.py) | lets clients treat individual objects and compositions uniformly |\n| [decorator](structural/decorator.py) | wrap functionality with other functionality in order to affect outputs |\n| [facade](structural/facade.py) | use one class as an API to a number of others |\n| [flyweight](structural/flyweight.py) | transparently reuse existing instances of objects with similar/identical state |\n| [front_controller](structural/front_controller.py) | single handler requests coming to the application |\n| [mvc](structural/mvc.py) | model\u003c-\u003eview\u003c-\u003econtroller (non-strict relationships) |\n| [proxy](structural/proxy.py) | an object funnels operations to something else |\n\n__Behavioral Patterns__:\n\n| Pattern | Description |\n|:-------:| ----------- |\n| [chain](behavioral/chain.py) | apply a chain of successive handlers to try and process the data |\n| [catalog](behavioral/catalog.py) | general methods will call different specialized methods based on construction parameter |\n| [chaining_method](behavioral/chaining_method.py) | continue callback next object method |\n| [command](behavioral/command.py) | bundle a command and arguments to call later |\n| [iterator](behavioral/iterator.py) | traverse a container and access the container's elements |\n| [mediator](behavioral/mediator.py) | an object that knows how to connect other objects and act as a proxy |\n| [memento](behavioral/memento.py) | generate an opaque token that can be used to go back to a previous state |\n| [observer](behavioral/observer.py) | provide a callback for notification of events/changes to data |\n| [publish_subscribe](behavioral/publish_subscribe.py) | a source syndicates events/data to 0+ registered listeners |\n| [registry](behavioral/registry.py) | keep track of all subclasses of a given class |\n| [specification](behavioral/specification.py) |  business rules can be recombined by chaining the business rules together using boolean logic |\n| [state](behavioral/state.py) | logic is organized into a discrete number of potential states and the next state that can be transitioned to |\n| [strategy](behavioral/strategy.py) | selectable operations over the same data |\n| [template](behavioral/template.py) | an object imposes a structure but takes pluggable components |\n| [visitor](behavioral/visitor.py) | invoke a callback for all items of a collection |\n\n__Design for Testability Patterns__:\n\n| Pattern | Description |\n|:-------:| ----------- |\n| [setter_injection](dft/setter_injection.py) | the client provides the depended-on object to the SUT via the setter injection (implementation variant of dependency injection) |\n\n__Fundamental Patterns__:\n\n| Pattern | Description |\n|:-------:| ----------- |\n| [delegation_pattern](fundamental/delegation_pattern.py) | an object handles a request by delegating to a second object (the delegate) |\n\n__Others__:\n\n| Pattern | Description |\n|:-------:| ----------- |\n| [blackboard](other/blackboard.py) | architectural model, assemble different sub-system knowledge to build a solution, AI approach - non gang of four pattern |\n| [graph_search](other/graph_search.py) | graphing algorithms - non gang of four pattern |\n| [hsm](other/hsm/hsm.py) | hierarchical state machine - non gang of four pattern |\n\n\nContributing\n------------\nWhen an implementation is added or modified, please review the following guidelines:\n\n##### Output\nAll files with example patterns have `### OUTPUT ###` section at the bottom.\n\nRun `append_output.sh` (e.g. `./append_output.sh borg.py`) to generate/update it.\n\n##### Docstrings\nAdd module level description in form of a docstring with links to corresponding references or other useful information.\n\n[strategy.py](behavioral/strategy.py) has a good example of detailed description,\nbut sometimes the shorter one as in [template.py](behavioral/template.py) would suffice.\n\nIn some cases class-level docstring with doctest would also help (see [adapter.py](structural/adapter.py))\n\n##### Python2/3 compatibility\nTry to keep it (discussion is held in [issue #208](https://github.com/faif/python-patterns/issues/208))\n- use new style classes (inherit from `object`)\n- use `from future import print`\n\n##### Update README\nWhen everything else is done - update corresponding part of README."
  }
]